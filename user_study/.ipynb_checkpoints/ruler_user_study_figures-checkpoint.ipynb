{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/rulerauthors/ruler/blob/master/user_study/ruler_user_study_figures.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XrzzXDLCIg8Y"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ae7487936fee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maltair\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'display.max_rows'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'display.max_columns'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EigrKeTbJ3Xo"
   },
   "source": [
    "## Load the full user study data from Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 606
    },
    "colab_type": "code",
    "id": "wVXo3EbXJlWx",
    "outputId": "5f6f12bb-e611-4572-bf82-02372ec6be01"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0a757023af37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfull_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://raw.githubusercontent.com/rulerauthors/ruler/master/user_study/full_study_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "full_data = pd.read_csv('https://raw.githubusercontent.com/rulerauthors/ruler/master/user_study/full_study_data.csv')\n",
    "display(full_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TactFI4vNNa8"
   },
   "source": [
    "## About this data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H5uuYGwDNlUr"
   },
   "source": [
    "\n",
    "\n",
    "> We carried out the study using  a within-subjects experiment design, where all participants performed tasks using both conditions (tools).  The sole independent variable controlled was the method of creating labeling functions. We counterbalanced the order in which the tools were used, as well as which classification task we performed with which tool. \n",
    "\n",
    "### Tasks and Procedure \n",
    "> We asked participants to write  labeling functions for two prevalent labeling tasks: spam detection and sentiment classification.  They performed these two tasks on  YouTube Comments and Amazon Reviews, respectively. Participants received 15 mins of instruction on how to use each tool, using a topic classification task (electronics vs. guns) over a newsgroup dataset~\\cite{rennie200820} as an example. We asked participants to write as many functions as they considered necessary for the goal of the task.  There were given 30 mins to complete each task and we recorded the labeling functions they created and these functions' individual and aggregate performances.  After completing both tasks, participants also filled out an exit survey, providing their qualitative feedback.\n",
    "\n",
    "> For the manual programming condition, we iteratively developed a Jupyter notebook interface based on the Snorkel tutorial. We provided a section for writing functions, a section with diverse analysis tools, and a section to train a logistic regression model on the labels they had generated (evaluated on the test set shown to the user, which is separate from our heldout test set used for the final evaluation).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A6VOJnAuJ9fw"
   },
   "source": [
    "## Select Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JqInQrTuMlUn"
   },
   "source": [
    "From [our EMNLP '20 submission](https://github.com/rulerauthors/ruler/blob/master/media/Ruler_EMNLP2020.pdf):\n",
    "\n",
    "\n",
    "\n",
    "> To analyze the performance of the labeling functions created by participants, for each participant we select and task the labeling  model  that achieved the highest f1 score on the development set.  For each labeling model, we then train a logistic regression model on a training dataset  generated by the model.  We finally evaluate the performance of the logistic regression model on a heldout test set. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DXYvcdQlJn8M"
   },
   "outputs": [],
   "source": [
    "def create_best_table_small(action='heldout_test_LR_stats'):\n",
    "  dt = pd.DataFrame()\n",
    "\n",
    "  subjects = full_data.participant.value_counts().index\n",
    "  datasets = ['amazon', 'youtube']\n",
    "\n",
    "  for _, pid in enumerate(subjects):\n",
    "    for d in datasets: \n",
    "      # gather all the rows logging participant {pid}'s progress on the given dataset/task\n",
    "      sub_df = full_data[(full_data['participant']==pid) & (full_data['dataset']==d)]\n",
    "      sub_df = sub_df.reset_index(drop=True)\n",
    "\n",
    "      # find index of best performance on dev set\n",
    "      idxmax = sub_df[sub_df.data == 'dev']['f1'].idxmax()\n",
    "\n",
    "      # choose the first logistic regression model trained after that,\n",
    "      # report the performance on the held out test data\n",
    "      try:\n",
    "        r = sub_df.loc[idxmax:][sub_df.action==action].iloc[0]\n",
    "      except IndexError:\n",
    "        # in one case the user never finished any labelling functions, \n",
    "        # so we report the initial 'baseline' LR performance\n",
    "        # which is f1 score of 0.5\n",
    "        r = sub_df[sub_df.action==action].iloc[0]\n",
    "\n",
    "      # the logged precision and recall are separated by class. \n",
    "      # we use the heldout dataset splits to compute micro precision and recall\n",
    "      size0 = 418\n",
    "      size1 = 382\n",
    "      if r.task==\"Youtube\":\n",
    "        size0=192\n",
    "        size1=164\n",
    "      prec = (r['precision_0']*size0 +r['precision_1']*size1)/(size0+size1)\n",
    "      rec = (r['recall_0']*size0+r['recall_1']*size1)/(size0+size1)\n",
    "\n",
    "      dt = dt.append({'participant': pid, \n",
    "                      'condition': r['condition'].lower(),\n",
    "                      'task':'sentiment' if d == 'amazon' else 'spam', \n",
    "                      'dataset':d,\n",
    "                      'f1':r['micro_f1'],\n",
    "                      'precision':prec, \n",
    "                      'recall':rec, \n",
    "                      'accuracy':r['accuracy'],\n",
    "                      'max_dev_f1': sub_df.at[idxmax, 'f1'],\n",
    "                      'training_label_coverage': r['training_label_coverage'],\n",
    "                      }, ignore_index=True)\n",
    "  return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "colab_type": "code",
    "id": "86_2nl8UJ1KB",
    "outputId": "814721cd-d8b5-4a7e-8c32-7116b5726a0a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>condition</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>max_dev_f1</th>\n",
       "      <th>participant</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>task</th>\n",
       "      <th>training_label_coverage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.483750</td>\n",
       "      <td>snorkel</td>\n",
       "      <td>amazon</td>\n",
       "      <td>0.562036</td>\n",
       "      <td>0.653386</td>\n",
       "      <td>p4</td>\n",
       "      <td>0.492273</td>\n",
       "      <td>0.483750</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>0.58375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.578652</td>\n",
       "      <td>ruler</td>\n",
       "      <td>youtube</td>\n",
       "      <td>0.713740</td>\n",
       "      <td>0.632558</td>\n",
       "      <td>p4</td>\n",
       "      <td>0.682599</td>\n",
       "      <td>0.525599</td>\n",
       "      <td>spam</td>\n",
       "      <td>0.13500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.482500</td>\n",
       "      <td>ruler</td>\n",
       "      <td>amazon</td>\n",
       "      <td>0.622951</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>p8</td>\n",
       "      <td>0.501770</td>\n",
       "      <td>0.482500</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>0.11250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.789326</td>\n",
       "      <td>snorkel</td>\n",
       "      <td>youtube</td>\n",
       "      <td>0.764890</td>\n",
       "      <td>0.697436</td>\n",
       "      <td>p8</td>\n",
       "      <td>0.821485</td>\n",
       "      <td>0.809982</td>\n",
       "      <td>spam</td>\n",
       "      <td>0.54375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.506250</td>\n",
       "      <td>ruler</td>\n",
       "      <td>amazon</td>\n",
       "      <td>0.655623</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>p2</td>\n",
       "      <td>0.667621</td>\n",
       "      <td>0.506250</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>0.40250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.679775</td>\n",
       "      <td>snorkel</td>\n",
       "      <td>youtube</td>\n",
       "      <td>0.604167</td>\n",
       "      <td>0.691892</td>\n",
       "      <td>p2</td>\n",
       "      <td>0.744225</td>\n",
       "      <td>0.710193</td>\n",
       "      <td>spam</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.588750</td>\n",
       "      <td>ruler</td>\n",
       "      <td>amazon</td>\n",
       "      <td>0.636464</td>\n",
       "      <td>0.644860</td>\n",
       "      <td>p1</td>\n",
       "      <td>0.608134</td>\n",
       "      <td>0.588750</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>0.29750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.528090</td>\n",
       "      <td>snorkel</td>\n",
       "      <td>youtube</td>\n",
       "      <td>0.596154</td>\n",
       "      <td>0.651163</td>\n",
       "      <td>p1</td>\n",
       "      <td>0.517664</td>\n",
       "      <td>0.512288</td>\n",
       "      <td>spam</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.510000</td>\n",
       "      <td>snorkel</td>\n",
       "      <td>amazon</td>\n",
       "      <td>0.565410</td>\n",
       "      <td>0.639456</td>\n",
       "      <td>p3</td>\n",
       "      <td>0.519668</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.612360</td>\n",
       "      <td>ruler</td>\n",
       "      <td>youtube</td>\n",
       "      <td>0.726190</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>p3</td>\n",
       "      <td>0.695697</td>\n",
       "      <td>0.566626</td>\n",
       "      <td>spam</td>\n",
       "      <td>0.62375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>snorkel</td>\n",
       "      <td>amazon</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.565657</td>\n",
       "      <td>p7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.710674</td>\n",
       "      <td>ruler</td>\n",
       "      <td>youtube</td>\n",
       "      <td>0.736573</td>\n",
       "      <td>0.751220</td>\n",
       "      <td>p7</td>\n",
       "      <td>0.708282</td>\n",
       "      <td>0.705396</td>\n",
       "      <td>spam</td>\n",
       "      <td>0.57875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.491250</td>\n",
       "      <td>ruler</td>\n",
       "      <td>amazon</td>\n",
       "      <td>0.647619</td>\n",
       "      <td>0.614786</td>\n",
       "      <td>p0</td>\n",
       "      <td>0.598714</td>\n",
       "      <td>0.491250</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>0.63250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.539326</td>\n",
       "      <td>snorkel</td>\n",
       "      <td>youtube</td>\n",
       "      <td>0.699634</td>\n",
       "      <td>0.687225</td>\n",
       "      <td>p0</td>\n",
       "      <td>0.518884</td>\n",
       "      <td>0.478199</td>\n",
       "      <td>spam</td>\n",
       "      <td>0.27375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.515000</td>\n",
       "      <td>snorkel</td>\n",
       "      <td>amazon</td>\n",
       "      <td>0.649186</td>\n",
       "      <td>0.648276</td>\n",
       "      <td>p9</td>\n",
       "      <td>0.601146</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>0.38875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.679775</td>\n",
       "      <td>ruler</td>\n",
       "      <td>youtube</td>\n",
       "      <td>0.707692</td>\n",
       "      <td>0.769874</td>\n",
       "      <td>p9</td>\n",
       "      <td>0.676727</td>\n",
       "      <td>0.674545</td>\n",
       "      <td>spam</td>\n",
       "      <td>0.57750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.630000</td>\n",
       "      <td>ruler</td>\n",
       "      <td>amazon</td>\n",
       "      <td>0.543210</td>\n",
       "      <td>0.672727</td>\n",
       "      <td>p6</td>\n",
       "      <td>0.636876</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.912921</td>\n",
       "      <td>snorkel</td>\n",
       "      <td>youtube</td>\n",
       "      <td>0.915531</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>p6</td>\n",
       "      <td>0.911618</td>\n",
       "      <td>0.918011</td>\n",
       "      <td>spam</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.658750</td>\n",
       "      <td>snorkel</td>\n",
       "      <td>amazon</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.705394</td>\n",
       "      <td>p5</td>\n",
       "      <td>0.683448</td>\n",
       "      <td>0.658750</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>0.96500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.533708</td>\n",
       "      <td>ruler</td>\n",
       "      <td>youtube</td>\n",
       "      <td>0.695971</td>\n",
       "      <td>0.701299</td>\n",
       "      <td>p5</td>\n",
       "      <td>0.256285</td>\n",
       "      <td>0.472526</td>\n",
       "      <td>spam</td>\n",
       "      <td>0.53000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy condition  dataset        f1  max_dev_f1 participant  precision  \\\n",
       "0   0.483750   snorkel   amazon  0.562036    0.653386          p4   0.492273   \n",
       "1   0.578652     ruler  youtube  0.713740    0.632558          p4   0.682599   \n",
       "2   0.482500     ruler   amazon  0.622951    0.588235          p8   0.501770   \n",
       "3   0.789326   snorkel  youtube  0.764890    0.697436          p8   0.821485   \n",
       "4   0.506250     ruler   amazon  0.655623    0.642857          p2   0.667621   \n",
       "5   0.679775   snorkel  youtube  0.604167    0.691892          p2   0.744225   \n",
       "6   0.588750     ruler   amazon  0.636464    0.644860          p1   0.608134   \n",
       "7   0.528090   snorkel  youtube  0.596154    0.651163          p1   0.517664   \n",
       "8   0.510000   snorkel   amazon  0.565410    0.639456          p3   0.519668   \n",
       "9   0.612360     ruler  youtube  0.726190    0.750000          p3   0.695697   \n",
       "10  0.500000   snorkel   amazon  0.500000    0.565657          p7   0.000000   \n",
       "11  0.710674     ruler  youtube  0.736573    0.751220          p7   0.708282   \n",
       "12  0.491250     ruler   amazon  0.647619    0.614786          p0   0.598714   \n",
       "13  0.539326   snorkel  youtube  0.699634    0.687225          p0   0.518884   \n",
       "14  0.515000   snorkel   amazon  0.649186    0.648276          p9   0.601146   \n",
       "15  0.679775     ruler  youtube  0.707692    0.769874          p9   0.676727   \n",
       "16  0.630000     ruler   amazon  0.543210    0.672727          p6   0.636876   \n",
       "17  0.912921   snorkel  youtube  0.915531    0.947368          p6   0.911618   \n",
       "18  0.658750   snorkel   amazon  0.695652    0.705394          p5   0.683448   \n",
       "19  0.533708     ruler  youtube  0.695971    0.701299          p5   0.256285   \n",
       "\n",
       "      recall       task  training_label_coverage  \n",
       "0   0.483750  sentiment                  0.58375  \n",
       "1   0.525599       spam                  0.13500  \n",
       "2   0.482500  sentiment                  0.11250  \n",
       "3   0.809982       spam                  0.54375  \n",
       "4   0.506250  sentiment                  0.40250  \n",
       "5   0.710193       spam                  1.00000  \n",
       "6   0.588750  sentiment                  0.29750  \n",
       "7   0.512288       spam                  1.00000  \n",
       "8   0.510000  sentiment                  1.00000  \n",
       "9   0.566626       spam                  0.62375  \n",
       "10  0.000000  sentiment                  0.00000  \n",
       "11  0.705396       spam                  0.57875  \n",
       "12  0.491250  sentiment                  0.63250  \n",
       "13  0.478199       spam                  0.27375  \n",
       "14  0.515000  sentiment                  0.38875  \n",
       "15  0.674545       spam                  0.57750  \n",
       "16  0.630000  sentiment                  1.00000  \n",
       "17  0.918011       spam                  1.00000  \n",
       "18  0.658750  sentiment                  0.96500  \n",
       "19  0.472526       spam                  0.53000  "
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dt_best_small  = create_best_table_small()\n",
    "display(dt_best_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qWaGsxu-NzXM"
   },
   "source": [
    "## Figures and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t-wWpsd_bPcE"
   },
   "source": [
    "### Quantitative Figure (model performance, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vuRwkwc2N2XK"
   },
   "outputs": [],
   "source": [
    "dt_bm_small = dt_best_small.melt(id_vars=['participant', 'condition', 'task', 'dataset'], \n",
    "        var_name=\"metric\", \n",
    "        value_name=\"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355
    },
    "colab_type": "code",
    "id": "rP01XKYMN3CF",
    "outputId": "5a6822a8-4059-4104-c233-4eb298eeeec6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-bffd8a14129047b688ba1e54a7a19465\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-bffd8a14129047b688ba1e54a7a19465\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-bffd8a14129047b688ba1e54a7a19465\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-277cd1406d98bcba4045d756263dfc50\"}, \"facet\": {\"type\": \"nominal\", \"field\": \"metric\", \"sort\": [\"f1\", \"accuracy\", \"training_label_coverage\", \"max_dev_f1\"], \"title\": null}, \"spec\": {\"layer\": [{\"mark\": {\"type\": \"errorbar\", \"extent\": \"stderr\"}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"condition\", \"sort\": [\"ruler\"]}, \"x\": {\"type\": \"quantitative\", \"field\": \"value\"}, \"y\": {\"type\": \"nominal\", \"field\": \"condition\"}}, \"height\": 50, \"width\": 300}, {\"mark\": {\"type\": \"point\", \"filled\": true}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"condition\", \"legend\": {\"orient\": \"top\", \"title\": null}, \"sort\": [\"ruler\"]}, \"text\": {\"type\": \"quantitative\", \"field\": \"value\"}, \"x\": {\"type\": \"quantitative\", \"aggregate\": \"mean\", \"axis\": {\"tickCount\": 10}, \"field\": \"value\", \"title\": null}, \"y\": {\"type\": \"nominal\", \"field\": \"condition\"}}, \"height\": 50, \"width\": 300}]}, \"columns\": 2, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-277cd1406d98bcba4045d756263dfc50\": [{\"participant\": \"p4\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"accuracy\", \"value\": 0.48375}, {\"participant\": \"p4\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"accuracy\", \"value\": 0.5786516853932584}, {\"participant\": \"p8\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"accuracy\", \"value\": 0.4825}, {\"participant\": \"p8\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"accuracy\", \"value\": 0.7893258426966292}, {\"participant\": \"p2\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"accuracy\", \"value\": 0.50625}, {\"participant\": \"p2\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"accuracy\", \"value\": 0.6797752808988764}, {\"participant\": \"p1\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"accuracy\", \"value\": 0.58875}, {\"participant\": \"p1\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"accuracy\", \"value\": 0.5280898876404494}, {\"participant\": \"p3\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"accuracy\", \"value\": 0.51}, {\"participant\": \"p3\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"accuracy\", \"value\": 0.6123595505617978}, {\"participant\": \"p7\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"accuracy\", \"value\": 0.5}, {\"participant\": \"p7\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"accuracy\", \"value\": 0.7106741573033708}, {\"participant\": \"p0\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"accuracy\", \"value\": 0.49125}, {\"participant\": \"p0\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"accuracy\", \"value\": 0.5393258426966292}, {\"participant\": \"p9\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"accuracy\", \"value\": 0.515}, {\"participant\": \"p9\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"accuracy\", \"value\": 0.6797752808988764}, {\"participant\": \"p6\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"accuracy\", \"value\": 0.63}, {\"participant\": \"p6\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"accuracy\", \"value\": 0.9129213483146068}, {\"participant\": \"p5\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"accuracy\", \"value\": 0.65875}, {\"participant\": \"p5\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"accuracy\", \"value\": 0.5337078651685393}, {\"participant\": \"p4\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"f1\", \"value\": 0.5620360551431601}, {\"participant\": \"p4\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"f1\", \"value\": 0.7137404580152672}, {\"participant\": \"p8\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"f1\", \"value\": 0.6229508196721313}, {\"participant\": \"p8\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"f1\", \"value\": 0.7648902821316614}, {\"participant\": \"p2\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"f1\", \"value\": 0.6556233653007847}, {\"participant\": \"p2\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"f1\", \"value\": 0.6041666666666666}, {\"participant\": \"p1\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"f1\", \"value\": 0.6364640883977901}, {\"participant\": \"p1\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"f1\", \"value\": 0.5961538461538463}, {\"participant\": \"p3\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"f1\", \"value\": 0.5654101995565409}, {\"participant\": \"p3\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"f1\", \"value\": 0.7261904761904762}, {\"participant\": \"p7\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"f1\", \"value\": 0.5}, {\"participant\": \"p7\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"f1\", \"value\": 0.7365728900255755}, {\"participant\": \"p0\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"f1\", \"value\": 0.6476190476190475}, {\"participant\": \"p0\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"f1\", \"value\": 0.6996336996336996}, {\"participant\": \"p9\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"f1\", \"value\": 0.6491862567811935}, {\"participant\": \"p9\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"f1\", \"value\": 0.7076923076923077}, {\"participant\": \"p6\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"f1\", \"value\": 0.5432098765432098}, {\"participant\": \"p6\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"f1\", \"value\": 0.9155313351498636}, {\"participant\": \"p5\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"f1\", \"value\": 0.6956521739130435}, {\"participant\": \"p5\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"f1\", \"value\": 0.6959706959706959}, {\"participant\": \"p4\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"max_dev_f1\", \"value\": 0.653386454183267}, {\"participant\": \"p4\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"max_dev_f1\", \"value\": 0.6325581395348836}, {\"participant\": \"p8\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"max_dev_f1\", \"value\": 0.5882352941176471}, {\"participant\": \"p8\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"max_dev_f1\", \"value\": 0.6974358974358974}, {\"participant\": \"p2\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"max_dev_f1\", \"value\": 0.6428571428571429}, {\"participant\": \"p2\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"max_dev_f1\", \"value\": 0.6918918918918918}, {\"participant\": \"p1\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"max_dev_f1\", \"value\": 0.6448598130841121}, {\"participant\": \"p1\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"max_dev_f1\", \"value\": 0.6511627906976744}, {\"participant\": \"p3\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"max_dev_f1\", \"value\": 0.6394557823129251}, {\"participant\": \"p3\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"max_dev_f1\", \"value\": 0.7500000000000001}, {\"participant\": \"p7\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"max_dev_f1\", \"value\": 0.5656565657}, {\"participant\": \"p7\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"max_dev_f1\", \"value\": 0.7512195121951221}, {\"participant\": \"p0\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"max_dev_f1\", \"value\": 0.6147859922178989}, {\"participant\": \"p0\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"max_dev_f1\", \"value\": 0.6872246696035242}, {\"participant\": \"p9\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"max_dev_f1\", \"value\": 0.6482758620689655}, {\"participant\": \"p9\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"max_dev_f1\", \"value\": 0.7698744769874477}, {\"participant\": \"p6\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"max_dev_f1\", \"value\": 0.6727272727272727}, {\"participant\": \"p6\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"max_dev_f1\", \"value\": 0.9473684210526316}, {\"participant\": \"p5\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"max_dev_f1\", \"value\": 0.7053941908713692}, {\"participant\": \"p5\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"max_dev_f1\", \"value\": 0.7012987012987013}, {\"participant\": \"p4\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"precision\", \"value\": 0.4922725221697655}, {\"participant\": \"p4\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"precision\", \"value\": 0.6825991465863452}, {\"participant\": \"p8\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"precision\", \"value\": 0.5017700851290237}, {\"participant\": \"p8\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"precision\", \"value\": 0.8214846301963346}, {\"participant\": \"p2\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"precision\", \"value\": 0.6676213818860878}, {\"participant\": \"p2\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"precision\", \"value\": 0.7442247596153845}, {\"participant\": \"p1\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"precision\", \"value\": 0.6081340813551366}, {\"participant\": \"p1\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"precision\", \"value\": 0.5176636904761905}, {\"participant\": \"p3\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"precision\", \"value\": 0.5196675824175824}, {\"participant\": \"p3\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"precision\", \"value\": 0.6956971153846154}, {\"participant\": \"p7\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"precision\", \"value\": 0.0}, {\"participant\": \"p7\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"precision\", \"value\": 0.7082824152610185}, {\"participant\": \"p0\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"precision\", \"value\": 0.5987136457285228}, {\"participant\": \"p0\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"precision\", \"value\": 0.5188841807909604}, {\"participant\": \"p9\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"precision\", \"value\": 0.601146408839779}, {\"participant\": \"p9\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"precision\", \"value\": 0.6767270809359417}, {\"participant\": \"p6\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"precision\", \"value\": 0.6368761792120751}, {\"participant\": \"p6\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"precision\", \"value\": 0.9116182320441989}, {\"participant\": \"p5\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"precision\", \"value\": 0.6834482200647249}, {\"participant\": \"p5\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"precision\", \"value\": 0.2562853107344633}, {\"participant\": \"p4\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"recall\", \"value\": 0.48375}, {\"participant\": \"p4\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"recall\", \"value\": 0.5255986407520326}, {\"participant\": \"p8\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"recall\", \"value\": 0.4825}, {\"participant\": \"p8\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"recall\", \"value\": 0.8099815802845528}, {\"participant\": \"p2\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"recall\", \"value\": 0.5062500000000001}, {\"participant\": \"p2\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"recall\", \"value\": 0.710193407012195}, {\"participant\": \"p1\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"recall\", \"value\": 0.58875}, {\"participant\": \"p1\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"recall\", \"value\": 0.512287855691057}, {\"participant\": \"p3\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"recall\", \"value\": 0.51}, {\"participant\": \"p3\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"recall\", \"value\": 0.5666263338414634}, {\"participant\": \"p7\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"recall\", \"value\": 0.0}, {\"participant\": \"p7\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"recall\", \"value\": 0.7053963414634146}, {\"participant\": \"p0\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"recall\", \"value\": 0.49125}, {\"participant\": \"p0\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"recall\", \"value\": 0.4781989964430894}, {\"participant\": \"p9\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"recall\", \"value\": 0.515}, {\"participant\": \"p9\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"recall\", \"value\": 0.674544588414634}, {\"participant\": \"p6\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"recall\", \"value\": 0.6299999999999999}, {\"participant\": \"p6\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"recall\", \"value\": 0.9180106707317072}, {\"participant\": \"p5\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"recall\", \"value\": 0.65875}, {\"participant\": \"p5\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"recall\", \"value\": 0.47252604166666673}, {\"participant\": \"p4\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"training_label_coverage\", \"value\": 0.58375}, {\"participant\": \"p4\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"training_label_coverage\", \"value\": 0.135}, {\"participant\": \"p8\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"training_label_coverage\", \"value\": 0.1125}, {\"participant\": \"p8\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"training_label_coverage\", \"value\": 0.54375}, {\"participant\": \"p2\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"training_label_coverage\", \"value\": 0.4025}, {\"participant\": \"p2\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"training_label_coverage\", \"value\": 1.0}, {\"participant\": \"p1\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"training_label_coverage\", \"value\": 0.2975}, {\"participant\": \"p1\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"training_label_coverage\", \"value\": 1.0}, {\"participant\": \"p3\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"training_label_coverage\", \"value\": 1.0}, {\"participant\": \"p3\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"training_label_coverage\", \"value\": 0.62375}, {\"participant\": \"p7\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"training_label_coverage\", \"value\": 0.0}, {\"participant\": \"p7\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"training_label_coverage\", \"value\": 0.57875}, {\"participant\": \"p0\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"training_label_coverage\", \"value\": 0.6325}, {\"participant\": \"p0\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"training_label_coverage\", \"value\": 0.27375}, {\"participant\": \"p9\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"training_label_coverage\", \"value\": 0.38875}, {\"participant\": \"p9\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"training_label_coverage\", \"value\": 0.5775}, {\"participant\": \"p6\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"training_label_coverage\", \"value\": 1.0}, {\"participant\": \"p6\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"training_label_coverage\", \"value\": 1.0}, {\"participant\": \"p5\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"training_label_coverage\", \"value\": 0.965}, {\"participant\": \"p5\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"training_label_coverage\", \"value\": 0.53}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.FacetChart(...)"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = 300\n",
    "H = 50\n",
    "error_bars = alt.Chart(dt_bm_small).mark_errorbar(extent='stderr').encode(\n",
    "  x=alt.X('value:Q'),\n",
    "  y=alt.Y('condition:N'),\n",
    "  color=alt.Color('condition:N', sort=['ruler'])\n",
    ").properties(width=W,height=H)\n",
    "\n",
    "points = alt.Chart(dt_bm_small).mark_point(filled=True).encode(\n",
    "  x=alt.X('value:Q', title=None, aggregate='mean', axis=alt.Axis(tickCount=10)),\n",
    "  y=alt.Y('condition:N'),\n",
    "  text=alt.Text('value:Q'),\n",
    "  color=alt.Color('condition:N', sort=['ruler'], legend=alt.Legend(title=None, orient='top'))\n",
    ").properties(width=W,height=H)\n",
    "\n",
    "(error_bars + points).facet(\n",
    "    facet= alt.Facet('metric:N', sort=['f1', 'accuracy', 'training_label_coverage', 'max_dev_f1'], title=None),\n",
    "    columns=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qsri_wdMJUYE"
   },
   "source": [
    "### Hypothesis Testing\n",
    "\n",
    "Let's see which of these differences are statistically significant, starting with the f1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I5U8bb97Uxts"
   },
   "source": [
    "#### F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RFZj-oj_JQeK",
    "outputId": "08dcb8f3-4bec-4cc7-9852-2082d2c0728b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_relResult(statistic=-0.518186116980372, pvalue=0.6168254933363918)"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "dt = dt_best_small\n",
    "ruler_f1 = dt[dt['condition']=='ruler']['accuracy']\n",
    "snorkel_f1 = dt[dt['condition']=='snorkel']['accuracy']\n",
    "stats.ttest_rel(ruler_f1, snorkel_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_yk4qdPiJth8"
   },
   "source": [
    "As  the figure suggested, the difference for f1 scores is not significant (**pvalue = 0.62 >> 0.05**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yg0p4o1RMBMV"
   },
   "source": [
    "For posterity, let's perform the above comparison using a mixed effects model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "zvWww3woKE9p",
    "outputId": "1d235267-bd94-466f-9651-fc7c73136913"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/rpy2/robjects/pandas2ri.py:14: FutureWarning: pandas.core.index is deprecated and will be removed in a future version.  The public classes are available in the top-level namespace.\n",
      "  from pandas.core.index import Index as PandasIndex\n",
      "/usr/local/lib/python3.6/dist-packages/rpy2/robjects/pandas2ri.py:34: UserWarning: pandas >= 1.0 is not supported.\n",
      "  warnings.warn('pandas >= 1.0 is not supported.')\n"
     ]
    }
   ],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "iULKUOx8KVcJ",
    "outputId": "d7174266-7e1f-4519-ea47-e9424ed5db33"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Installing package into /usr/local/lib/R/site-library\n",
      "(as lib is unspecified)\n",
      "\n",
      "R[write to console]: also installing the dependencies minqa, nloptr, statmod, RcppEigen\n",
      "\n",
      "\n",
      "R[write to console]: trying URL 'https://cran.rstudio.com/src/contrib/minqa_1.2.4.tar.gz'\n",
      "\n",
      "R[write to console]: Content type 'application/x-gzip'\n",
      "R[write to console]:  length 53548 bytes (52 KB)\n",
      "\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: \n",
      "\n",
      "R[write to console]: downloaded 52 KB\n",
      "\n",
      "\n",
      "R[write to console]: trying URL 'https://cran.rstudio.com/src/contrib/nloptr_1.2.2.1.tar.gz'\n",
      "\n",
      "R[write to console]: Content type 'application/x-gzip'\n",
      "R[write to console]:  length 2450984 bytes (2.3 MB)\n",
      "\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: \n",
      "\n",
      "R[write to console]: downloaded 2.3 MB\n",
      "\n",
      "\n",
      "R[write to console]: trying URL 'https://cran.rstudio.com/src/contrib/statmod_1.4.34.tar.gz'\n",
      "\n",
      "R[write to console]: Content type 'application/x-gzip'\n",
      "R[write to console]:  length 61829 bytes (60 KB)\n",
      "\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: \n",
      "\n",
      "R[write to console]: downloaded 60 KB\n",
      "\n",
      "\n",
      "R[write to console]: trying URL 'https://cran.rstudio.com/src/contrib/RcppEigen_0.3.3.7.0.tar.gz'\n",
      "\n",
      "R[write to console]: Content type 'application/x-gzip'\n",
      "R[write to console]:  length 1643103 bytes (1.6 MB)\n",
      "\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: \n",
      "\n",
      "R[write to console]: downloaded 1.6 MB\n",
      "\n",
      "\n",
      "R[write to console]: trying URL 'https://cran.rstudio.com/src/contrib/lme4_1.1-23.tar.gz'\n",
      "\n",
      "R[write to console]: Content type 'application/x-gzip'\n",
      "R[write to console]:  length 4137774 bytes (3.9 MB)\n",
      "\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: \n",
      "\n",
      "R[write to console]: downloaded 3.9 MB\n",
      "\n",
      "\n",
      "R[write to console]: \n",
      "\n",
      "R[write to console]: \n",
      "R[write to console]: The downloaded source packages are in\n",
      "\t/tmp/RtmpEL7Te1/downloaded_packages\n",
      "R[write to console]: \n",
      "R[write to console]: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "install.packages(\"lme4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "rvRkrwRhKSpB",
    "outputId": "7162e4ef-72a2-4d92-8388-1fab6cde2a07"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Loading required package: Matrix\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%R library(lme4)\n",
    "%R -i dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yxOrWLGwKjsd"
   },
   "source": [
    "Use linear mixed-effects (LME) regression to analyze the effect of **condition** on **f1** (see, e.g., https://web.stanford.edu/class/psych252/section/Mixed_models_tutorial.html, https://jontalle.web.engr.illinois.edu/MISC/lme4/bw_LME_tutorial.pdf). The main difference between the LME model below and the paired t-test model above is the LME model  takes  the differences among users, e.g., due to experience, familiarity, etc., and among the task types into consideration (the `(1|participant)` and `(1 | task)` parts). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "UpW8Bz9wKjKV",
    "outputId": "4101df4f-dc1c-4481-ceab-1e4fca1ed562"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: boundary (singular) fit: see ?isSingular\n",
      "\n",
      "R[write to console]: boundary (singular) fit: see ?isSingular\n",
      "\n",
      "R[write to console]: refitting model(s) with ML (instead of REML)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: dt\n",
      "Models:\n",
      "compact: f1 ~ 1 + (1 | participant) + (1 | task)\n",
      "augmented: f1 ~ condition + (1 | participant) + (1 | task)\n",
      "          npar     AIC     BIC logLik deviance Chisq Df Pr(>Chisq)\n",
      "compact      4 -35.005 -31.022 21.502  -43.005                    \n",
      "augmented    5 -33.159 -28.180 21.579  -43.159 0.154  1     0.6947\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "# the first model suggests the f1 scores can be modeled as a linear function of \n",
    "# a constant, per-subject random effects, per-task random effects, and a measurement noise \n",
    "compact = lmer('f1 ~ 1 + (1|participant) + (1|task)', data=dt) \n",
    "\n",
    "# the second model suggests the f1 scores can be  modeled as a linear function of \n",
    "# a constant, the value of condition (fixed effect), per-subject random effects, per-task random effects, and a measurement noise \n",
    "augmented = lmer('f1 ~ condition + (1|participant) + (1|task)', data=dt)\n",
    "\n",
    "# So, does one model explain the data better than the other? We can compare the two models using the $\\chi^2$ test \n",
    "anova(compact, augmented)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4AyVIqAPKsj2"
   },
   "source": [
    "If we look at the last column for the second row (augmented), similar to the paired t-test performed earlier, we can see the difference between these two models is not significant (**pvalue=0.69 >> 0.05**). So far, **ruler** and **snorkel** have no statistically  significant performance difference as measured by the **f1** score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RlSR4KzyLGP0"
   },
   "source": [
    "Let's repeat the paired t-test also in R to further verify our conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "colab_type": "code",
    "id": "0KBBTFFCLKwk",
    "outputId": "d98ec6ae-e478-408e-9f55-c225806307f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tPaired t-test\n",
      "\n",
      "data:  dt$f1[dt$condition == \"ruler\"] and dt$f1[dt$condition == \"snorkel\"]\n",
      "t = 0.24251, df = 9, p-value = 0.8138\n",
      "alternative hypothesis: true difference in means is not equal to 0\n",
      "95 percent confidence interval:\n",
      " -0.1110753  0.1377500\n",
      "sample estimates:\n",
      "mean of the differences \n",
      "             0.01333735 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "t.test(dt$f1[dt$condition==\"ruler\"],\n",
    "       dt$f1[dt$condition==\"snorkel\"],\n",
    "       alternative = \"two.sided\",\n",
    "       paired = T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3QBRGtaMMW6I"
   },
   "source": [
    "Before moving on, let's calculate the effect size (https://en.wikipedia.org/wiki/Effect_size). For that, we can use one of many R packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "c5Tnh37OMajt",
    "outputId": "f0ee7245-ef78-4aa5-f9a9-a628131e1bfb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Installing package into /usr/local/lib/R/site-library\n",
      "(as lib is unspecified)\n",
      "\n",
      "R[write to console]: also installing the dependencies zip, SparseM, MatrixModels, sp, data.table, openxlsx, carData, abind, pbkrtest, quantreg, maptools, rio, corrplot, car\n",
      "\n",
      "\n",
      "R[write to console]: trying URL 'https://cran.rstudio.com/src/contrib/zip_2.0.4.tar.gz'\n",
      "\n",
      "R[write to console]: Content type 'application/x-gzip'\n",
      "R[write to console]:  length 97756 bytes (95 KB)\n",
      "\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: \n",
      "\n",
      "R[write to console]: downloaded 95 KB\n",
      "\n",
      "\n",
      "R[write to console]: trying URL 'https://cran.rstudio.com/src/contrib/SparseM_1.78.tar.gz'\n",
      "\n",
      "R[write to console]: Content type 'application/x-gzip'\n",
      "R[write to console]:  length 735024 bytes (717 KB)\n",
      "\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: \n",
      "\n",
      "R[write to console]: downloaded 717 KB\n",
      "\n",
      "\n",
      "R[write to console]: trying URL 'https://cran.rstudio.com/src/contrib/MatrixModels_0.4-1.tar.gz'\n",
      "\n",
      "R[write to console]: Content type 'application/x-gzip'\n",
      "R[write to console]:  length 23065 bytes (22 KB)\n",
      "\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: \n",
      "\n",
      "R[write to console]: downloaded 22 KB\n",
      "\n",
      "\n",
      "R[write to console]: trying URL 'https://cran.rstudio.com/src/contrib/sp_1.4-2.tar.gz'\n",
      "\n",
      "R[write to console]: Content type 'application/x-gzip'\n",
      "R[write to console]:  length 1135318 bytes (1.1 MB)\n",
      "\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: \n",
      "\n",
      "R[write to console]: downloaded 1.1 MB\n",
      "\n",
      "\n",
      "R[write to console]: trying URL 'https://cran.rstudio.com/src/contrib/data.table_1.12.8.tar.gz'\n",
      "\n",
      "R[write to console]: Content type 'application/x-gzip'\n",
      "R[write to console]:  length 4948391 bytes (4.7 MB)\n",
      "\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: \n",
      "\n",
      "R[write to console]: downloaded 4.7 MB\n",
      "\n",
      "\n",
      "R[write to console]: trying URL 'https://cran.rstudio.com/src/contrib/openxlsx_4.1.5.tar.gz'\n",
      "\n",
      "R[write to console]: Content type 'application/x-gzip'\n",
      "R[write to console]:  length 1388952 bytes (1.3 MB)\n",
      "\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: \n",
      "\n",
      "R[write to console]: downloaded 1.3 MB\n",
      "\n",
      "\n",
      "R[write to console]: trying URL 'https://cran.rstudio.com/src/contrib/carData_3.0-4.tar.gz'\n",
      "\n",
      "R[write to console]: Content type 'application/x-gzip'\n",
      "R[write to console]:  length 993536 bytes (970 KB)\n",
      "\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: \n",
      "\n",
      "R[write to console]: downloaded 970 KB\n",
      "\n",
      "\n",
      "R[write to console]: trying URL 'https://cran.rstudio.com/src/contrib/abind_1.4-5.tar.gz'\n",
      "\n",
      "R[write to console]: Content type 'application/x-gzip'\n",
      "R[write to console]:  length 21810 bytes (21 KB)\n",
      "\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: \n",
      "\n",
      "R[write to console]: downloaded 21 KB\n",
      "\n",
      "\n",
      "R[write to console]: trying URL 'https://cran.rstudio.com/src/contrib/pbkrtest_0.4-8.6.tar.gz'\n",
      "\n",
      "R[write to console]: Content type 'application/x-gzip'\n",
      "R[write to console]:  length 174962 bytes (170 KB)\n",
      "\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: \n",
      "\n",
      "R[write to console]: downloaded 170 KB\n",
      "\n",
      "\n",
      "R[write to console]: trying URL 'https://cran.rstudio.com/src/contrib/quantreg_5.55.tar.gz'\n",
      "\n",
      "R[write to console]: Content type 'application/x-gzip'\n",
      "R[write to console]:  length 995519 bytes (972 KB)\n",
      "\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: \n",
      "\n",
      "R[write to console]: downloaded 972 KB\n",
      "\n",
      "\n",
      "R[write to console]: trying URL 'https://cran.rstudio.com/src/contrib/maptools_1.0-1.tar.gz'\n",
      "\n",
      "R[write to console]: Content type 'application/x-gzip'\n",
      "R[write to console]:  length 1584087 bytes (1.5 MB)\n",
      "\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: \n",
      "\n",
      "R[write to console]: downloaded 1.5 MB\n",
      "\n",
      "\n",
      "R[write to console]: trying URL 'https://cran.rstudio.com/src/contrib/rio_0.5.16.tar.gz'\n",
      "\n",
      "R[write to console]: Content type 'application/x-gzip'\n",
      "R[write to console]:  length 420489 bytes (410 KB)\n",
      "\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: \n",
      "\n",
      "R[write to console]: downloaded 410 KB\n",
      "\n",
      "\n",
      "R[write to console]: trying URL 'https://cran.rstudio.com/src/contrib/corrplot_0.84.tar.gz'\n",
      "\n",
      "R[write to console]: Content type 'application/x-gzip'\n",
      "R[write to console]:  length 5385275 bytes (5.1 MB)\n",
      "\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: \n",
      "\n",
      "R[write to console]: downloaded 5.1 MB\n",
      "\n",
      "\n",
      "R[write to console]: trying URL 'https://cran.rstudio.com/src/contrib/car_3.0-8.tar.gz'\n",
      "\n",
      "R[write to console]: Content type 'application/x-gzip'\n",
      "R[write to console]:  length 499932 bytes (488 KB)\n",
      "\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: \n",
      "\n",
      "R[write to console]: downloaded 488 KB\n",
      "\n",
      "\n",
      "R[write to console]: trying URL 'https://cran.rstudio.com/src/contrib/rstatix_0.5.0.tar.gz'\n",
      "\n",
      "R[write to console]: Content type 'application/x-gzip'\n",
      "R[write to console]:  length 391492 bytes (382 KB)\n",
      "\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: \n",
      "\n",
      "R[write to console]: downloaded 382 KB\n",
      "\n",
      "\n",
      "R[write to console]: \n",
      "\n",
      "R[write to console]: \n",
      "R[write to console]: The downloaded source packages are in\n",
      "\t/tmp/RtmpEL7Te1/downloaded_packages\n",
      "R[write to console]: \n",
      "R[write to console]: \n",
      "\n",
      "R[write to console]: Installing package into /usr/local/lib/R/site-library\n",
      "(as lib is unspecified)\n",
      "\n",
      "R[write to console]: also installing the dependencies zoo, TH.data, sandwich, libcoin, matrixStats, modeltools, mvtnorm, multcomp\n",
      "\n",
      "\n",
      "R[write to console]: trying URL 'https://cran.rstudio.com/src/contrib/zoo_1.8-8.tar.gz'\n",
      "\n",
      "R[write to console]: Content type 'application/x-gzip'\n",
      "R[write to console]:  length 849487 bytes (829 KB)\n",
      "\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: \n",
      "\n",
      "R[write to console]: downloaded 829 KB\n",
      "\n",
      "\n",
      "R[write to console]: trying URL 'https://cran.rstudio.com/src/contrib/TH.data_1.0-10.tar.gz'\n",
      "\n",
      "R[write to console]: Content type 'application/x-gzip'\n",
      "R[write to console]:  length 8238275 bytes (7.9 MB)\n",
      "\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: \n",
      "\n",
      "R[write to console]: downloaded 7.9 MB\n",
      "\n",
      "\n",
      "R[write to console]: trying URL 'https://cran.rstudio.com/src/contrib/sandwich_2.5-1.tar.gz'\n",
      "\n",
      "R[write to console]: Content type 'application/x-gzip'\n",
      "R[write to console]:  length 1349536 bytes (1.3 MB)\n",
      "\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: \n",
      "\n",
      "R[write to console]: downloaded 1.3 MB\n",
      "\n",
      "\n",
      "R[write to console]: trying URL 'https://cran.rstudio.com/src/contrib/libcoin_1.0-5.tar.gz'\n",
      "\n",
      "R[write to console]: Content type 'application/x-gzip'\n",
      "R[write to console]:  length 779010 bytes (760 KB)\n",
      "\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: \n",
      "\n",
      "R[write to console]: downloaded 760 KB\n",
      "\n",
      "\n",
      "R[write to console]: trying URL 'https://cran.rstudio.com/src/contrib/matrixStats_0.56.0.tar.gz'\n",
      "\n",
      "R[write to console]: Content type 'application/x-gzip'\n",
      "R[write to console]:  length 188501 bytes (184 KB)\n",
      "\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: \n",
      "\n",
      "R[write to console]: downloaded 184 KB\n",
      "\n",
      "\n",
      "R[write to console]: trying URL 'https://cran.rstudio.com/src/contrib/modeltools_0.2-23.tar.gz'\n",
      "\n",
      "R[write to console]: Content type 'application/x-gzip'\n",
      "R[write to console]:  length 14893 bytes (14 KB)\n",
      "\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: \n",
      "\n",
      "R[write to console]: downloaded 14 KB\n",
      "\n",
      "\n",
      "R[write to console]: trying URL 'https://cran.rstudio.com/src/contrib/mvtnorm_1.1-1.tar.gz'\n",
      "\n",
      "R[write to console]: Content type 'application/x-gzip'\n",
      "R[write to console]:  length 163902 bytes (160 KB)\n",
      "\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: \n",
      "\n",
      "R[write to console]: downloaded 160 KB\n",
      "\n",
      "\n",
      "R[write to console]: trying URL 'https://cran.rstudio.com/src/contrib/multcomp_1.4-13.tar.gz'\n",
      "\n",
      "R[write to console]: Content type 'application/x-gzip'\n",
      "R[write to console]:  length 609725 bytes (595 KB)\n",
      "\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: \n",
      "\n",
      "R[write to console]: downloaded 595 KB\n",
      "\n",
      "\n",
      "R[write to console]: trying URL 'https://cran.rstudio.com/src/contrib/coin_1.3-1.tar.gz'\n",
      "\n",
      "R[write to console]: Content type 'application/x-gzip'\n",
      "R[write to console]:  length 1263884 bytes (1.2 MB)\n",
      "\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: \n",
      "\n",
      "R[write to console]: downloaded 1.2 MB\n",
      "\n",
      "\n",
      "R[write to console]: \n",
      "\n",
      "R[write to console]: \n",
      "R[write to console]: The downloaded source packages are in\n",
      "\t/tmp/RtmpEL7Te1/downloaded_packages\n",
      "R[write to console]: \n",
      "R[write to console]: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "install.packages(\"rstatix\")\n",
    "install.packages('coin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "colab_type": "code",
    "id": "25zBe0-fMemp",
    "outputId": "63184ab9-fd5d-4589-8d99-ad6911e3c7b9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Registered S3 methods overwritten by 'car':\n",
      "  method                          from\n",
      "  influence.merMod                lme4\n",
      "  cooks.distance.influence.merMod lme4\n",
      "  dfbeta.influence.merMod         lme4\n",
      "  dfbetas.influence.merMod        lme4\n",
      "\n",
      "R[write to console]: \n",
      "Attaching package: rstatix\n",
      "\n",
      "\n",
      "R[write to console]: The following object is masked from package:stats:\n",
      "\n",
      "    filter\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# A tibble: 1 x 7\n",
      "  .y.      group1 group2  effsize    n1    n2 magnitude \n",
      "* <chr>    <chr>  <chr>     <dbl> <int> <int> <ord>     \n",
      "1 accuracy ruler  snorkel  -0.164    10    10 negligible\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "library(rstatix)\n",
    "cohens_d(dt, accuracy ~ condition, paired=T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qNe3cT_7MnYT"
   },
   "source": [
    "Wilcoxon tests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 776
    },
    "colab_type": "code",
    "id": "1T2vrDNlMi_j",
    "outputId": "ef5920bb-604a-4924-f105-02651f49e9cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tWilcoxon signed rank test\n",
      "\n",
      "data:  dt$f1[dt$condition == \"ruler\"] and dt$f1[dt$condition == \"snorkel\"]\n",
      "V = 35, p-value = 0.4922\n",
      "alternative hypothesis: true location shift is not equal to 0\n",
      "\n",
      "# A tibble: 1 x 7\n",
      "  .y.   group1 group2  effsize    n1    n2 magnitude\n",
      "* <chr> <chr>  <chr>     <dbl> <int> <int> <ord>    \n",
      "1 f1    ruler  snorkel   0.242    10    10 small    \n",
      "\n",
      "\tWilcoxon signed rank test\n",
      "\n",
      "data:  dt$precision[dt$condition == \"ruler\"] and dt$precision[dt$condition == \"snorkel\"]\n",
      "V = 29, p-value = 0.9219\n",
      "alternative hypothesis: true location shift is not equal to 0\n",
      "\n",
      "# A tibble: 1 x 7\n",
      "  .y.       group1 group2  effsize    n1    n2 magnitude\n",
      "* <chr>     <chr>  <chr>     <dbl> <int> <int> <ord>    \n",
      "1 precision ruler  snorkel  0.0483    10    10 small    \n",
      "\n",
      "\tWilcoxon signed rank test\n",
      "\n",
      "data:  dt$recall[dt$condition == \"ruler\"] and dt$recall[dt$condition == \"snorkel\"]\n",
      "V = 25, p-value = 0.8457\n",
      "alternative hypothesis: true location shift is not equal to 0\n",
      "\n",
      "# A tibble: 1 x 7\n",
      "  .y.    group1 group2  effsize    n1    n2 magnitude\n",
      "* <chr>  <chr>  <chr>     <dbl> <int> <int> <ord>    \n",
      "1 recall ruler  snorkel  0.0806    10    10 small    \n",
      "\n",
      "\tWilcoxon signed rank test\n",
      "\n",
      "data:  dt$accuracy[dt$condition == \"ruler\"] and dt$f1[dt$condition == \"snorkel\"]\n",
      "V = 17, p-value = 0.3223\n",
      "alternative hypothesis: true location shift is not equal to 0\n",
      "\n",
      "# A tibble: 1 x 7\n",
      "  .y.      group1 group2  effsize    n1    n2 magnitude\n",
      "* <chr>    <chr>  <chr>     <dbl> <int> <int> <ord>    \n",
      "1 accuracy ruler  snorkel   0.145    10    10 small    \n"
     ]
    }
   ],
   "source": [
    " %%R \n",
    "\n",
    "# f1 \n",
    "print(wilcox.test(dt$f1[dt$condition==\"ruler\"],dt$f1[dt$condition==\"snorkel\"] , paired = TRUE))\n",
    "print(wilcox_effsize(dt, f1~condition, paired=T))\n",
    "\n",
    "# precision \n",
    "print(wilcox.test(dt$precision[dt$condition==\"ruler\"],dt$precision[dt$condition==\"snorkel\"] , paired = TRUE))\n",
    "print(wilcox_effsize(dt, precision~condition, paired=T))\n",
    "\n",
    "# recall \n",
    "print(wilcox.test(dt$recall[dt$condition==\"ruler\"],dt$recall[dt$condition==\"snorkel\"] , paired = TRUE))\n",
    "print(wilcox_effsize(dt, recall~condition, paired=T))\n",
    "\n",
    "# accuracy \n",
    "print(wilcox.test(dt$accuracy[dt$condition==\"ruler\"],dt$f1[dt$condition==\"snorkel\"] , paired = TRUE))\n",
    "print(wilcox_effsize(dt, accuracy~condition, paired=T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hTGkREUBM1Qa"
   },
   "source": [
    "The effect size for our paired-t test is **small**. As mentioned earlier, the **f1** values for both  conditions have high variance, which in turn causes high variance in differences. A paired test will have a large effect if the average difference between paired values is high while their variance is low. \n",
    "\n",
    "In fact, the effect size computation based on Cohen's d measure for a paired t-test is  relatively simple: $d=\\frac{\\text{mean of paired differences}}{\\text{std of paired differences}}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mKmM-C6FM3i8"
   },
   "outputs": [],
   "source": [
    " # our implementation \n",
    "def cohensd(g1, g2): \n",
    "  return  np.mean(g1-g2) / np.std(g1-g2, ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rfv2hn-nM5GP",
    "outputId": "8cac5b62-660e-471e-d2e6-d909bb8eb8b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1638648381536429"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohensd(ruler_f1.values, snorkel_f1.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hpdbEmdrM8se"
   },
   "source": [
    "Great, we got the same value as the R `rstatix` package's `cohens_d()` function. \n",
    "\n",
    "One remaining question is how to interpret the effect size values. The answer is _depends_ but in general the effect size is assumed to be  $\\left \\{ \n",
    "  \\begin{array}{ll} \n",
    "  \\text{small} & \\text{if $d\\sim 0.2$} \\\\\n",
    "  \\text{moderate} & \\text{if $d\\sim 0.5$} \\\\\n",
    "  \\text{large} & \\text{if $d\\sim 0.8$} \\\\\n",
    "  \\end{array}\n",
    "  \\right.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dn4S_ZbiM_-6"
   },
   "source": [
    "Now, let's repeat the significance analysis for the other metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "txrOMQ5mOr-9"
   },
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "colab_type": "code",
    "id": "832UbZmeNjN0",
    "outputId": "6f858b53-46f2-429b-9f9f-8b162a54770d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-2b1a10c864164b4da5e86c881b80a9a2\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-2b1a10c864164b4da5e86c881b80a9a2\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-2b1a10c864164b4da5e86c881b80a9a2\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"layer\": [{\"mark\": {\"type\": \"errorbar\", \"extent\": \"stderr\"}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"condition\", \"sort\": [\"ruler\"]}, \"x\": {\"type\": \"quantitative\", \"field\": \"precision\"}, \"y\": {\"type\": \"nominal\", \"field\": \"condition\"}}}, {\"mark\": {\"type\": \"point\", \"filled\": true}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"condition\", \"sort\": [\"ruler\"]}, \"x\": {\"type\": \"quantitative\", \"aggregate\": \"mean\", \"field\": \"precision\"}, \"y\": {\"type\": \"nominal\", \"field\": \"condition\"}}}], \"data\": {\"name\": \"data-087497bff17fdcb684f0b14eb7cac74f\"}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-087497bff17fdcb684f0b14eb7cac74f\": [{\"accuracy\": 0.48375, \"condition\": \"snorkel\", \"dataset\": \"amazon\", \"f1\": 0.5620360551431601, \"max_dev_f1\": 0.653386454183267, \"participant\": \"p4\", \"precision\": 0.4922725221697655, \"recall\": 0.48375, \"task\": \"sentiment\", \"training_label_coverage\": 0.58375}, {\"accuracy\": 0.5786516853932584, \"condition\": \"ruler\", \"dataset\": \"youtube\", \"f1\": 0.7137404580152672, \"max_dev_f1\": 0.6325581395348836, \"participant\": \"p4\", \"precision\": 0.6825991465863452, \"recall\": 0.5255986407520326, \"task\": \"spam\", \"training_label_coverage\": 0.135}, {\"accuracy\": 0.4825, \"condition\": \"ruler\", \"dataset\": \"amazon\", \"f1\": 0.6229508196721313, \"max_dev_f1\": 0.5882352941176471, \"participant\": \"p8\", \"precision\": 0.5017700851290237, \"recall\": 0.4825, \"task\": \"sentiment\", \"training_label_coverage\": 0.1125}, {\"accuracy\": 0.7893258426966292, \"condition\": \"snorkel\", \"dataset\": \"youtube\", \"f1\": 0.7648902821316614, \"max_dev_f1\": 0.6974358974358974, \"participant\": \"p8\", \"precision\": 0.8214846301963346, \"recall\": 0.8099815802845528, \"task\": \"spam\", \"training_label_coverage\": 0.54375}, {\"accuracy\": 0.50625, \"condition\": \"ruler\", \"dataset\": \"amazon\", \"f1\": 0.6556233653007847, \"max_dev_f1\": 0.6428571428571429, \"participant\": \"p2\", \"precision\": 0.6676213818860878, \"recall\": 0.5062500000000001, \"task\": \"sentiment\", \"training_label_coverage\": 0.4025}, {\"accuracy\": 0.6797752808988764, \"condition\": \"snorkel\", \"dataset\": \"youtube\", \"f1\": 0.6041666666666666, \"max_dev_f1\": 0.6918918918918918, \"participant\": \"p2\", \"precision\": 0.7442247596153845, \"recall\": 0.710193407012195, \"task\": \"spam\", \"training_label_coverage\": 1.0}, {\"accuracy\": 0.58875, \"condition\": \"ruler\", \"dataset\": \"amazon\", \"f1\": 0.6364640883977901, \"max_dev_f1\": 0.6448598130841121, \"participant\": \"p1\", \"precision\": 0.6081340813551366, \"recall\": 0.58875, \"task\": \"sentiment\", \"training_label_coverage\": 0.2975}, {\"accuracy\": 0.5280898876404494, \"condition\": \"snorkel\", \"dataset\": \"youtube\", \"f1\": 0.5961538461538463, \"max_dev_f1\": 0.6511627906976744, \"participant\": \"p1\", \"precision\": 0.5176636904761905, \"recall\": 0.512287855691057, \"task\": \"spam\", \"training_label_coverage\": 1.0}, {\"accuracy\": 0.51, \"condition\": \"snorkel\", \"dataset\": \"amazon\", \"f1\": 0.5654101995565409, \"max_dev_f1\": 0.6394557823129251, \"participant\": \"p3\", \"precision\": 0.5196675824175824, \"recall\": 0.51, \"task\": \"sentiment\", \"training_label_coverage\": 1.0}, {\"accuracy\": 0.6123595505617978, \"condition\": \"ruler\", \"dataset\": \"youtube\", \"f1\": 0.7261904761904762, \"max_dev_f1\": 0.7500000000000001, \"participant\": \"p3\", \"precision\": 0.6956971153846154, \"recall\": 0.5666263338414634, \"task\": \"spam\", \"training_label_coverage\": 0.62375}, {\"accuracy\": 0.5, \"condition\": \"snorkel\", \"dataset\": \"amazon\", \"f1\": 0.5, \"max_dev_f1\": 0.5656565657, \"participant\": \"p7\", \"precision\": 0.0, \"recall\": 0.0, \"task\": \"sentiment\", \"training_label_coverage\": 0.0}, {\"accuracy\": 0.7106741573033708, \"condition\": \"ruler\", \"dataset\": \"youtube\", \"f1\": 0.7365728900255755, \"max_dev_f1\": 0.7512195121951221, \"participant\": \"p7\", \"precision\": 0.7082824152610185, \"recall\": 0.7053963414634146, \"task\": \"spam\", \"training_label_coverage\": 0.57875}, {\"accuracy\": 0.49125, \"condition\": \"ruler\", \"dataset\": \"amazon\", \"f1\": 0.6476190476190475, \"max_dev_f1\": 0.6147859922178989, \"participant\": \"p0\", \"precision\": 0.5987136457285228, \"recall\": 0.49125, \"task\": \"sentiment\", \"training_label_coverage\": 0.6325}, {\"accuracy\": 0.5393258426966292, \"condition\": \"snorkel\", \"dataset\": \"youtube\", \"f1\": 0.6996336996336996, \"max_dev_f1\": 0.6872246696035242, \"participant\": \"p0\", \"precision\": 0.5188841807909604, \"recall\": 0.4781989964430894, \"task\": \"spam\", \"training_label_coverage\": 0.27375}, {\"accuracy\": 0.515, \"condition\": \"snorkel\", \"dataset\": \"amazon\", \"f1\": 0.6491862567811935, \"max_dev_f1\": 0.6482758620689655, \"participant\": \"p9\", \"precision\": 0.601146408839779, \"recall\": 0.515, \"task\": \"sentiment\", \"training_label_coverage\": 0.38875}, {\"accuracy\": 0.6797752808988764, \"condition\": \"ruler\", \"dataset\": \"youtube\", \"f1\": 0.7076923076923077, \"max_dev_f1\": 0.7698744769874477, \"participant\": \"p9\", \"precision\": 0.6767270809359417, \"recall\": 0.674544588414634, \"task\": \"spam\", \"training_label_coverage\": 0.5775}, {\"accuracy\": 0.63, \"condition\": \"ruler\", \"dataset\": \"amazon\", \"f1\": 0.5432098765432098, \"max_dev_f1\": 0.6727272727272727, \"participant\": \"p6\", \"precision\": 0.6368761792120751, \"recall\": 0.6299999999999999, \"task\": \"sentiment\", \"training_label_coverage\": 1.0}, {\"accuracy\": 0.9129213483146068, \"condition\": \"snorkel\", \"dataset\": \"youtube\", \"f1\": 0.9155313351498636, \"max_dev_f1\": 0.9473684210526316, \"participant\": \"p6\", \"precision\": 0.9116182320441989, \"recall\": 0.9180106707317072, \"task\": \"spam\", \"training_label_coverage\": 1.0}, {\"accuracy\": 0.65875, \"condition\": \"snorkel\", \"dataset\": \"amazon\", \"f1\": 0.6956521739130435, \"max_dev_f1\": 0.7053941908713692, \"participant\": \"p5\", \"precision\": 0.6834482200647249, \"recall\": 0.65875, \"task\": \"sentiment\", \"training_label_coverage\": 0.965}, {\"accuracy\": 0.5337078651685393, \"condition\": \"ruler\", \"dataset\": \"youtube\", \"f1\": 0.6959706959706959, \"max_dev_f1\": 0.7012987012987013, \"participant\": \"p5\", \"precision\": 0.2562853107344633, \"recall\": 0.47252604166666673, \"task\": \"spam\", \"training_label_coverage\": 0.53}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_bars = alt.Chart(dt).mark_errorbar(extent='stderr').encode(\n",
    "  x=alt.X('precision:Q'),\n",
    "  y=alt.Y('condition:N'),\n",
    "    color=alt.Color('condition:N', sort=['ruler'])\n",
    ")\n",
    "\n",
    "points = alt.Chart(dt).mark_point(filled=True).encode(\n",
    "  x=alt.X('precision:Q', aggregate='mean'),\n",
    "  y=alt.Y('condition:N'),\n",
    " color=alt.Color('condition:N', sort=['ruler'])\n",
    ")\n",
    "\n",
    "error_bars + points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6fy-3X1dNmq6",
    "outputId": "3cccfca0-4a15-49b0-c5b3-e003644d1dec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_relResult(statistic=0.21633993091864037, pvalue=0.8335467862551649)"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ruler_precision = dt[dt['condition']=='ruler']['precision']\n",
    "snorkel_precision = dt[dt['condition']=='snorkel']['precision']\n",
    "stats.ttest_rel(ruler_precision, snorkel_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rGc6cso4Nox7"
   },
   "source": [
    "*As* expected, the difference is not significant for PRECISION (**pvalue=0.83 >> 0.05**)\n",
    "\n",
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "colab_type": "code",
    "id": "cIexCrOoNpsB",
    "outputId": "6cda6cc1-29ab-47b6-d422-84a6fc746f0e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-a75dfa6e5339464eaac29afb49505e5e\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-a75dfa6e5339464eaac29afb49505e5e\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-a75dfa6e5339464eaac29afb49505e5e\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"layer\": [{\"mark\": {\"type\": \"errorbar\", \"extent\": \"stderr\"}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"condition\", \"sort\": [\"ruler\"]}, \"x\": {\"type\": \"quantitative\", \"field\": \"recall\"}, \"y\": {\"type\": \"nominal\", \"field\": \"condition\"}}}, {\"mark\": {\"type\": \"point\", \"filled\": true}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"condition\", \"sort\": [\"ruler\"]}, \"x\": {\"type\": \"quantitative\", \"aggregate\": \"mean\", \"field\": \"recall\"}, \"y\": {\"type\": \"nominal\", \"field\": \"condition\"}}}], \"data\": {\"name\": \"data-087497bff17fdcb684f0b14eb7cac74f\"}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-087497bff17fdcb684f0b14eb7cac74f\": [{\"accuracy\": 0.48375, \"condition\": \"snorkel\", \"dataset\": \"amazon\", \"f1\": 0.5620360551431601, \"max_dev_f1\": 0.653386454183267, \"participant\": \"p4\", \"precision\": 0.4922725221697655, \"recall\": 0.48375, \"task\": \"sentiment\", \"training_label_coverage\": 0.58375}, {\"accuracy\": 0.5786516853932584, \"condition\": \"ruler\", \"dataset\": \"youtube\", \"f1\": 0.7137404580152672, \"max_dev_f1\": 0.6325581395348836, \"participant\": \"p4\", \"precision\": 0.6825991465863452, \"recall\": 0.5255986407520326, \"task\": \"spam\", \"training_label_coverage\": 0.135}, {\"accuracy\": 0.4825, \"condition\": \"ruler\", \"dataset\": \"amazon\", \"f1\": 0.6229508196721313, \"max_dev_f1\": 0.5882352941176471, \"participant\": \"p8\", \"precision\": 0.5017700851290237, \"recall\": 0.4825, \"task\": \"sentiment\", \"training_label_coverage\": 0.1125}, {\"accuracy\": 0.7893258426966292, \"condition\": \"snorkel\", \"dataset\": \"youtube\", \"f1\": 0.7648902821316614, \"max_dev_f1\": 0.6974358974358974, \"participant\": \"p8\", \"precision\": 0.8214846301963346, \"recall\": 0.8099815802845528, \"task\": \"spam\", \"training_label_coverage\": 0.54375}, {\"accuracy\": 0.50625, \"condition\": \"ruler\", \"dataset\": \"amazon\", \"f1\": 0.6556233653007847, \"max_dev_f1\": 0.6428571428571429, \"participant\": \"p2\", \"precision\": 0.6676213818860878, \"recall\": 0.5062500000000001, \"task\": \"sentiment\", \"training_label_coverage\": 0.4025}, {\"accuracy\": 0.6797752808988764, \"condition\": \"snorkel\", \"dataset\": \"youtube\", \"f1\": 0.6041666666666666, \"max_dev_f1\": 0.6918918918918918, \"participant\": \"p2\", \"precision\": 0.7442247596153845, \"recall\": 0.710193407012195, \"task\": \"spam\", \"training_label_coverage\": 1.0}, {\"accuracy\": 0.58875, \"condition\": \"ruler\", \"dataset\": \"amazon\", \"f1\": 0.6364640883977901, \"max_dev_f1\": 0.6448598130841121, \"participant\": \"p1\", \"precision\": 0.6081340813551366, \"recall\": 0.58875, \"task\": \"sentiment\", \"training_label_coverage\": 0.2975}, {\"accuracy\": 0.5280898876404494, \"condition\": \"snorkel\", \"dataset\": \"youtube\", \"f1\": 0.5961538461538463, \"max_dev_f1\": 0.6511627906976744, \"participant\": \"p1\", \"precision\": 0.5176636904761905, \"recall\": 0.512287855691057, \"task\": \"spam\", \"training_label_coverage\": 1.0}, {\"accuracy\": 0.51, \"condition\": \"snorkel\", \"dataset\": \"amazon\", \"f1\": 0.5654101995565409, \"max_dev_f1\": 0.6394557823129251, \"participant\": \"p3\", \"precision\": 0.5196675824175824, \"recall\": 0.51, \"task\": \"sentiment\", \"training_label_coverage\": 1.0}, {\"accuracy\": 0.6123595505617978, \"condition\": \"ruler\", \"dataset\": \"youtube\", \"f1\": 0.7261904761904762, \"max_dev_f1\": 0.7500000000000001, \"participant\": \"p3\", \"precision\": 0.6956971153846154, \"recall\": 0.5666263338414634, \"task\": \"spam\", \"training_label_coverage\": 0.62375}, {\"accuracy\": 0.5, \"condition\": \"snorkel\", \"dataset\": \"amazon\", \"f1\": 0.5, \"max_dev_f1\": 0.5656565657, \"participant\": \"p7\", \"precision\": 0.0, \"recall\": 0.0, \"task\": \"sentiment\", \"training_label_coverage\": 0.0}, {\"accuracy\": 0.7106741573033708, \"condition\": \"ruler\", \"dataset\": \"youtube\", \"f1\": 0.7365728900255755, \"max_dev_f1\": 0.7512195121951221, \"participant\": \"p7\", \"precision\": 0.7082824152610185, \"recall\": 0.7053963414634146, \"task\": \"spam\", \"training_label_coverage\": 0.57875}, {\"accuracy\": 0.49125, \"condition\": \"ruler\", \"dataset\": \"amazon\", \"f1\": 0.6476190476190475, \"max_dev_f1\": 0.6147859922178989, \"participant\": \"p0\", \"precision\": 0.5987136457285228, \"recall\": 0.49125, \"task\": \"sentiment\", \"training_label_coverage\": 0.6325}, {\"accuracy\": 0.5393258426966292, \"condition\": \"snorkel\", \"dataset\": \"youtube\", \"f1\": 0.6996336996336996, \"max_dev_f1\": 0.6872246696035242, \"participant\": \"p0\", \"precision\": 0.5188841807909604, \"recall\": 0.4781989964430894, \"task\": \"spam\", \"training_label_coverage\": 0.27375}, {\"accuracy\": 0.515, \"condition\": \"snorkel\", \"dataset\": \"amazon\", \"f1\": 0.6491862567811935, \"max_dev_f1\": 0.6482758620689655, \"participant\": \"p9\", \"precision\": 0.601146408839779, \"recall\": 0.515, \"task\": \"sentiment\", \"training_label_coverage\": 0.38875}, {\"accuracy\": 0.6797752808988764, \"condition\": \"ruler\", \"dataset\": \"youtube\", \"f1\": 0.7076923076923077, \"max_dev_f1\": 0.7698744769874477, \"participant\": \"p9\", \"precision\": 0.6767270809359417, \"recall\": 0.674544588414634, \"task\": \"spam\", \"training_label_coverage\": 0.5775}, {\"accuracy\": 0.63, \"condition\": \"ruler\", \"dataset\": \"amazon\", \"f1\": 0.5432098765432098, \"max_dev_f1\": 0.6727272727272727, \"participant\": \"p6\", \"precision\": 0.6368761792120751, \"recall\": 0.6299999999999999, \"task\": \"sentiment\", \"training_label_coverage\": 1.0}, {\"accuracy\": 0.9129213483146068, \"condition\": \"snorkel\", \"dataset\": \"youtube\", \"f1\": 0.9155313351498636, \"max_dev_f1\": 0.9473684210526316, \"participant\": \"p6\", \"precision\": 0.9116182320441989, \"recall\": 0.9180106707317072, \"task\": \"spam\", \"training_label_coverage\": 1.0}, {\"accuracy\": 0.65875, \"condition\": \"snorkel\", \"dataset\": \"amazon\", \"f1\": 0.6956521739130435, \"max_dev_f1\": 0.7053941908713692, \"participant\": \"p5\", \"precision\": 0.6834482200647249, \"recall\": 0.65875, \"task\": \"sentiment\", \"training_label_coverage\": 0.965}, {\"accuracy\": 0.5337078651685393, \"condition\": \"ruler\", \"dataset\": \"youtube\", \"f1\": 0.6959706959706959, \"max_dev_f1\": 0.7012987012987013, \"participant\": \"p5\", \"precision\": 0.2562853107344633, \"recall\": 0.47252604166666673, \"task\": \"spam\", \"training_label_coverage\": 0.53}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_bars = alt.Chart(dt).mark_errorbar(extent='stderr').encode(\n",
    "  x=alt.X('recall:Q'),\n",
    "  y=alt.Y('condition:N'),\n",
    "    color=alt.Color('condition:N', sort=['ruler'])\n",
    ")\n",
    "\n",
    "points = alt.Chart(dt).mark_point(filled=True).encode(\n",
    "  x=alt.X('recall:Q', aggregate='mean'),\n",
    "  y=alt.Y('condition:N'),\n",
    " color=alt.Color('condition:N', sort=['ruler'])\n",
    ")\n",
    "\n",
    "error_bars + points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hxZ6bEfBNv0S",
    "outputId": "7618f65e-edc7-4283-faf8-eec431ed9104"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_relResult(statistic=0.050128779325306946, pvalue=0.961114662015901)"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ruler_recall = dt[dt['condition']=='ruler']['recall']\n",
    "snorkel_recall = dt[dt['condition']=='snorkel']['recall']\n",
    "stats.ttest_rel(ruler_recall, snorkel_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7s4nHMgoNzjP"
   },
   "source": [
    "*Again*, differences in RECALL not significant (**pvalue=0.96 >> 0.05**)\n",
    "\n",
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "colab_type": "code",
    "id": "1AR5JI-vN6Ju",
    "outputId": "feba91be-afa2-4b11-e173-dcfebea73436"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-2d54d863f35643ab8c56cfa1d311c468\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-2d54d863f35643ab8c56cfa1d311c468\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-2d54d863f35643ab8c56cfa1d311c468\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"layer\": [{\"mark\": {\"type\": \"errorbar\", \"extent\": \"stderr\"}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"condition\", \"sort\": [\"ruler\"]}, \"x\": {\"type\": \"quantitative\", \"field\": \"accuracy\"}, \"y\": {\"type\": \"nominal\", \"field\": \"condition\"}}}, {\"mark\": {\"type\": \"point\", \"filled\": true}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"condition\", \"sort\": [\"ruler\"]}, \"x\": {\"type\": \"quantitative\", \"aggregate\": \"mean\", \"field\": \"accuracy\"}, \"y\": {\"type\": \"nominal\", \"field\": \"condition\"}}}], \"data\": {\"name\": \"data-087497bff17fdcb684f0b14eb7cac74f\"}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-087497bff17fdcb684f0b14eb7cac74f\": [{\"accuracy\": 0.48375, \"condition\": \"snorkel\", \"dataset\": \"amazon\", \"f1\": 0.5620360551431601, \"max_dev_f1\": 0.653386454183267, \"participant\": \"p4\", \"precision\": 0.4922725221697655, \"recall\": 0.48375, \"task\": \"sentiment\", \"training_label_coverage\": 0.58375}, {\"accuracy\": 0.5786516853932584, \"condition\": \"ruler\", \"dataset\": \"youtube\", \"f1\": 0.7137404580152672, \"max_dev_f1\": 0.6325581395348836, \"participant\": \"p4\", \"precision\": 0.6825991465863452, \"recall\": 0.5255986407520326, \"task\": \"spam\", \"training_label_coverage\": 0.135}, {\"accuracy\": 0.4825, \"condition\": \"ruler\", \"dataset\": \"amazon\", \"f1\": 0.6229508196721313, \"max_dev_f1\": 0.5882352941176471, \"participant\": \"p8\", \"precision\": 0.5017700851290237, \"recall\": 0.4825, \"task\": \"sentiment\", \"training_label_coverage\": 0.1125}, {\"accuracy\": 0.7893258426966292, \"condition\": \"snorkel\", \"dataset\": \"youtube\", \"f1\": 0.7648902821316614, \"max_dev_f1\": 0.6974358974358974, \"participant\": \"p8\", \"precision\": 0.8214846301963346, \"recall\": 0.8099815802845528, \"task\": \"spam\", \"training_label_coverage\": 0.54375}, {\"accuracy\": 0.50625, \"condition\": \"ruler\", \"dataset\": \"amazon\", \"f1\": 0.6556233653007847, \"max_dev_f1\": 0.6428571428571429, \"participant\": \"p2\", \"precision\": 0.6676213818860878, \"recall\": 0.5062500000000001, \"task\": \"sentiment\", \"training_label_coverage\": 0.4025}, {\"accuracy\": 0.6797752808988764, \"condition\": \"snorkel\", \"dataset\": \"youtube\", \"f1\": 0.6041666666666666, \"max_dev_f1\": 0.6918918918918918, \"participant\": \"p2\", \"precision\": 0.7442247596153845, \"recall\": 0.710193407012195, \"task\": \"spam\", \"training_label_coverage\": 1.0}, {\"accuracy\": 0.58875, \"condition\": \"ruler\", \"dataset\": \"amazon\", \"f1\": 0.6364640883977901, \"max_dev_f1\": 0.6448598130841121, \"participant\": \"p1\", \"precision\": 0.6081340813551366, \"recall\": 0.58875, \"task\": \"sentiment\", \"training_label_coverage\": 0.2975}, {\"accuracy\": 0.5280898876404494, \"condition\": \"snorkel\", \"dataset\": \"youtube\", \"f1\": 0.5961538461538463, \"max_dev_f1\": 0.6511627906976744, \"participant\": \"p1\", \"precision\": 0.5176636904761905, \"recall\": 0.512287855691057, \"task\": \"spam\", \"training_label_coverage\": 1.0}, {\"accuracy\": 0.51, \"condition\": \"snorkel\", \"dataset\": \"amazon\", \"f1\": 0.5654101995565409, \"max_dev_f1\": 0.6394557823129251, \"participant\": \"p3\", \"precision\": 0.5196675824175824, \"recall\": 0.51, \"task\": \"sentiment\", \"training_label_coverage\": 1.0}, {\"accuracy\": 0.6123595505617978, \"condition\": \"ruler\", \"dataset\": \"youtube\", \"f1\": 0.7261904761904762, \"max_dev_f1\": 0.7500000000000001, \"participant\": \"p3\", \"precision\": 0.6956971153846154, \"recall\": 0.5666263338414634, \"task\": \"spam\", \"training_label_coverage\": 0.62375}, {\"accuracy\": 0.5, \"condition\": \"snorkel\", \"dataset\": \"amazon\", \"f1\": 0.5, \"max_dev_f1\": 0.5656565657, \"participant\": \"p7\", \"precision\": 0.0, \"recall\": 0.0, \"task\": \"sentiment\", \"training_label_coverage\": 0.0}, {\"accuracy\": 0.7106741573033708, \"condition\": \"ruler\", \"dataset\": \"youtube\", \"f1\": 0.7365728900255755, \"max_dev_f1\": 0.7512195121951221, \"participant\": \"p7\", \"precision\": 0.7082824152610185, \"recall\": 0.7053963414634146, \"task\": \"spam\", \"training_label_coverage\": 0.57875}, {\"accuracy\": 0.49125, \"condition\": \"ruler\", \"dataset\": \"amazon\", \"f1\": 0.6476190476190475, \"max_dev_f1\": 0.6147859922178989, \"participant\": \"p0\", \"precision\": 0.5987136457285228, \"recall\": 0.49125, \"task\": \"sentiment\", \"training_label_coverage\": 0.6325}, {\"accuracy\": 0.5393258426966292, \"condition\": \"snorkel\", \"dataset\": \"youtube\", \"f1\": 0.6996336996336996, \"max_dev_f1\": 0.6872246696035242, \"participant\": \"p0\", \"precision\": 0.5188841807909604, \"recall\": 0.4781989964430894, \"task\": \"spam\", \"training_label_coverage\": 0.27375}, {\"accuracy\": 0.515, \"condition\": \"snorkel\", \"dataset\": \"amazon\", \"f1\": 0.6491862567811935, \"max_dev_f1\": 0.6482758620689655, \"participant\": \"p9\", \"precision\": 0.601146408839779, \"recall\": 0.515, \"task\": \"sentiment\", \"training_label_coverage\": 0.38875}, {\"accuracy\": 0.6797752808988764, \"condition\": \"ruler\", \"dataset\": \"youtube\", \"f1\": 0.7076923076923077, \"max_dev_f1\": 0.7698744769874477, \"participant\": \"p9\", \"precision\": 0.6767270809359417, \"recall\": 0.674544588414634, \"task\": \"spam\", \"training_label_coverage\": 0.5775}, {\"accuracy\": 0.63, \"condition\": \"ruler\", \"dataset\": \"amazon\", \"f1\": 0.5432098765432098, \"max_dev_f1\": 0.6727272727272727, \"participant\": \"p6\", \"precision\": 0.6368761792120751, \"recall\": 0.6299999999999999, \"task\": \"sentiment\", \"training_label_coverage\": 1.0}, {\"accuracy\": 0.9129213483146068, \"condition\": \"snorkel\", \"dataset\": \"youtube\", \"f1\": 0.9155313351498636, \"max_dev_f1\": 0.9473684210526316, \"participant\": \"p6\", \"precision\": 0.9116182320441989, \"recall\": 0.9180106707317072, \"task\": \"spam\", \"training_label_coverage\": 1.0}, {\"accuracy\": 0.65875, \"condition\": \"snorkel\", \"dataset\": \"amazon\", \"f1\": 0.6956521739130435, \"max_dev_f1\": 0.7053941908713692, \"participant\": \"p5\", \"precision\": 0.6834482200647249, \"recall\": 0.65875, \"task\": \"sentiment\", \"training_label_coverage\": 0.965}, {\"accuracy\": 0.5337078651685393, \"condition\": \"ruler\", \"dataset\": \"youtube\", \"f1\": 0.6959706959706959, \"max_dev_f1\": 0.7012987012987013, \"participant\": \"p5\", \"precision\": 0.2562853107344633, \"recall\": 0.47252604166666673, \"task\": \"spam\", \"training_label_coverage\": 0.53}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_bars = alt.Chart(dt).mark_errorbar(extent='stderr').encode(\n",
    "  x=alt.X('accuracy:Q'),\n",
    "  y=alt.Y('condition:N'),\n",
    "    color=alt.Color('condition:N', sort=['ruler'])\n",
    ")\n",
    "\n",
    "points = alt.Chart(dt).mark_point(filled=True).encode(\n",
    "  x=alt.X('accuracy:Q', aggregate='mean'),\n",
    "  y=alt.Y('condition:N'),\n",
    " color=alt.Color('condition:N', sort=['ruler'])\n",
    ")\n",
    "\n",
    "error_bars + points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "g9JfYzG1ObcR",
    "outputId": "1e30166a-8abc-4314-afba-95721a90fd51"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_relResult(statistic=0.050128779325306946, pvalue=0.961114662015901)"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ruler_accuracy = dt[dt['condition']=='ruler']['recall']\n",
    "snorkel_accuracy = dt[dt['condition']=='snorkel']['recall']\n",
    "stats.ttest_rel(ruler_accuracy, snorkel_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "02B0tlieOfOb"
   },
   "source": [
    "Not significant (**pvalue=0.96 >> 0.05**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jAnDe8bQbKgk"
   },
   "source": [
    "### Qualitative Figure (Survey responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nmaH-5fSN7XN"
   },
   "outputs": [],
   "source": [
    "background = pd.read_csv('https://raw.githubusercontent.com/rulerauthors/ruler/master/user_study/background_survey_anon.csv', index_col=0)\n",
    "exit_survey = pd.read_csv('https://raw.githubusercontent.com/rulerauthors/ruler/master/user_study/exit_survey_anon.csv', index_col=0)\n",
    "final_survey = pd.read_csv('https://raw.githubusercontent.com/rulerauthors/ruler/master/user_study/final_survey_anon.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mRrF7zE-dgjo"
   },
   "source": [
    "The original column names for exit_survey shows the statements that the users ranked their agreement with, on a Likert scale of 1-5.\n",
    "\n",
    "We'll shorten these column names for our figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zxi4TOm6c16Q"
   },
   "outputs": [],
   "source": [
    "# simplify column names\n",
    "exit_survey.columns = ['Timestamp', 'condition',\n",
    "       'overall satisfaction', 'ease of use',\n",
    "       'expressivity',\n",
    "       'ease of learning',\n",
    "       'feedback',\n",
    "       'how to improve',\n",
    "       'other',\n",
    "       'comments', 'participant']\n",
    "exit_survey = exit_survey.drop('Timestamp', axis=1)\n",
    "exit_survey['condition'] = exit_survey['condition'].str.lower()\n",
    "\n",
    "exit_survey.fillna({'comments':'','how to improve':'', 'feedback':'', 'other':''},inplace=True) # this is necessary to be able to pass the dataframe to R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K_V1DLKmTwyd"
   },
   "outputs": [],
   "source": [
    "df_q = exit_survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LykHpTaLcw3q"
   },
   "outputs": [],
   "source": [
    "df_qm = df_q.melt(id_vars=['participant', 'condition','comments', 'how to improve', 'feedback', 'other'], \n",
    "        var_name=\"metric\", \n",
    "        value_name=\"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "colab_type": "code",
    "id": "oI5rSlHzcGPQ",
    "outputId": "1efbfb31-2014-4b0e-a023-155f4a2c4f5d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-d5851011f8f8407486bd988d2b20b809\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-d5851011f8f8407486bd988d2b20b809\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-d5851011f8f8407486bd988d2b20b809\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-3758d33144e3374e825b7f05a1f45de3\"}, \"facet\": {\"type\": \"nominal\", \"field\": \"metric\", \"sort\": [\"ease of use\", \"expressivity\", \"ease of learning\", \"overall\"]}, \"spec\": {\"layer\": [{\"mark\": {\"type\": \"errorbar\", \"extent\": \"stderr\"}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"condition\", \"sort\": [\"ruler\"]}, \"x\": {\"type\": \"quantitative\", \"field\": \"value\"}, \"y\": {\"type\": \"nominal\", \"field\": \"condition\"}}, \"height\": 100, \"width\": 400}, {\"mark\": {\"type\": \"point\", \"filled\": true}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"condition\", \"sort\": [\"ruler\"]}, \"x\": {\"type\": \"quantitative\", \"aggregate\": \"mean\", \"field\": \"value\"}, \"y\": {\"type\": \"nominal\", \"field\": \"condition\"}}, \"height\": 100, \"width\": 400}]}, \"columns\": 2, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-3758d33144e3374e825b7f05a1f45de3\": [{\"participant\": \"p7\", \"condition\": \"ruler\", \"comments\": \"\", \"how to improve\": \"1. summary panel for labeling function helping to group and delete/add LFs. 2. adding LFs not by examples should be combined\", \"feedback\": \"\", \"other\": \"\", \"metric\": \"overall satisfaction\", \"value\": 4}, {\"participant\": \"p7\", \"condition\": \"snorkel\", \"comments\": \"\", \"how to improve\": \"\", \"feedback\": \"\", \"other\": \"\", \"metric\": \"overall satisfaction\", \"value\": 3}, {\"participant\": \"p3\", \"condition\": \"ruler\", \"comments\": \"Thanks!\", \"how to improve\": \"Below are the bugs we discussed. I am not suggesting they all need to be fixed :).\\n1. When I submitted a value modification for a concept row, the span annotations in the text did not update unless a new span could be identified in the text after the modification. Example: changing \\\"firearm\\\" to \\\"fire\\\" would cause the annotations to update, but a change from \\\"firearm\\\" to \\\"filkjwerlkjsdf\\\" would not cause an update.\\n2. I could not delete concept rows.\\n\\nBelow are some possible improvements I can think of:\\n1. I wasn't always sure when a concept modification had taken effect. It would be nice if there were some indication in the UI that the concept modification was in fact affect the output.\\n2. We could show a plot of the historical performance of the model over time. The plot could be a multi-line chart of statistics in the top-right corner on the y-axis and the chronological change id on the x-axis. Then, if a user were to click on a point in the plot, they would see a modal that would ask them if they would like to download the model from that point in time.\\n3. It would be nice to see class-specific statistics for snorkel's labelling model.\\n4. Should we give the user the option to select \\\"weighted average\\\" or \\\"simple average\\\" for the statistics on the entire development set?\\n5. I do think that organizing labelling functions into concepts helped quite a bit. Users who are using concepts to organize their functions (and thus modifying their concepts over time) would want to see diffs for the statistics after each concept modification.\", \"feedback\": \"I wonder if I should have added more functions more quickly and done more pruning given the diffs.\\n\\nI do think that organizing labelling functions into concepts helped quite a bit. However, users who are using concepts to organize their functions (and thus modifying their concepts over time) would want to see diffs for the statistics after each concept modification.\", \"other\": \"I thought it was conducted very well!\", \"metric\": \"overall satisfaction\", \"value\": 5}, {\"participant\": \"p3\", \"condition\": \"snorkel\", \"comments\": \"We were having trouble determining why Snorkel was telling us we had classified 94 positive results correctly and 0 incorrectly but achieved only 47% accuracy. I think Snorkel was saying we correctly labeled 94 of the 94 actual positive examples, and 47% of the examples we identified as positive were actually positive. In other words, Snorkel was telling us that our recall was 100% and our precision was 47%.\", \"how to improve\": \"They may already have this, but I would add the ability to pass in your own metric definitions to the evaluation step\", \"feedback\": \"What worked:\\n1. Writing my own functions to analyze why labeling functions were making incorrect predictions\\n2. I didn't have time to do this, but I would have probably analyzed the model performance metrics myself. We ran into a problem where we weren't entirely sure how the metrics were being calculated, and so I would probably calculate them myself to have complete understanding.\", \"other\": \"The study was great! I would use the \\\"Table of Contents (2)\\\" extension to enable the users to more easily navigate the Jupyter Notebook.\\n\\nhttps://jupyter-contrib-nbextensions.readthedocs.io/en/latest/nbextensions/toc2/README.html\\n\\nhttps://jupyter-contrib-nbextensions.readthedocs.io/en/latest/\", \"metric\": \"overall satisfaction\", \"value\": 4}, {\"participant\": \"p9\", \"condition\": \"snorkel\", \"comments\": \"\", \"how to improve\": \"I would show statistics about term frequency to users to help writing labeling functions. Also, I would like to make some helper functions to get synonyms and antonyms easily available, so that a user can improve coverage of simple keyword-matching strategy.\", \"feedback\": \"It was hard to write complex functions because of the time limit. I wanted to see overall statistics of the term frequency, but I was not able to check the statistics easily in time.\\nSo, I just used simple keyword matching as labeling functions\", \"other\": \"Tutorial and explanations were thorough. The researcher remained unbiased and objective.\", \"metric\": \"overall satisfaction\", \"value\": 3}, {\"participant\": \"p9\", \"condition\": \"ruler\", \"comments\": \"\", \"how to improve\": \"I'd add the 'not have' condition. It was hard to find out 'must-have' keywords for the 'not spam' label. I'd like to add some statistical characteristics (word count, text length) of a data record and synthesize in/equalities using them.\", \"feedback\": \"I tried to construct concepts with relevant keywords for each label. This strategy worked for certain label (spam) but didn't work well for another label (not spam). I also tried to build rules based on entity labels, but it didn't work well.\", \"other\": \"\", \"metric\": \"overall satisfaction\", \"value\": 4}, {\"participant\": \"p4\", \"condition\": \"snorkel\", \"comments\": \"\", \"how to improve\": \"The pre-defined functions provided by the responsible for the experiment helped a lot. Having more pre-defined functions would be very useful (even for programmers).\", \"feedback\": \"I'm used using Python, but I usually need to double check many of the commands syntax, so it makes the process of generating the rules a little bit slower.\", \"other\": \"I liked it.\", \"metric\": \"overall satisfaction\", \"value\": 3}, {\"participant\": \"p4\", \"condition\": \"ruler\", \"comments\": \"\", \"how to improve\": \"Allowing the addition of:\\n1)  \\\"negative examples rules\\\";\\n2) general rules (not associated to any specific example);\\n3) a \\\"python window\\\" in which you could use python code (as used in Snorkel), thus explore best of both worlds (the easiness and speed of current Ruler, and the expressiveness of Snorkel)\", \"feedback\": \"I enjoyed using the tool! I could quickly define a set of rules with reasonable Precision/Recall over the available data. It would take much longer to get to the same performance without the help of Ruler. \\nOne type of rule that I could not create is for negative examples. I tried to create a rule that would be a negative example of spam. In the controlled experiment scenario (as it is a binary classification task) I could get the same effect by set the \\\"negative example rule\\\" for one class as a \\\"positive example\\\" rule to the other class.\\nAlso, I tried to create a rule (based on my domain knowledge) that was not specifically associated to a instance, but I could not.\", \"other\": \"\", \"metric\": \"overall satisfaction\", \"value\": 5}, {\"participant\": \"p6\", \"condition\": \"ruler\", \"comments\": \"\", \"how to improve\": \"1. Give formal definitions to key terms. \\n2. Prepare a step-by-step tutorial.  \", \"feedback\": \"Work\\n1. The tool can capture keyword-based functions. \\n2. The tool supports AND and OR operators. \\n\\nDidn't work\\n1. Some terms are not well-defined (e.g. Concept).\\n2. The tool lacks step-by-step documents.  \", \"other\": \"The tool is cool. \", \"metric\": \"overall satisfaction\", \"value\": 5}, {\"participant\": \"p6\", \"condition\": \"snorkel\", \"comments\": \"\", \"how to improve\": \"1. Reduce unnecessary coding as much as possible. \\n2. Make the evaluation of labeling functions instant. \", \"feedback\": \"Worked\\n1. Snorkel supports python that I am familiar with. \\n2. Snorkel enables me to leverage programming skills to label data.\\n\\nDidn't work\\n1. Snorkel is coding intensive that I have to run multiple Snorkel cells to evaluate labelling functions. \\n2. Snorkel does not instantly evaluate labeling functions. I have to rerun evaluation codes each time I update labeling functions. \", \"other\": \"The Snorkel tool is cool. \", \"metric\": \"overall satisfaction\", \"value\": 4}, {\"participant\": \"p5\", \"condition\": \"ruler\", \"comments\": \"\", \"how to improve\": \"\", \"feedback\": \"\", \"other\": \"\", \"metric\": \"overall satisfaction\", \"value\": 3}, {\"participant\": \"p5\", \"condition\": \"snorkel\", \"comments\": \"\", \"how to improve\": \"\", \"feedback\": \"\", \"other\": \"\", \"metric\": \"overall satisfaction\", \"value\": 5}, {\"participant\": \"p0\", \"condition\": \"ruler\", \"comments\": \"It was great exercise for me! thank you!\", \"how to improve\": \"When I mouse over token(s), I wished I had a popup to categorize it\", \"feedback\": \"Categorizing tokens was hard.for me\", \"other\": \"\", \"metric\": \"overall satisfaction\", \"value\": 4}, {\"participant\": \"p0\", \"condition\": \"snorkel\", \"comments\": \"Great experiments! I will look into snorkel as I have some ML tasks. Thanks!\", \"how to improve\": \"If non-essential codes (e.g. evaluation codes) were defined out of the notebooks, they would be more easy to understand.\", \"feedback\": \"I firstly write functions so that recalls get high.\", \"other\": \"\", \"metric\": \"overall satisfaction\", \"value\": 5}, {\"participant\": \"p1\", \"condition\": \"snorkel\", \"comments\": \"\", \"how to improve\": \"\", \"feedback\": \"\", \"other\": \"\", \"metric\": \"overall satisfaction\", \"value\": 2}, {\"participant\": \"p1\", \"condition\": \"ruler\", \"comments\": \"\", \"how to improve\": \"Scalability: the system becomes slower towards the end. Further optimization & approximation could be considered.\", \"feedback\": \"I noticed that my label accuracy did not constantly improving: first improves and then drops. Maybe this is just an extreme case, but I feel it is important to validate if other users also show similar trend.\", \"other\": \"\", \"metric\": \"overall satisfaction\", \"value\": 4}, {\"participant\": \"p2\", \"condition\": \"snorkel\", \"comments\": \"\", \"how to improve\": \"I was confused with the metric shown in the second block of the Apply function section. The comment \\\"Don't worry\\\" was not enough for me to disregard the value. :p\\n\\nMy work was going back and forth between labeling function, applying function and training a classifier. I would be helpful if the cells that the user runs are compiled into a single function (on a single cell) so I could simply call the function.\\n\\nFor example, prepare a function that traverses the namespace to list up any functions that begin with lf (or a longer prefix if it conflicts with something).\\n\", \"feedback\": \"My strategy was to directly take a look at examples and came up with salient words/phrases to write down keyword-based labeling functions. \", \"other\": \"The instructions are clear and the user study is organized well. I'm curious about the (psychological) effect of the time limit and being monitored/recorded.\\n\\nI may not pay attention but the input argument `x` of each labeling function was not clear at the beginning, which took a couple of minutes to figure out.\", \"metric\": \"overall satisfaction\", \"value\": 2}, {\"participant\": \"p2\", \"condition\": \"ruler\", \"comments\": \"\", \"how to improve\": \"1)\\nIt was not very intuitive how the system makes use of the labeling functions I made. What I was confused is when I saw the recall of class 0 dropped after adding labeling function for class 0. I had to conduct label-function engineering to figure out the best combination.\\n\\n2)\\nAs I asked during the user study, it would be helpful if I could directly add rules that are not activated by the current example (which I could with Snorkel.)\\n\\n3)\\nSomehow, it seems that the data has more positive examples than negative examples. It will be helpful if the system has a search function to retrieve a negative example (that contains certain words etc.)\\n\\n4)\\nI found several examples from which I didn't want to create labeling functions, but I would like to simply label the examples. This may be for the user study but only allowing the user to create labeling functions may not be the best way.\\n\\n5)\\nSimilarly, in practice, it should be better having a base classifier/base dictionary as a starting point. For example (I worked on a sentiment analysis task today), using a pre-trained classifier (trained on other sentiment classifier dataset) and/and/or using sentiment dictionary that contains words with sentiment polarity information. I feel like what I did with the system is approximately close to reconstructing an affective dictionary from scratch (tuned toward the dataset, in one sense.)\\n\", \"feedback\": \"My strategy is simply adding salient words/phrases of each class while monitoring the dev set performance. As I added labeling functions, I tried to create more detailed rules as I progress.\", \"other\": \"The user study was well organized and instructions were clear.\\n\\nI'm wondering if the user study randomly shuffles the order of methods. Although two tools/datasets are different, I feel like I was more prepared to work on the task.\", \"metric\": \"overall satisfaction\", \"value\": 4}, {\"participant\": \"p8\", \"condition\": \"snorkel\", \"comments\": \"\", \"how to improve\": \"Providing more stats/exploration options in terms of helping the user improve coverage. Instead of simply showing overlapping and conflicts, it would be better to see the samples and stats of two LFs by their overlap or conflict. Also it could automatically search for ensembling the labeling functions and provide suggestions to the user.\", \"feedback\": \"I tried to create a bunch of labeling functions on filtering by certain tokens at beginning and see how each works; then I modified those that turn out to have most wrong labels and got better results. Also I tried to have labeling functions on length and non-letters of the text, which seems to be not very useful. I should have tried to ensemble the labeling functions, which would have improve the coverage a lot.\", \"other\": \"\", \"metric\": \"overall satisfaction\", \"value\": 3}, {\"participant\": \"p8\", \"condition\": \"ruler\", \"comments\": \"\", \"how to improve\": \"Like snorkel, I think auto suggestions on improving the coverage of LFs could be very helpful. And it may be useful to allow users combine LFs or edit LFs in python in order to reduce the execution time and to be more flexible for users with coding experience. The regex concept should be very useful but I only use it for several times, and I think it would be great if the tool can suggest regex expressions based on user's annotation.\", \"feedback\": \"I think it is task dependent. For sentiment analysis, the coverage of labeling functions on certain tokens or phrases can be relatively small and may not be accurate as there are many variants in the phrase and the negation would affect the result. \", \"other\": \"I feel that there could be 10-15 more time after the tutorial for users to play with the tool on the example task. In both user studies I figured out ways to improve the performance shortly after it is finished.\", \"metric\": \"overall satisfaction\", \"value\": 4}, {\"participant\": \"p7\", \"condition\": \"ruler\", \"comments\": \"\", \"how to improve\": \"1. summary panel for labeling function helping to group and delete/add LFs. 2. adding LFs not by examples should be combined\", \"feedback\": \"\", \"other\": \"\", \"metric\": \"ease of use\", \"value\": 4}, {\"participant\": \"p7\", \"condition\": \"snorkel\", \"comments\": \"\", \"how to improve\": \"\", \"feedback\": \"\", \"other\": \"\", \"metric\": \"ease of use\", \"value\": 2}, {\"participant\": \"p3\", \"condition\": \"ruler\", \"comments\": \"Thanks!\", \"how to improve\": \"Below are the bugs we discussed. I am not suggesting they all need to be fixed :).\\n1. When I submitted a value modification for a concept row, the span annotations in the text did not update unless a new span could be identified in the text after the modification. Example: changing \\\"firearm\\\" to \\\"fire\\\" would cause the annotations to update, but a change from \\\"firearm\\\" to \\\"filkjwerlkjsdf\\\" would not cause an update.\\n2. I could not delete concept rows.\\n\\nBelow are some possible improvements I can think of:\\n1. I wasn't always sure when a concept modification had taken effect. It would be nice if there were some indication in the UI that the concept modification was in fact affect the output.\\n2. We could show a plot of the historical performance of the model over time. The plot could be a multi-line chart of statistics in the top-right corner on the y-axis and the chronological change id on the x-axis. Then, if a user were to click on a point in the plot, they would see a modal that would ask them if they would like to download the model from that point in time.\\n3. It would be nice to see class-specific statistics for snorkel's labelling model.\\n4. Should we give the user the option to select \\\"weighted average\\\" or \\\"simple average\\\" for the statistics on the entire development set?\\n5. I do think that organizing labelling functions into concepts helped quite a bit. Users who are using concepts to organize their functions (and thus modifying their concepts over time) would want to see diffs for the statistics after each concept modification.\", \"feedback\": \"I wonder if I should have added more functions more quickly and done more pruning given the diffs.\\n\\nI do think that organizing labelling functions into concepts helped quite a bit. However, users who are using concepts to organize their functions (and thus modifying their concepts over time) would want to see diffs for the statistics after each concept modification.\", \"other\": \"I thought it was conducted very well!\", \"metric\": \"ease of use\", \"value\": 5}, {\"participant\": \"p3\", \"condition\": \"snorkel\", \"comments\": \"We were having trouble determining why Snorkel was telling us we had classified 94 positive results correctly and 0 incorrectly but achieved only 47% accuracy. I think Snorkel was saying we correctly labeled 94 of the 94 actual positive examples, and 47% of the examples we identified as positive were actually positive. In other words, Snorkel was telling us that our recall was 100% and our precision was 47%.\", \"how to improve\": \"They may already have this, but I would add the ability to pass in your own metric definitions to the evaluation step\", \"feedback\": \"What worked:\\n1. Writing my own functions to analyze why labeling functions were making incorrect predictions\\n2. I didn't have time to do this, but I would have probably analyzed the model performance metrics myself. We ran into a problem where we weren't entirely sure how the metrics were being calculated, and so I would probably calculate them myself to have complete understanding.\", \"other\": \"The study was great! I would use the \\\"Table of Contents (2)\\\" extension to enable the users to more easily navigate the Jupyter Notebook.\\n\\nhttps://jupyter-contrib-nbextensions.readthedocs.io/en/latest/nbextensions/toc2/README.html\\n\\nhttps://jupyter-contrib-nbextensions.readthedocs.io/en/latest/\", \"metric\": \"ease of use\", \"value\": 4}, {\"participant\": \"p9\", \"condition\": \"snorkel\", \"comments\": \"\", \"how to improve\": \"I would show statistics about term frequency to users to help writing labeling functions. Also, I would like to make some helper functions to get synonyms and antonyms easily available, so that a user can improve coverage of simple keyword-matching strategy.\", \"feedback\": \"It was hard to write complex functions because of the time limit. I wanted to see overall statistics of the term frequency, but I was not able to check the statistics easily in time.\\nSo, I just used simple keyword matching as labeling functions\", \"other\": \"Tutorial and explanations were thorough. The researcher remained unbiased and objective.\", \"metric\": \"ease of use\", \"value\": 2}, {\"participant\": \"p9\", \"condition\": \"ruler\", \"comments\": \"\", \"how to improve\": \"I'd add the 'not have' condition. It was hard to find out 'must-have' keywords for the 'not spam' label. I'd like to add some statistical characteristics (word count, text length) of a data record and synthesize in/equalities using them.\", \"feedback\": \"I tried to construct concepts with relevant keywords for each label. This strategy worked for certain label (spam) but didn't work well for another label (not spam). I also tried to build rules based on entity labels, but it didn't work well.\", \"other\": \"\", \"metric\": \"ease of use\", \"value\": 5}, {\"participant\": \"p4\", \"condition\": \"snorkel\", \"comments\": \"\", \"how to improve\": \"The pre-defined functions provided by the responsible for the experiment helped a lot. Having more pre-defined functions would be very useful (even for programmers).\", \"feedback\": \"I'm used using Python, but I usually need to double check many of the commands syntax, so it makes the process of generating the rules a little bit slower.\", \"other\": \"I liked it.\", \"metric\": \"ease of use\", \"value\": 3}, {\"participant\": \"p4\", \"condition\": \"ruler\", \"comments\": \"\", \"how to improve\": \"Allowing the addition of:\\n1)  \\\"negative examples rules\\\";\\n2) general rules (not associated to any specific example);\\n3) a \\\"python window\\\" in which you could use python code (as used in Snorkel), thus explore best of both worlds (the easiness and speed of current Ruler, and the expressiveness of Snorkel)\", \"feedback\": \"I enjoyed using the tool! I could quickly define a set of rules with reasonable Precision/Recall over the available data. It would take much longer to get to the same performance without the help of Ruler. \\nOne type of rule that I could not create is for negative examples. I tried to create a rule that would be a negative example of spam. In the controlled experiment scenario (as it is a binary classification task) I could get the same effect by set the \\\"negative example rule\\\" for one class as a \\\"positive example\\\" rule to the other class.\\nAlso, I tried to create a rule (based on my domain knowledge) that was not specifically associated to a instance, but I could not.\", \"other\": \"\", \"metric\": \"ease of use\", \"value\": 5}, {\"participant\": \"p6\", \"condition\": \"ruler\", \"comments\": \"\", \"how to improve\": \"1. Give formal definitions to key terms. \\n2. Prepare a step-by-step tutorial.  \", \"feedback\": \"Work\\n1. The tool can capture keyword-based functions. \\n2. The tool supports AND and OR operators. \\n\\nDidn't work\\n1. Some terms are not well-defined (e.g. Concept).\\n2. The tool lacks step-by-step documents.  \", \"other\": \"The tool is cool. \", \"metric\": \"ease of use\", \"value\": 4}, {\"participant\": \"p6\", \"condition\": \"snorkel\", \"comments\": \"\", \"how to improve\": \"1. Reduce unnecessary coding as much as possible. \\n2. Make the evaluation of labeling functions instant. \", \"feedback\": \"Worked\\n1. Snorkel supports python that I am familiar with. \\n2. Snorkel enables me to leverage programming skills to label data.\\n\\nDidn't work\\n1. Snorkel is coding intensive that I have to run multiple Snorkel cells to evaluate labelling functions. \\n2. Snorkel does not instantly evaluate labeling functions. I have to rerun evaluation codes each time I update labeling functions. \", \"other\": \"The Snorkel tool is cool. \", \"metric\": \"ease of use\", \"value\": 4}, {\"participant\": \"p5\", \"condition\": \"ruler\", \"comments\": \"\", \"how to improve\": \"\", \"feedback\": \"\", \"other\": \"\", \"metric\": \"ease of use\", \"value\": 5}, {\"participant\": \"p5\", \"condition\": \"snorkel\", \"comments\": \"\", \"how to improve\": \"\", \"feedback\": \"\", \"other\": \"\", \"metric\": \"ease of use\", \"value\": 4}, {\"participant\": \"p0\", \"condition\": \"ruler\", \"comments\": \"It was great exercise for me! thank you!\", \"how to improve\": \"When I mouse over token(s), I wished I had a popup to categorize it\", \"feedback\": \"Categorizing tokens was hard.for me\", \"other\": \"\", \"metric\": \"ease of use\", \"value\": 3}, {\"participant\": \"p0\", \"condition\": \"snorkel\", \"comments\": \"Great experiments! I will look into snorkel as I have some ML tasks. Thanks!\", \"how to improve\": \"If non-essential codes (e.g. evaluation codes) were defined out of the notebooks, they would be more easy to understand.\", \"feedback\": \"I firstly write functions so that recalls get high.\", \"other\": \"\", \"metric\": \"ease of use\", \"value\": 4}, {\"participant\": \"p1\", \"condition\": \"snorkel\", \"comments\": \"\", \"how to improve\": \"\", \"feedback\": \"\", \"other\": \"\", \"metric\": \"ease of use\", \"value\": 3}, {\"participant\": \"p1\", \"condition\": \"ruler\", \"comments\": \"\", \"how to improve\": \"Scalability: the system becomes slower towards the end. Further optimization & approximation could be considered.\", \"feedback\": \"I noticed that my label accuracy did not constantly improving: first improves and then drops. Maybe this is just an extreme case, but I feel it is important to validate if other users also show similar trend.\", \"other\": \"\", \"metric\": \"ease of use\", \"value\": 5}, {\"participant\": \"p2\", \"condition\": \"snorkel\", \"comments\": \"\", \"how to improve\": \"I was confused with the metric shown in the second block of the Apply function section. The comment \\\"Don't worry\\\" was not enough for me to disregard the value. :p\\n\\nMy work was going back and forth between labeling function, applying function and training a classifier. I would be helpful if the cells that the user runs are compiled into a single function (on a single cell) so I could simply call the function.\\n\\nFor example, prepare a function that traverses the namespace to list up any functions that begin with lf (or a longer prefix if it conflicts with something).\\n\", \"feedback\": \"My strategy was to directly take a look at examples and came up with salient words/phrases to write down keyword-based labeling functions. \", \"other\": \"The instructions are clear and the user study is organized well. I'm curious about the (psychological) effect of the time limit and being monitored/recorded.\\n\\nI may not pay attention but the input argument `x` of each labeling function was not clear at the beginning, which took a couple of minutes to figure out.\", \"metric\": \"ease of use\", \"value\": 2}, {\"participant\": \"p2\", \"condition\": \"ruler\", \"comments\": \"\", \"how to improve\": \"1)\\nIt was not very intuitive how the system makes use of the labeling functions I made. What I was confused is when I saw the recall of class 0 dropped after adding labeling function for class 0. I had to conduct label-function engineering to figure out the best combination.\\n\\n2)\\nAs I asked during the user study, it would be helpful if I could directly add rules that are not activated by the current example (which I could with Snorkel.)\\n\\n3)\\nSomehow, it seems that the data has more positive examples than negative examples. It will be helpful if the system has a search function to retrieve a negative example (that contains certain words etc.)\\n\\n4)\\nI found several examples from which I didn't want to create labeling functions, but I would like to simply label the examples. This may be for the user study but only allowing the user to create labeling functions may not be the best way.\\n\\n5)\\nSimilarly, in practice, it should be better having a base classifier/base dictionary as a starting point. For example (I worked on a sentiment analysis task today), using a pre-trained classifier (trained on other sentiment classifier dataset) and/and/or using sentiment dictionary that contains words with sentiment polarity information. I feel like what I did with the system is approximately close to reconstructing an affective dictionary from scratch (tuned toward the dataset, in one sense.)\\n\", \"feedback\": \"My strategy is simply adding salient words/phrases of each class while monitoring the dev set performance. As I added labeling functions, I tried to create more detailed rules as I progress.\", \"other\": \"The user study was well organized and instructions were clear.\\n\\nI'm wondering if the user study randomly shuffles the order of methods. Although two tools/datasets are different, I feel like I was more prepared to work on the task.\", \"metric\": \"ease of use\", \"value\": 5}, {\"participant\": \"p8\", \"condition\": \"snorkel\", \"comments\": \"\", \"how to improve\": \"Providing more stats/exploration options in terms of helping the user improve coverage. Instead of simply showing overlapping and conflicts, it would be better to see the samples and stats of two LFs by their overlap or conflict. Also it could automatically search for ensembling the labeling functions and provide suggestions to the user.\", \"feedback\": \"I tried to create a bunch of labeling functions on filtering by certain tokens at beginning and see how each works; then I modified those that turn out to have most wrong labels and got better results. Also I tried to have labeling functions on length and non-letters of the text, which seems to be not very useful. I should have tried to ensemble the labeling functions, which would have improve the coverage a lot.\", \"other\": \"\", \"metric\": \"ease of use\", \"value\": 4}, {\"participant\": \"p8\", \"condition\": \"ruler\", \"comments\": \"\", \"how to improve\": \"Like snorkel, I think auto suggestions on improving the coverage of LFs could be very helpful. And it may be useful to allow users combine LFs or edit LFs in python in order to reduce the execution time and to be more flexible for users with coding experience. The regex concept should be very useful but I only use it for several times, and I think it would be great if the tool can suggest regex expressions based on user's annotation.\", \"feedback\": \"I think it is task dependent. For sentiment analysis, the coverage of labeling functions on certain tokens or phrases can be relatively small and may not be accurate as there are many variants in the phrase and the negation would affect the result. \", \"other\": \"I feel that there could be 10-15 more time after the tutorial for users to play with the tool on the example task. In both user studies I figured out ways to improve the performance shortly after it is finished.\", \"metric\": \"ease of use\", \"value\": 4}, {\"participant\": \"p7\", \"condition\": \"ruler\", \"comments\": \"\", \"how to improve\": \"1. summary panel for labeling function helping to group and delete/add LFs. 2. adding LFs not by examples should be combined\", \"feedback\": \"\", \"other\": \"\", \"metric\": \"expressivity\", \"value\": 2}, {\"participant\": \"p7\", \"condition\": \"snorkel\", \"comments\": \"\", \"how to improve\": \"\", \"feedback\": \"\", \"other\": \"\", \"metric\": \"expressivity\", \"value\": 5}, {\"participant\": \"p3\", \"condition\": \"ruler\", \"comments\": \"Thanks!\", \"how to improve\": \"Below are the bugs we discussed. I am not suggesting they all need to be fixed :).\\n1. When I submitted a value modification for a concept row, the span annotations in the text did not update unless a new span could be identified in the text after the modification. Example: changing \\\"firearm\\\" to \\\"fire\\\" would cause the annotations to update, but a change from \\\"firearm\\\" to \\\"filkjwerlkjsdf\\\" would not cause an update.\\n2. I could not delete concept rows.\\n\\nBelow are some possible improvements I can think of:\\n1. I wasn't always sure when a concept modification had taken effect. It would be nice if there were some indication in the UI that the concept modification was in fact affect the output.\\n2. We could show a plot of the historical performance of the model over time. The plot could be a multi-line chart of statistics in the top-right corner on the y-axis and the chronological change id on the x-axis. Then, if a user were to click on a point in the plot, they would see a modal that would ask them if they would like to download the model from that point in time.\\n3. It would be nice to see class-specific statistics for snorkel's labelling model.\\n4. Should we give the user the option to select \\\"weighted average\\\" or \\\"simple average\\\" for the statistics on the entire development set?\\n5. I do think that organizing labelling functions into concepts helped quite a bit. Users who are using concepts to organize their functions (and thus modifying their concepts over time) would want to see diffs for the statistics after each concept modification.\", \"feedback\": \"I wonder if I should have added more functions more quickly and done more pruning given the diffs.\\n\\nI do think that organizing labelling functions into concepts helped quite a bit. However, users who are using concepts to organize their functions (and thus modifying their concepts over time) would want to see diffs for the statistics after each concept modification.\", \"other\": \"I thought it was conducted very well!\", \"metric\": \"expressivity\", \"value\": 5}, {\"participant\": \"p3\", \"condition\": \"snorkel\", \"comments\": \"We were having trouble determining why Snorkel was telling us we had classified 94 positive results correctly and 0 incorrectly but achieved only 47% accuracy. I think Snorkel was saying we correctly labeled 94 of the 94 actual positive examples, and 47% of the examples we identified as positive were actually positive. In other words, Snorkel was telling us that our recall was 100% and our precision was 47%.\", \"how to improve\": \"They may already have this, but I would add the ability to pass in your own metric definitions to the evaluation step\", \"feedback\": \"What worked:\\n1. Writing my own functions to analyze why labeling functions were making incorrect predictions\\n2. I didn't have time to do this, but I would have probably analyzed the model performance metrics myself. We ran into a problem where we weren't entirely sure how the metrics were being calculated, and so I would probably calculate them myself to have complete understanding.\", \"other\": \"The study was great! I would use the \\\"Table of Contents (2)\\\" extension to enable the users to more easily navigate the Jupyter Notebook.\\n\\nhttps://jupyter-contrib-nbextensions.readthedocs.io/en/latest/nbextensions/toc2/README.html\\n\\nhttps://jupyter-contrib-nbextensions.readthedocs.io/en/latest/\", \"metric\": \"expressivity\", \"value\": 5}, {\"participant\": \"p9\", \"condition\": \"snorkel\", \"comments\": \"\", \"how to improve\": \"I would show statistics about term frequency to users to help writing labeling functions. Also, I would like to make some helper functions to get synonyms and antonyms easily available, so that a user can improve coverage of simple keyword-matching strategy.\", \"feedback\": \"It was hard to write complex functions because of the time limit. I wanted to see overall statistics of the term frequency, but I was not able to check the statistics easily in time.\\nSo, I just used simple keyword matching as labeling functions\", \"other\": \"Tutorial and explanations were thorough. The researcher remained unbiased and objective.\", \"metric\": \"expressivity\", \"value\": 5}, {\"participant\": \"p9\", \"condition\": \"ruler\", \"comments\": \"\", \"how to improve\": \"I'd add the 'not have' condition. It was hard to find out 'must-have' keywords for the 'not spam' label. I'd like to add some statistical characteristics (word count, text length) of a data record and synthesize in/equalities using them.\", \"feedback\": \"I tried to construct concepts with relevant keywords for each label. This strategy worked for certain label (spam) but didn't work well for another label (not spam). I also tried to build rules based on entity labels, but it didn't work well.\", \"other\": \"\", \"metric\": \"expressivity\", \"value\": 4}, {\"participant\": \"p4\", \"condition\": \"snorkel\", \"comments\": \"\", \"how to improve\": \"The pre-defined functions provided by the responsible for the experiment helped a lot. Having more pre-defined functions would be very useful (even for programmers).\", \"feedback\": \"I'm used using Python, but I usually need to double check many of the commands syntax, so it makes the process of generating the rules a little bit slower.\", \"other\": \"I liked it.\", \"metric\": \"expressivity\", \"value\": 4}, {\"participant\": \"p4\", \"condition\": \"ruler\", \"comments\": \"\", \"how to improve\": \"Allowing the addition of:\\n1)  \\\"negative examples rules\\\";\\n2) general rules (not associated to any specific example);\\n3) a \\\"python window\\\" in which you could use python code (as used in Snorkel), thus explore best of both worlds (the easiness and speed of current Ruler, and the expressiveness of Snorkel)\", \"feedback\": \"I enjoyed using the tool! I could quickly define a set of rules with reasonable Precision/Recall over the available data. It would take much longer to get to the same performance without the help of Ruler. \\nOne type of rule that I could not create is for negative examples. I tried to create a rule that would be a negative example of spam. In the controlled experiment scenario (as it is a binary classification task) I could get the same effect by set the \\\"negative example rule\\\" for one class as a \\\"positive example\\\" rule to the other class.\\nAlso, I tried to create a rule (based on my domain knowledge) that was not specifically associated to a instance, but I could not.\", \"other\": \"\", \"metric\": \"expressivity\", \"value\": 4}, {\"participant\": \"p6\", \"condition\": \"ruler\", \"comments\": \"\", \"how to improve\": \"1. Give formal definitions to key terms. \\n2. Prepare a step-by-step tutorial.  \", \"feedback\": \"Work\\n1. The tool can capture keyword-based functions. \\n2. The tool supports AND and OR operators. \\n\\nDidn't work\\n1. Some terms are not well-defined (e.g. Concept).\\n2. The tool lacks step-by-step documents.  \", \"other\": \"The tool is cool. \", \"metric\": \"expressivity\", \"value\": 4}, {\"participant\": \"p6\", \"condition\": \"snorkel\", \"comments\": \"\", \"how to improve\": \"1. Reduce unnecessary coding as much as possible. \\n2. Make the evaluation of labeling functions instant. \", \"feedback\": \"Worked\\n1. Snorkel supports python that I am familiar with. \\n2. Snorkel enables me to leverage programming skills to label data.\\n\\nDidn't work\\n1. Snorkel is coding intensive that I have to run multiple Snorkel cells to evaluate labelling functions. \\n2. Snorkel does not instantly evaluate labeling functions. I have to rerun evaluation codes each time I update labeling functions. \", \"other\": \"The Snorkel tool is cool. \", \"metric\": \"expressivity\", \"value\": 4}, {\"participant\": \"p5\", \"condition\": \"ruler\", \"comments\": \"\", \"how to improve\": \"\", \"feedback\": \"\", \"other\": \"\", \"metric\": \"expressivity\", \"value\": 3}, {\"participant\": \"p5\", \"condition\": \"snorkel\", \"comments\": \"\", \"how to improve\": \"\", \"feedback\": \"\", \"other\": \"\", \"metric\": \"expressivity\", \"value\": 5}, {\"participant\": \"p0\", \"condition\": \"ruler\", \"comments\": \"It was great exercise for me! thank you!\", \"how to improve\": \"When I mouse over token(s), I wished I had a popup to categorize it\", \"feedback\": \"Categorizing tokens was hard.for me\", \"other\": \"\", \"metric\": \"expressivity\", \"value\": 4}, {\"participant\": \"p0\", \"condition\": \"snorkel\", \"comments\": \"Great experiments! I will look into snorkel as I have some ML tasks. Thanks!\", \"how to improve\": \"If non-essential codes (e.g. evaluation codes) were defined out of the notebooks, they would be more easy to understand.\", \"feedback\": \"I firstly write functions so that recalls get high.\", \"other\": \"\", \"metric\": \"expressivity\", \"value\": 5}, {\"participant\": \"p1\", \"condition\": \"snorkel\", \"comments\": \"\", \"how to improve\": \"\", \"feedback\": \"\", \"other\": \"\", \"metric\": \"expressivity\", \"value\": 4}, {\"participant\": \"p1\", \"condition\": \"ruler\", \"comments\": \"\", \"how to improve\": \"Scalability: the system becomes slower towards the end. Further optimization & approximation could be considered.\", \"feedback\": \"I noticed that my label accuracy did not constantly improving: first improves and then drops. Maybe this is just an extreme case, but I feel it is important to validate if other users also show similar trend.\", \"other\": \"\", \"metric\": \"expressivity\", \"value\": 4}, {\"participant\": \"p2\", \"condition\": \"snorkel\", \"comments\": \"\", \"how to improve\": \"I was confused with the metric shown in the second block of the Apply function section. The comment \\\"Don't worry\\\" was not enough for me to disregard the value. :p\\n\\nMy work was going back and forth between labeling function, applying function and training a classifier. I would be helpful if the cells that the user runs are compiled into a single function (on a single cell) so I could simply call the function.\\n\\nFor example, prepare a function that traverses the namespace to list up any functions that begin with lf (or a longer prefix if it conflicts with something).\\n\", \"feedback\": \"My strategy was to directly take a look at examples and came up with salient words/phrases to write down keyword-based labeling functions. \", \"other\": \"The instructions are clear and the user study is organized well. I'm curious about the (psychological) effect of the time limit and being monitored/recorded.\\n\\nI may not pay attention but the input argument `x` of each labeling function was not clear at the beginning, which took a couple of minutes to figure out.\", \"metric\": \"expressivity\", \"value\": 4}, {\"participant\": \"p2\", \"condition\": \"ruler\", \"comments\": \"\", \"how to improve\": \"1)\\nIt was not very intuitive how the system makes use of the labeling functions I made. What I was confused is when I saw the recall of class 0 dropped after adding labeling function for class 0. I had to conduct label-function engineering to figure out the best combination.\\n\\n2)\\nAs I asked during the user study, it would be helpful if I could directly add rules that are not activated by the current example (which I could with Snorkel.)\\n\\n3)\\nSomehow, it seems that the data has more positive examples than negative examples. It will be helpful if the system has a search function to retrieve a negative example (that contains certain words etc.)\\n\\n4)\\nI found several examples from which I didn't want to create labeling functions, but I would like to simply label the examples. This may be for the user study but only allowing the user to create labeling functions may not be the best way.\\n\\n5)\\nSimilarly, in practice, it should be better having a base classifier/base dictionary as a starting point. For example (I worked on a sentiment analysis task today), using a pre-trained classifier (trained on other sentiment classifier dataset) and/and/or using sentiment dictionary that contains words with sentiment polarity information. I feel like what I did with the system is approximately close to reconstructing an affective dictionary from scratch (tuned toward the dataset, in one sense.)\\n\", \"feedback\": \"My strategy is simply adding salient words/phrases of each class while monitoring the dev set performance. As I added labeling functions, I tried to create more detailed rules as I progress.\", \"other\": \"The user study was well organized and instructions were clear.\\n\\nI'm wondering if the user study randomly shuffles the order of methods. Although two tools/datasets are different, I feel like I was more prepared to work on the task.\", \"metric\": \"expressivity\", \"value\": 3}, {\"participant\": \"p8\", \"condition\": \"snorkel\", \"comments\": \"\", \"how to improve\": \"Providing more stats/exploration options in terms of helping the user improve coverage. Instead of simply showing overlapping and conflicts, it would be better to see the samples and stats of two LFs by their overlap or conflict. Also it could automatically search for ensembling the labeling functions and provide suggestions to the user.\", \"feedback\": \"I tried to create a bunch of labeling functions on filtering by certain tokens at beginning and see how each works; then I modified those that turn out to have most wrong labels and got better results. Also I tried to have labeling functions on length and non-letters of the text, which seems to be not very useful. I should have tried to ensemble the labeling functions, which would have improve the coverage a lot.\", \"other\": \"\", \"metric\": \"expressivity\", \"value\": 2}, {\"participant\": \"p8\", \"condition\": \"ruler\", \"comments\": \"\", \"how to improve\": \"Like snorkel, I think auto suggestions on improving the coverage of LFs could be very helpful. And it may be useful to allow users combine LFs or edit LFs in python in order to reduce the execution time and to be more flexible for users with coding experience. The regex concept should be very useful but I only use it for several times, and I think it would be great if the tool can suggest regex expressions based on user's annotation.\", \"feedback\": \"I think it is task dependent. For sentiment analysis, the coverage of labeling functions on certain tokens or phrases can be relatively small and may not be accurate as there are many variants in the phrase and the negation would affect the result. \", \"other\": \"I feel that there could be 10-15 more time after the tutorial for users to play with the tool on the example task. In both user studies I figured out ways to improve the performance shortly after it is finished.\", \"metric\": \"expressivity\", \"value\": 2}, {\"participant\": \"p7\", \"condition\": \"ruler\", \"comments\": \"\", \"how to improve\": \"1. summary panel for labeling function helping to group and delete/add LFs. 2. adding LFs not by examples should be combined\", \"feedback\": \"\", \"other\": \"\", \"metric\": \"ease of learning\", \"value\": 4}, {\"participant\": \"p7\", \"condition\": \"snorkel\", \"comments\": \"\", \"how to improve\": \"\", \"feedback\": \"\", \"other\": \"\", \"metric\": \"ease of learning\", \"value\": 4}, {\"participant\": \"p3\", \"condition\": \"ruler\", \"comments\": \"Thanks!\", \"how to improve\": \"Below are the bugs we discussed. I am not suggesting they all need to be fixed :).\\n1. When I submitted a value modification for a concept row, the span annotations in the text did not update unless a new span could be identified in the text after the modification. Example: changing \\\"firearm\\\" to \\\"fire\\\" would cause the annotations to update, but a change from \\\"firearm\\\" to \\\"filkjwerlkjsdf\\\" would not cause an update.\\n2. I could not delete concept rows.\\n\\nBelow are some possible improvements I can think of:\\n1. I wasn't always sure when a concept modification had taken effect. It would be nice if there were some indication in the UI that the concept modification was in fact affect the output.\\n2. We could show a plot of the historical performance of the model over time. The plot could be a multi-line chart of statistics in the top-right corner on the y-axis and the chronological change id on the x-axis. Then, if a user were to click on a point in the plot, they would see a modal that would ask them if they would like to download the model from that point in time.\\n3. It would be nice to see class-specific statistics for snorkel's labelling model.\\n4. Should we give the user the option to select \\\"weighted average\\\" or \\\"simple average\\\" for the statistics on the entire development set?\\n5. I do think that organizing labelling functions into concepts helped quite a bit. Users who are using concepts to organize their functions (and thus modifying their concepts over time) would want to see diffs for the statistics after each concept modification.\", \"feedback\": \"I wonder if I should have added more functions more quickly and done more pruning given the diffs.\\n\\nI do think that organizing labelling functions into concepts helped quite a bit. However, users who are using concepts to organize their functions (and thus modifying their concepts over time) would want to see diffs for the statistics after each concept modification.\", \"other\": \"I thought it was conducted very well!\", \"metric\": \"ease of learning\", \"value\": 5}, {\"participant\": \"p3\", \"condition\": \"snorkel\", \"comments\": \"We were having trouble determining why Snorkel was telling us we had classified 94 positive results correctly and 0 incorrectly but achieved only 47% accuracy. I think Snorkel was saying we correctly labeled 94 of the 94 actual positive examples, and 47% of the examples we identified as positive were actually positive. In other words, Snorkel was telling us that our recall was 100% and our precision was 47%.\", \"how to improve\": \"They may already have this, but I would add the ability to pass in your own metric definitions to the evaluation step\", \"feedback\": \"What worked:\\n1. Writing my own functions to analyze why labeling functions were making incorrect predictions\\n2. I didn't have time to do this, but I would have probably analyzed the model performance metrics myself. We ran into a problem where we weren't entirely sure how the metrics were being calculated, and so I would probably calculate them myself to have complete understanding.\", \"other\": \"The study was great! I would use the \\\"Table of Contents (2)\\\" extension to enable the users to more easily navigate the Jupyter Notebook.\\n\\nhttps://jupyter-contrib-nbextensions.readthedocs.io/en/latest/nbextensions/toc2/README.html\\n\\nhttps://jupyter-contrib-nbextensions.readthedocs.io/en/latest/\", \"metric\": \"ease of learning\", \"value\": 4}, {\"participant\": \"p9\", \"condition\": \"snorkel\", \"comments\": \"\", \"how to improve\": \"I would show statistics about term frequency to users to help writing labeling functions. Also, I would like to make some helper functions to get synonyms and antonyms easily available, so that a user can improve coverage of simple keyword-matching strategy.\", \"feedback\": \"It was hard to write complex functions because of the time limit. I wanted to see overall statistics of the term frequency, but I was not able to check the statistics easily in time.\\nSo, I just used simple keyword matching as labeling functions\", \"other\": \"Tutorial and explanations were thorough. The researcher remained unbiased and objective.\", \"metric\": \"ease of learning\", \"value\": 2}, {\"participant\": \"p9\", \"condition\": \"ruler\", \"comments\": \"\", \"how to improve\": \"I'd add the 'not have' condition. It was hard to find out 'must-have' keywords for the 'not spam' label. I'd like to add some statistical characteristics (word count, text length) of a data record and synthesize in/equalities using them.\", \"feedback\": \"I tried to construct concepts with relevant keywords for each label. This strategy worked for certain label (spam) but didn't work well for another label (not spam). I also tried to build rules based on entity labels, but it didn't work well.\", \"other\": \"\", \"metric\": \"ease of learning\", \"value\": 5}, {\"participant\": \"p4\", \"condition\": \"snorkel\", \"comments\": \"\", \"how to improve\": \"The pre-defined functions provided by the responsible for the experiment helped a lot. Having more pre-defined functions would be very useful (even for programmers).\", \"feedback\": \"I'm used using Python, but I usually need to double check many of the commands syntax, so it makes the process of generating the rules a little bit slower.\", \"other\": \"I liked it.\", \"metric\": \"ease of learning\", \"value\": 4}, {\"participant\": \"p4\", \"condition\": \"ruler\", \"comments\": \"\", \"how to improve\": \"Allowing the addition of:\\n1)  \\\"negative examples rules\\\";\\n2) general rules (not associated to any specific example);\\n3) a \\\"python window\\\" in which you could use python code (as used in Snorkel), thus explore best of both worlds (the easiness and speed of current Ruler, and the expressiveness of Snorkel)\", \"feedback\": \"I enjoyed using the tool! I could quickly define a set of rules with reasonable Precision/Recall over the available data. It would take much longer to get to the same performance without the help of Ruler. \\nOne type of rule that I could not create is for negative examples. I tried to create a rule that would be a negative example of spam. In the controlled experiment scenario (as it is a binary classification task) I could get the same effect by set the \\\"negative example rule\\\" for one class as a \\\"positive example\\\" rule to the other class.\\nAlso, I tried to create a rule (based on my domain knowledge) that was not specifically associated to a instance, but I could not.\", \"other\": \"\", \"metric\": \"ease of learning\", \"value\": 5}, {\"participant\": \"p6\", \"condition\": \"ruler\", \"comments\": \"\", \"how to improve\": \"1. Give formal definitions to key terms. \\n2. Prepare a step-by-step tutorial.  \", \"feedback\": \"Work\\n1. The tool can capture keyword-based functions. \\n2. The tool supports AND and OR operators. \\n\\nDidn't work\\n1. Some terms are not well-defined (e.g. Concept).\\n2. The tool lacks step-by-step documents.  \", \"other\": \"The tool is cool. \", \"metric\": \"ease of learning\", \"value\": 3}, {\"participant\": \"p6\", \"condition\": \"snorkel\", \"comments\": \"\", \"how to improve\": \"1. Reduce unnecessary coding as much as possible. \\n2. Make the evaluation of labeling functions instant. \", \"feedback\": \"Worked\\n1. Snorkel supports python that I am familiar with. \\n2. Snorkel enables me to leverage programming skills to label data.\\n\\nDidn't work\\n1. Snorkel is coding intensive that I have to run multiple Snorkel cells to evaluate labelling functions. \\n2. Snorkel does not instantly evaluate labeling functions. I have to rerun evaluation codes each time I update labeling functions. \", \"other\": \"The Snorkel tool is cool. \", \"metric\": \"ease of learning\", \"value\": 2}, {\"participant\": \"p5\", \"condition\": \"ruler\", \"comments\": \"\", \"how to improve\": \"\", \"feedback\": \"\", \"other\": \"\", \"metric\": \"ease of learning\", \"value\": 5}, {\"participant\": \"p5\", \"condition\": \"snorkel\", \"comments\": \"\", \"how to improve\": \"\", \"feedback\": \"\", \"other\": \"\", \"metric\": \"ease of learning\", \"value\": 5}, {\"participant\": \"p0\", \"condition\": \"ruler\", \"comments\": \"It was great exercise for me! thank you!\", \"how to improve\": \"When I mouse over token(s), I wished I had a popup to categorize it\", \"feedback\": \"Categorizing tokens was hard.for me\", \"other\": \"\", \"metric\": \"ease of learning\", \"value\": 2}, {\"participant\": \"p0\", \"condition\": \"snorkel\", \"comments\": \"Great experiments! I will look into snorkel as I have some ML tasks. Thanks!\", \"how to improve\": \"If non-essential codes (e.g. evaluation codes) were defined out of the notebooks, they would be more easy to understand.\", \"feedback\": \"I firstly write functions so that recalls get high.\", \"other\": \"\", \"metric\": \"ease of learning\", \"value\": 4}, {\"participant\": \"p1\", \"condition\": \"snorkel\", \"comments\": \"\", \"how to improve\": \"\", \"feedback\": \"\", \"other\": \"\", \"metric\": \"ease of learning\", \"value\": 3}, {\"participant\": \"p1\", \"condition\": \"ruler\", \"comments\": \"\", \"how to improve\": \"Scalability: the system becomes slower towards the end. Further optimization & approximation could be considered.\", \"feedback\": \"I noticed that my label accuracy did not constantly improving: first improves and then drops. Maybe this is just an extreme case, but I feel it is important to validate if other users also show similar trend.\", \"other\": \"\", \"metric\": \"ease of learning\", \"value\": 4}, {\"participant\": \"p2\", \"condition\": \"snorkel\", \"comments\": \"\", \"how to improve\": \"I was confused with the metric shown in the second block of the Apply function section. The comment \\\"Don't worry\\\" was not enough for me to disregard the value. :p\\n\\nMy work was going back and forth between labeling function, applying function and training a classifier. I would be helpful if the cells that the user runs are compiled into a single function (on a single cell) so I could simply call the function.\\n\\nFor example, prepare a function that traverses the namespace to list up any functions that begin with lf (or a longer prefix if it conflicts with something).\\n\", \"feedback\": \"My strategy was to directly take a look at examples and came up with salient words/phrases to write down keyword-based labeling functions. \", \"other\": \"The instructions are clear and the user study is organized well. I'm curious about the (psychological) effect of the time limit and being monitored/recorded.\\n\\nI may not pay attention but the input argument `x` of each labeling function was not clear at the beginning, which took a couple of minutes to figure out.\", \"metric\": \"ease of learning\", \"value\": 2}, {\"participant\": \"p2\", \"condition\": \"ruler\", \"comments\": \"\", \"how to improve\": \"1)\\nIt was not very intuitive how the system makes use of the labeling functions I made. What I was confused is when I saw the recall of class 0 dropped after adding labeling function for class 0. I had to conduct label-function engineering to figure out the best combination.\\n\\n2)\\nAs I asked during the user study, it would be helpful if I could directly add rules that are not activated by the current example (which I could with Snorkel.)\\n\\n3)\\nSomehow, it seems that the data has more positive examples than negative examples. It will be helpful if the system has a search function to retrieve a negative example (that contains certain words etc.)\\n\\n4)\\nI found several examples from which I didn't want to create labeling functions, but I would like to simply label the examples. This may be for the user study but only allowing the user to create labeling functions may not be the best way.\\n\\n5)\\nSimilarly, in practice, it should be better having a base classifier/base dictionary as a starting point. For example (I worked on a sentiment analysis task today), using a pre-trained classifier (trained on other sentiment classifier dataset) and/and/or using sentiment dictionary that contains words with sentiment polarity information. I feel like what I did with the system is approximately close to reconstructing an affective dictionary from scratch (tuned toward the dataset, in one sense.)\\n\", \"feedback\": \"My strategy is simply adding salient words/phrases of each class while monitoring the dev set performance. As I added labeling functions, I tried to create more detailed rules as I progress.\", \"other\": \"The user study was well organized and instructions were clear.\\n\\nI'm wondering if the user study randomly shuffles the order of methods. Although two tools/datasets are different, I feel like I was more prepared to work on the task.\", \"metric\": \"ease of learning\", \"value\": 5}, {\"participant\": \"p8\", \"condition\": \"snorkel\", \"comments\": \"\", \"how to improve\": \"Providing more stats/exploration options in terms of helping the user improve coverage. Instead of simply showing overlapping and conflicts, it would be better to see the samples and stats of two LFs by their overlap or conflict. Also it could automatically search for ensembling the labeling functions and provide suggestions to the user.\", \"feedback\": \"I tried to create a bunch of labeling functions on filtering by certain tokens at beginning and see how each works; then I modified those that turn out to have most wrong labels and got better results. Also I tried to have labeling functions on length and non-letters of the text, which seems to be not very useful. I should have tried to ensemble the labeling functions, which would have improve the coverage a lot.\", \"other\": \"\", \"metric\": \"ease of learning\", \"value\": 4}, {\"participant\": \"p8\", \"condition\": \"ruler\", \"comments\": \"\", \"how to improve\": \"Like snorkel, I think auto suggestions on improving the coverage of LFs could be very helpful. And it may be useful to allow users combine LFs or edit LFs in python in order to reduce the execution time and to be more flexible for users with coding experience. The regex concept should be very useful but I only use it for several times, and I think it would be great if the tool can suggest regex expressions based on user's annotation.\", \"feedback\": \"I think it is task dependent. For sentiment analysis, the coverage of labeling functions on certain tokens or phrases can be relatively small and may not be accurate as there are many variants in the phrase and the negation would affect the result. \", \"other\": \"I feel that there could be 10-15 more time after the tutorial for users to play with the tool on the example task. In both user studies I figured out ways to improve the performance shortly after it is finished.\", \"metric\": \"ease of learning\", \"value\": 5}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.FacetChart(...)"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_bars = alt.Chart(df_qm).mark_errorbar(extent='stderr').encode(\n",
    "  x=alt.X('value:Q'),\n",
    "  y=alt.Y('condition:N'),\n",
    "      color=alt.Color('condition:N', sort=['ruler'])\n",
    ").properties(width=400,height=100)\n",
    "\n",
    "points = alt.Chart(df_qm).mark_point(filled=True).encode(\n",
    "  x=alt.X('value:Q', aggregate='mean'),\n",
    "  y=alt.Y('condition:N'),\n",
    "    color=alt.Color('condition:N', sort=['ruler'])\n",
    ").properties(width=400,height=100)\n",
    "\n",
    "(error_bars + points).facet(\n",
    "    facet= alt.Facet('metric:N',sort=['ease of use', 'expressivity', 'ease of learning', 'overall']),\n",
    "    columns=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XQGBU4iUPC7G"
   },
   "source": [
    "### Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IJzTFkkBPMM5"
   },
   "source": [
    "We'll perform an analysis similar to what we did with the model performance metrics.  Let's start with **expressivity**.\n",
    "\n",
    "#### Expressivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "colab_type": "code",
    "id": "lNPdOWUnPKo_",
    "outputId": "bd8df33a-0aea-4f44-dd1b-13a968e9a40b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-2d59cc50a7c64ba8847a92bb028e81c1\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-2d59cc50a7c64ba8847a92bb028e81c1\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-2d59cc50a7c64ba8847a92bb028e81c1\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"layer\": [{\"mark\": {\"type\": \"errorbar\", \"extent\": \"stderr\"}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"condition\", \"sort\": [\"ruler\"]}, \"x\": {\"type\": \"quantitative\", \"field\": \"expressivity\"}, \"y\": {\"type\": \"nominal\", \"field\": \"condition\"}}}, {\"mark\": {\"type\": \"point\", \"filled\": true}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"condition\", \"sort\": [\"ruler\"]}, \"x\": {\"type\": \"quantitative\", \"aggregate\": \"mean\", \"field\": \"expressivity\"}, \"y\": {\"type\": \"nominal\", \"field\": \"condition\"}}}], \"data\": {\"name\": \"data-bf287e31fd84c14b647e1d12f1a39ff4\"}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-bf287e31fd84c14b647e1d12f1a39ff4\": [{\"condition\": \"ruler\", \"overall satisfaction\": 4, \"ease of use\": 4, \"expressivity\": 2, \"ease of learning\": 4, \"feedback\": \"\", \"how to improve\": \"1. summary panel for labeling function helping to group and delete/add LFs. 2. adding LFs not by examples should be combined\", \"other\": \"\", \"comments\": \"\", \"participant\": \"p7\"}, {\"condition\": \"snorkel\", \"overall satisfaction\": 3, \"ease of use\": 2, \"expressivity\": 5, \"ease of learning\": 4, \"feedback\": \"\", \"how to improve\": \"\", \"other\": \"\", \"comments\": \"\", \"participant\": \"p7\"}, {\"condition\": \"ruler\", \"overall satisfaction\": 5, \"ease of use\": 5, \"expressivity\": 5, \"ease of learning\": 5, \"feedback\": \"I wonder if I should have added more functions more quickly and done more pruning given the diffs.\\n\\nI do think that organizing labelling functions into concepts helped quite a bit. However, users who are using concepts to organize their functions (and thus modifying their concepts over time) would want to see diffs for the statistics after each concept modification.\", \"how to improve\": \"Below are the bugs we discussed. I am not suggesting they all need to be fixed :).\\n1. When I submitted a value modification for a concept row, the span annotations in the text did not update unless a new span could be identified in the text after the modification. Example: changing \\\"firearm\\\" to \\\"fire\\\" would cause the annotations to update, but a change from \\\"firearm\\\" to \\\"filkjwerlkjsdf\\\" would not cause an update.\\n2. I could not delete concept rows.\\n\\nBelow are some possible improvements I can think of:\\n1. I wasn't always sure when a concept modification had taken effect. It would be nice if there were some indication in the UI that the concept modification was in fact affect the output.\\n2. We could show a plot of the historical performance of the model over time. The plot could be a multi-line chart of statistics in the top-right corner on the y-axis and the chronological change id on the x-axis. Then, if a user were to click on a point in the plot, they would see a modal that would ask them if they would like to download the model from that point in time.\\n3. It would be nice to see class-specific statistics for snorkel's labelling model.\\n4. Should we give the user the option to select \\\"weighted average\\\" or \\\"simple average\\\" for the statistics on the entire development set?\\n5. I do think that organizing labelling functions into concepts helped quite a bit. Users who are using concepts to organize their functions (and thus modifying their concepts over time) would want to see diffs for the statistics after each concept modification.\", \"other\": \"I thought it was conducted very well!\", \"comments\": \"Thanks!\", \"participant\": \"p3\"}, {\"condition\": \"snorkel\", \"overall satisfaction\": 4, \"ease of use\": 4, \"expressivity\": 5, \"ease of learning\": 4, \"feedback\": \"What worked:\\n1. Writing my own functions to analyze why labeling functions were making incorrect predictions\\n2. I didn't have time to do this, but I would have probably analyzed the model performance metrics myself. We ran into a problem where we weren't entirely sure how the metrics were being calculated, and so I would probably calculate them myself to have complete understanding.\", \"how to improve\": \"They may already have this, but I would add the ability to pass in your own metric definitions to the evaluation step\", \"other\": \"The study was great! I would use the \\\"Table of Contents (2)\\\" extension to enable the users to more easily navigate the Jupyter Notebook.\\n\\nhttps://jupyter-contrib-nbextensions.readthedocs.io/en/latest/nbextensions/toc2/README.html\\n\\nhttps://jupyter-contrib-nbextensions.readthedocs.io/en/latest/\", \"comments\": \"We were having trouble determining why Snorkel was telling us we had classified 94 positive results correctly and 0 incorrectly but achieved only 47% accuracy. I think Snorkel was saying we correctly labeled 94 of the 94 actual positive examples, and 47% of the examples we identified as positive were actually positive. In other words, Snorkel was telling us that our recall was 100% and our precision was 47%.\", \"participant\": \"p3\"}, {\"condition\": \"snorkel\", \"overall satisfaction\": 3, \"ease of use\": 2, \"expressivity\": 5, \"ease of learning\": 2, \"feedback\": \"It was hard to write complex functions because of the time limit. I wanted to see overall statistics of the term frequency, but I was not able to check the statistics easily in time.\\nSo, I just used simple keyword matching as labeling functions\", \"how to improve\": \"I would show statistics about term frequency to users to help writing labeling functions. Also, I would like to make some helper functions to get synonyms and antonyms easily available, so that a user can improve coverage of simple keyword-matching strategy.\", \"other\": \"Tutorial and explanations were thorough. The researcher remained unbiased and objective.\", \"comments\": \"\", \"participant\": \"p9\"}, {\"condition\": \"ruler\", \"overall satisfaction\": 4, \"ease of use\": 5, \"expressivity\": 4, \"ease of learning\": 5, \"feedback\": \"I tried to construct concepts with relevant keywords for each label. This strategy worked for certain label (spam) but didn't work well for another label (not spam). I also tried to build rules based on entity labels, but it didn't work well.\", \"how to improve\": \"I'd add the 'not have' condition. It was hard to find out 'must-have' keywords for the 'not spam' label. I'd like to add some statistical characteristics (word count, text length) of a data record and synthesize in/equalities using them.\", \"other\": \"\", \"comments\": \"\", \"participant\": \"p9\"}, {\"condition\": \"snorkel\", \"overall satisfaction\": 3, \"ease of use\": 3, \"expressivity\": 4, \"ease of learning\": 4, \"feedback\": \"I'm used using Python, but I usually need to double check many of the commands syntax, so it makes the process of generating the rules a little bit slower.\", \"how to improve\": \"The pre-defined functions provided by the responsible for the experiment helped a lot. Having more pre-defined functions would be very useful (even for programmers).\", \"other\": \"I liked it.\", \"comments\": \"\", \"participant\": \"p4\"}, {\"condition\": \"ruler\", \"overall satisfaction\": 5, \"ease of use\": 5, \"expressivity\": 4, \"ease of learning\": 5, \"feedback\": \"I enjoyed using the tool! I could quickly define a set of rules with reasonable Precision/Recall over the available data. It would take much longer to get to the same performance without the help of Ruler. \\nOne type of rule that I could not create is for negative examples. I tried to create a rule that would be a negative example of spam. In the controlled experiment scenario (as it is a binary classification task) I could get the same effect by set the \\\"negative example rule\\\" for one class as a \\\"positive example\\\" rule to the other class.\\nAlso, I tried to create a rule (based on my domain knowledge) that was not specifically associated to a instance, but I could not.\", \"how to improve\": \"Allowing the addition of:\\n1)  \\\"negative examples rules\\\";\\n2) general rules (not associated to any specific example);\\n3) a \\\"python window\\\" in which you could use python code (as used in Snorkel), thus explore best of both worlds (the easiness and speed of current Ruler, and the expressiveness of Snorkel)\", \"other\": \"\", \"comments\": \"\", \"participant\": \"p4\"}, {\"condition\": \"ruler\", \"overall satisfaction\": 5, \"ease of use\": 4, \"expressivity\": 4, \"ease of learning\": 3, \"feedback\": \"Work\\n1. The tool can capture keyword-based functions. \\n2. The tool supports AND and OR operators. \\n\\nDidn't work\\n1. Some terms are not well-defined (e.g. Concept).\\n2. The tool lacks step-by-step documents.  \", \"how to improve\": \"1. Give formal definitions to key terms. \\n2. Prepare a step-by-step tutorial.  \", \"other\": \"The tool is cool. \", \"comments\": \"\", \"participant\": \"p6\"}, {\"condition\": \"snorkel\", \"overall satisfaction\": 4, \"ease of use\": 4, \"expressivity\": 4, \"ease of learning\": 2, \"feedback\": \"Worked\\n1. Snorkel supports python that I am familiar with. \\n2. Snorkel enables me to leverage programming skills to label data.\\n\\nDidn't work\\n1. Snorkel is coding intensive that I have to run multiple Snorkel cells to evaluate labelling functions. \\n2. Snorkel does not instantly evaluate labeling functions. I have to rerun evaluation codes each time I update labeling functions. \", \"how to improve\": \"1. Reduce unnecessary coding as much as possible. \\n2. Make the evaluation of labeling functions instant. \", \"other\": \"The Snorkel tool is cool. \", \"comments\": \"\", \"participant\": \"p6\"}, {\"condition\": \"ruler\", \"overall satisfaction\": 3, \"ease of use\": 5, \"expressivity\": 3, \"ease of learning\": 5, \"feedback\": \"\", \"how to improve\": \"\", \"other\": \"\", \"comments\": \"\", \"participant\": \"p5\"}, {\"condition\": \"snorkel\", \"overall satisfaction\": 5, \"ease of use\": 4, \"expressivity\": 5, \"ease of learning\": 5, \"feedback\": \"\", \"how to improve\": \"\", \"other\": \"\", \"comments\": \"\", \"participant\": \"p5\"}, {\"condition\": \"ruler\", \"overall satisfaction\": 4, \"ease of use\": 3, \"expressivity\": 4, \"ease of learning\": 2, \"feedback\": \"Categorizing tokens was hard.for me\", \"how to improve\": \"When I mouse over token(s), I wished I had a popup to categorize it\", \"other\": \"\", \"comments\": \"It was great exercise for me! thank you!\", \"participant\": \"p0\"}, {\"condition\": \"snorkel\", \"overall satisfaction\": 5, \"ease of use\": 4, \"expressivity\": 5, \"ease of learning\": 4, \"feedback\": \"I firstly write functions so that recalls get high.\", \"how to improve\": \"If non-essential codes (e.g. evaluation codes) were defined out of the notebooks, they would be more easy to understand.\", \"other\": \"\", \"comments\": \"Great experiments! I will look into snorkel as I have some ML tasks. Thanks!\", \"participant\": \"p0\"}, {\"condition\": \"snorkel\", \"overall satisfaction\": 2, \"ease of use\": 3, \"expressivity\": 4, \"ease of learning\": 3, \"feedback\": \"\", \"how to improve\": \"\", \"other\": \"\", \"comments\": \"\", \"participant\": \"p1\"}, {\"condition\": \"ruler\", \"overall satisfaction\": 4, \"ease of use\": 5, \"expressivity\": 4, \"ease of learning\": 4, \"feedback\": \"I noticed that my label accuracy did not constantly improving: first improves and then drops. Maybe this is just an extreme case, but I feel it is important to validate if other users also show similar trend.\", \"how to improve\": \"Scalability: the system becomes slower towards the end. Further optimization & approximation could be considered.\", \"other\": \"\", \"comments\": \"\", \"participant\": \"p1\"}, {\"condition\": \"snorkel\", \"overall satisfaction\": 2, \"ease of use\": 2, \"expressivity\": 4, \"ease of learning\": 2, \"feedback\": \"My strategy was to directly take a look at examples and came up with salient words/phrases to write down keyword-based labeling functions. \", \"how to improve\": \"I was confused with the metric shown in the second block of the Apply function section. The comment \\\"Don't worry\\\" was not enough for me to disregard the value. :p\\n\\nMy work was going back and forth between labeling function, applying function and training a classifier. I would be helpful if the cells that the user runs are compiled into a single function (on a single cell) so I could simply call the function.\\n\\nFor example, prepare a function that traverses the namespace to list up any functions that begin with lf (or a longer prefix if it conflicts with something).\\n\", \"other\": \"The instructions are clear and the user study is organized well. I'm curious about the (psychological) effect of the time limit and being monitored/recorded.\\n\\nI may not pay attention but the input argument `x` of each labeling function was not clear at the beginning, which took a couple of minutes to figure out.\", \"comments\": \"\", \"participant\": \"p2\"}, {\"condition\": \"ruler\", \"overall satisfaction\": 4, \"ease of use\": 5, \"expressivity\": 3, \"ease of learning\": 5, \"feedback\": \"My strategy is simply adding salient words/phrases of each class while monitoring the dev set performance. As I added labeling functions, I tried to create more detailed rules as I progress.\", \"how to improve\": \"1)\\nIt was not very intuitive how the system makes use of the labeling functions I made. What I was confused is when I saw the recall of class 0 dropped after adding labeling function for class 0. I had to conduct label-function engineering to figure out the best combination.\\n\\n2)\\nAs I asked during the user study, it would be helpful if I could directly add rules that are not activated by the current example (which I could with Snorkel.)\\n\\n3)\\nSomehow, it seems that the data has more positive examples than negative examples. It will be helpful if the system has a search function to retrieve a negative example (that contains certain words etc.)\\n\\n4)\\nI found several examples from which I didn't want to create labeling functions, but I would like to simply label the examples. This may be for the user study but only allowing the user to create labeling functions may not be the best way.\\n\\n5)\\nSimilarly, in practice, it should be better having a base classifier/base dictionary as a starting point. For example (I worked on a sentiment analysis task today), using a pre-trained classifier (trained on other sentiment classifier dataset) and/and/or using sentiment dictionary that contains words with sentiment polarity information. I feel like what I did with the system is approximately close to reconstructing an affective dictionary from scratch (tuned toward the dataset, in one sense.)\\n\", \"other\": \"The user study was well organized and instructions were clear.\\n\\nI'm wondering if the user study randomly shuffles the order of methods. Although two tools/datasets are different, I feel like I was more prepared to work on the task.\", \"comments\": \"\", \"participant\": \"p2\"}, {\"condition\": \"snorkel\", \"overall satisfaction\": 3, \"ease of use\": 4, \"expressivity\": 2, \"ease of learning\": 4, \"feedback\": \"I tried to create a bunch of labeling functions on filtering by certain tokens at beginning and see how each works; then I modified those that turn out to have most wrong labels and got better results. Also I tried to have labeling functions on length and non-letters of the text, which seems to be not very useful. I should have tried to ensemble the labeling functions, which would have improve the coverage a lot.\", \"how to improve\": \"Providing more stats/exploration options in terms of helping the user improve coverage. Instead of simply showing overlapping and conflicts, it would be better to see the samples and stats of two LFs by their overlap or conflict. Also it could automatically search for ensembling the labeling functions and provide suggestions to the user.\", \"other\": \"\", \"comments\": \"\", \"participant\": \"p8\"}, {\"condition\": \"ruler\", \"overall satisfaction\": 4, \"ease of use\": 4, \"expressivity\": 2, \"ease of learning\": 5, \"feedback\": \"I think it is task dependent. For sentiment analysis, the coverage of labeling functions on certain tokens or phrases can be relatively small and may not be accurate as there are many variants in the phrase and the negation would affect the result. \", \"how to improve\": \"Like snorkel, I think auto suggestions on improving the coverage of LFs could be very helpful. And it may be useful to allow users combine LFs or edit LFs in python in order to reduce the execution time and to be more flexible for users with coding experience. The regex concept should be very useful but I only use it for several times, and I think it would be great if the tool can suggest regex expressions based on user's annotation.\", \"other\": \"I feel that there could be 10-15 more time after the tutorial for users to play with the tool on the example task. In both user studies I figured out ways to improve the performance shortly after it is finished.\", \"comments\": \"\", \"participant\": \"p8\"}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_bars = alt.Chart(exit_survey).mark_errorbar(extent='stderr').encode(\n",
    "  x=alt.X('expressivity:Q'),\n",
    "  y=alt.Y('condition:N'),\n",
    "    color=alt.Color('condition:N', sort=['ruler'])\n",
    ")\n",
    "\n",
    "points = alt.Chart(exit_survey).mark_point(filled=True).encode(\n",
    "  x=alt.X('expressivity:Q', aggregate='mean'),\n",
    "  y=alt.Y('condition:N'),\n",
    " color=alt.Color('condition:N', sort=['ruler'])\n",
    ")\n",
    "\n",
    "error_bars + points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QwXb-56jQVsJ"
   },
   "source": [
    "It appears that subjects found **snorkel** more expressive than **ruler**. Let's test if this is statistically significant, which is what the figure suggests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ngmYq6vuPORn",
    "outputId": "01c66af0-f5bb-4ccc-cbef-2a2624dcd71b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_relResult(statistic=-2.4494897427831783, pvalue=0.03678749787978613)"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "ruler_expr =  exit_survey[exit_survey['condition']=='ruler']['expressivity']\n",
    "snorkel_expr = exit_survey[exit_survey['condition']=='snorkel']['expressivity']\n",
    "stats.ttest_rel(ruler_expr, snorkel_expr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0nM4CbSjQ8gH"
   },
   "source": [
    "OK. Participants found, as opined on a Likert scale of 5, **snorkel** significantly more expressive than **ruler** at **pvalue = 0.04 < 0.05**. Let's compute the effect size of the difference, which appears to be small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WqTtWgE7P_yl"
   },
   "outputs": [],
   "source": [
    "%R -i df_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "ZLMhN-yzQtNF",
    "outputId": "f626176e-1d4d-4636-9c78-cbf5367b665f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# A tibble: 1 x 7\n",
      "  .y.          group1 group2  effsize    n1    n2 magnitude\n",
      "* <chr>        <chr>  <chr>     <dbl> <int> <int> <ord>    \n",
      "1 expressivity ruler  snorkel  -0.775    10    10 moderate \n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "library(rstatix)\n",
    "cohens_d(df_q,expressivity~condition,paired=T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "colab_type": "code",
    "id": "bhyDJB7iQvTn",
    "outputId": "5519b033-9cd3-471f-85d9-37b84466c648"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tWilcoxon signed rank test with continuity correction\n",
      "\n",
      "data:  df_q$expressivity[df_q$condition == \"ruler\"] and df_q$expressivity[df_q$condition == \"snorkel\"]\n",
      "V = 0, p-value = 0.05447\n",
      "alternative hypothesis: true location shift is not equal to 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%R \n",
    "library(rstatix)\n",
    "wilcox.test(df_q$expressivity[df_q$condition==\"ruler\"],df_q$expressivity[df_q$condition==\"snorkel\"] , paired = TRUE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 77
    },
    "colab_type": "code",
    "id": "ucWO_rvcR4ql",
    "outputId": "7e44d297-f5af-44dc-ab1a-b24fc2429e80"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>.y.</th>\n",
       "      <th>group1</th>\n",
       "      <th>group2</th>\n",
       "      <th>effsize</th>\n",
       "      <th>n1</th>\n",
       "      <th>n2</th>\n",
       "      <th>magnitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>expressivity</td>\n",
       "      <td>ruler</td>\n",
       "      <td>snorkel</td>\n",
       "      <td>0.69843</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            .y. group1   group2  effsize  n1  n2 magnitude\n",
       "1  expressivity  ruler  snorkel  0.69843  10  10     large"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%R wilcox_effsize(df_q, expressivity~condition, paired=T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QovSsZLpSLia"
   },
   "source": [
    "We were wrong; we got a moderate effect size for the significace of the difference in expressivity. Let's move on to other subjective measures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XILD-MDYSzT8"
   },
   "source": [
    "####Ease of Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "colab_type": "code",
    "id": "nOuu06jKSH0W",
    "outputId": "7f65de49-54d2-4928-ef80-0cb634cd8692"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-6ffe99d7f068434e842e1a80757b4648\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-6ffe99d7f068434e842e1a80757b4648\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-6ffe99d7f068434e842e1a80757b4648\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"layer\": [{\"mark\": {\"type\": \"errorbar\", \"extent\": \"stderr\"}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"condition\", \"sort\": [\"ruler\"]}, \"x\": {\"type\": \"quantitative\", \"field\": \"ease of use\"}, \"y\": {\"type\": \"nominal\", \"field\": \"condition\"}}}, {\"mark\": {\"type\": \"point\", \"filled\": true}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"condition\", \"sort\": [\"ruler\"]}, \"x\": {\"type\": \"quantitative\", \"aggregate\": \"mean\", \"field\": \"ease of use\"}, \"y\": {\"type\": \"nominal\", \"field\": \"condition\"}}}], \"data\": {\"name\": \"data-bf287e31fd84c14b647e1d12f1a39ff4\"}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-bf287e31fd84c14b647e1d12f1a39ff4\": [{\"condition\": \"ruler\", \"overall satisfaction\": 4, \"ease of use\": 4, \"expressivity\": 2, \"ease of learning\": 4, \"feedback\": \"\", \"how to improve\": \"1. summary panel for labeling function helping to group and delete/add LFs. 2. adding LFs not by examples should be combined\", \"other\": \"\", \"comments\": \"\", \"participant\": \"p7\"}, {\"condition\": \"snorkel\", \"overall satisfaction\": 3, \"ease of use\": 2, \"expressivity\": 5, \"ease of learning\": 4, \"feedback\": \"\", \"how to improve\": \"\", \"other\": \"\", \"comments\": \"\", \"participant\": \"p7\"}, {\"condition\": \"ruler\", \"overall satisfaction\": 5, \"ease of use\": 5, \"expressivity\": 5, \"ease of learning\": 5, \"feedback\": \"I wonder if I should have added more functions more quickly and done more pruning given the diffs.\\n\\nI do think that organizing labelling functions into concepts helped quite a bit. However, users who are using concepts to organize their functions (and thus modifying their concepts over time) would want to see diffs for the statistics after each concept modification.\", \"how to improve\": \"Below are the bugs we discussed. I am not suggesting they all need to be fixed :).\\n1. When I submitted a value modification for a concept row, the span annotations in the text did not update unless a new span could be identified in the text after the modification. Example: changing \\\"firearm\\\" to \\\"fire\\\" would cause the annotations to update, but a change from \\\"firearm\\\" to \\\"filkjwerlkjsdf\\\" would not cause an update.\\n2. I could not delete concept rows.\\n\\nBelow are some possible improvements I can think of:\\n1. I wasn't always sure when a concept modification had taken effect. It would be nice if there were some indication in the UI that the concept modification was in fact affect the output.\\n2. We could show a plot of the historical performance of the model over time. The plot could be a multi-line chart of statistics in the top-right corner on the y-axis and the chronological change id on the x-axis. Then, if a user were to click on a point in the plot, they would see a modal that would ask them if they would like to download the model from that point in time.\\n3. It would be nice to see class-specific statistics for snorkel's labelling model.\\n4. Should we give the user the option to select \\\"weighted average\\\" or \\\"simple average\\\" for the statistics on the entire development set?\\n5. I do think that organizing labelling functions into concepts helped quite a bit. Users who are using concepts to organize their functions (and thus modifying their concepts over time) would want to see diffs for the statistics after each concept modification.\", \"other\": \"I thought it was conducted very well!\", \"comments\": \"Thanks!\", \"participant\": \"p3\"}, {\"condition\": \"snorkel\", \"overall satisfaction\": 4, \"ease of use\": 4, \"expressivity\": 5, \"ease of learning\": 4, \"feedback\": \"What worked:\\n1. Writing my own functions to analyze why labeling functions were making incorrect predictions\\n2. I didn't have time to do this, but I would have probably analyzed the model performance metrics myself. We ran into a problem where we weren't entirely sure how the metrics were being calculated, and so I would probably calculate them myself to have complete understanding.\", \"how to improve\": \"They may already have this, but I would add the ability to pass in your own metric definitions to the evaluation step\", \"other\": \"The study was great! I would use the \\\"Table of Contents (2)\\\" extension to enable the users to more easily navigate the Jupyter Notebook.\\n\\nhttps://jupyter-contrib-nbextensions.readthedocs.io/en/latest/nbextensions/toc2/README.html\\n\\nhttps://jupyter-contrib-nbextensions.readthedocs.io/en/latest/\", \"comments\": \"We were having trouble determining why Snorkel was telling us we had classified 94 positive results correctly and 0 incorrectly but achieved only 47% accuracy. I think Snorkel was saying we correctly labeled 94 of the 94 actual positive examples, and 47% of the examples we identified as positive were actually positive. In other words, Snorkel was telling us that our recall was 100% and our precision was 47%.\", \"participant\": \"p3\"}, {\"condition\": \"snorkel\", \"overall satisfaction\": 3, \"ease of use\": 2, \"expressivity\": 5, \"ease of learning\": 2, \"feedback\": \"It was hard to write complex functions because of the time limit. I wanted to see overall statistics of the term frequency, but I was not able to check the statistics easily in time.\\nSo, I just used simple keyword matching as labeling functions\", \"how to improve\": \"I would show statistics about term frequency to users to help writing labeling functions. Also, I would like to make some helper functions to get synonyms and antonyms easily available, so that a user can improve coverage of simple keyword-matching strategy.\", \"other\": \"Tutorial and explanations were thorough. The researcher remained unbiased and objective.\", \"comments\": \"\", \"participant\": \"p9\"}, {\"condition\": \"ruler\", \"overall satisfaction\": 4, \"ease of use\": 5, \"expressivity\": 4, \"ease of learning\": 5, \"feedback\": \"I tried to construct concepts with relevant keywords for each label. This strategy worked for certain label (spam) but didn't work well for another label (not spam). I also tried to build rules based on entity labels, but it didn't work well.\", \"how to improve\": \"I'd add the 'not have' condition. It was hard to find out 'must-have' keywords for the 'not spam' label. I'd like to add some statistical characteristics (word count, text length) of a data record and synthesize in/equalities using them.\", \"other\": \"\", \"comments\": \"\", \"participant\": \"p9\"}, {\"condition\": \"snorkel\", \"overall satisfaction\": 3, \"ease of use\": 3, \"expressivity\": 4, \"ease of learning\": 4, \"feedback\": \"I'm used using Python, but I usually need to double check many of the commands syntax, so it makes the process of generating the rules a little bit slower.\", \"how to improve\": \"The pre-defined functions provided by the responsible for the experiment helped a lot. Having more pre-defined functions would be very useful (even for programmers).\", \"other\": \"I liked it.\", \"comments\": \"\", \"participant\": \"p4\"}, {\"condition\": \"ruler\", \"overall satisfaction\": 5, \"ease of use\": 5, \"expressivity\": 4, \"ease of learning\": 5, \"feedback\": \"I enjoyed using the tool! I could quickly define a set of rules with reasonable Precision/Recall over the available data. It would take much longer to get to the same performance without the help of Ruler. \\nOne type of rule that I could not create is for negative examples. I tried to create a rule that would be a negative example of spam. In the controlled experiment scenario (as it is a binary classification task) I could get the same effect by set the \\\"negative example rule\\\" for one class as a \\\"positive example\\\" rule to the other class.\\nAlso, I tried to create a rule (based on my domain knowledge) that was not specifically associated to a instance, but I could not.\", \"how to improve\": \"Allowing the addition of:\\n1)  \\\"negative examples rules\\\";\\n2) general rules (not associated to any specific example);\\n3) a \\\"python window\\\" in which you could use python code (as used in Snorkel), thus explore best of both worlds (the easiness and speed of current Ruler, and the expressiveness of Snorkel)\", \"other\": \"\", \"comments\": \"\", \"participant\": \"p4\"}, {\"condition\": \"ruler\", \"overall satisfaction\": 5, \"ease of use\": 4, \"expressivity\": 4, \"ease of learning\": 3, \"feedback\": \"Work\\n1. The tool can capture keyword-based functions. \\n2. The tool supports AND and OR operators. \\n\\nDidn't work\\n1. Some terms are not well-defined (e.g. Concept).\\n2. The tool lacks step-by-step documents.  \", \"how to improve\": \"1. Give formal definitions to key terms. \\n2. Prepare a step-by-step tutorial.  \", \"other\": \"The tool is cool. \", \"comments\": \"\", \"participant\": \"p6\"}, {\"condition\": \"snorkel\", \"overall satisfaction\": 4, \"ease of use\": 4, \"expressivity\": 4, \"ease of learning\": 2, \"feedback\": \"Worked\\n1. Snorkel supports python that I am familiar with. \\n2. Snorkel enables me to leverage programming skills to label data.\\n\\nDidn't work\\n1. Snorkel is coding intensive that I have to run multiple Snorkel cells to evaluate labelling functions. \\n2. Snorkel does not instantly evaluate labeling functions. I have to rerun evaluation codes each time I update labeling functions. \", \"how to improve\": \"1. Reduce unnecessary coding as much as possible. \\n2. Make the evaluation of labeling functions instant. \", \"other\": \"The Snorkel tool is cool. \", \"comments\": \"\", \"participant\": \"p6\"}, {\"condition\": \"ruler\", \"overall satisfaction\": 3, \"ease of use\": 5, \"expressivity\": 3, \"ease of learning\": 5, \"feedback\": \"\", \"how to improve\": \"\", \"other\": \"\", \"comments\": \"\", \"participant\": \"p5\"}, {\"condition\": \"snorkel\", \"overall satisfaction\": 5, \"ease of use\": 4, \"expressivity\": 5, \"ease of learning\": 5, \"feedback\": \"\", \"how to improve\": \"\", \"other\": \"\", \"comments\": \"\", \"participant\": \"p5\"}, {\"condition\": \"ruler\", \"overall satisfaction\": 4, \"ease of use\": 3, \"expressivity\": 4, \"ease of learning\": 2, \"feedback\": \"Categorizing tokens was hard.for me\", \"how to improve\": \"When I mouse over token(s), I wished I had a popup to categorize it\", \"other\": \"\", \"comments\": \"It was great exercise for me! thank you!\", \"participant\": \"p0\"}, {\"condition\": \"snorkel\", \"overall satisfaction\": 5, \"ease of use\": 4, \"expressivity\": 5, \"ease of learning\": 4, \"feedback\": \"I firstly write functions so that recalls get high.\", \"how to improve\": \"If non-essential codes (e.g. evaluation codes) were defined out of the notebooks, they would be more easy to understand.\", \"other\": \"\", \"comments\": \"Great experiments! I will look into snorkel as I have some ML tasks. Thanks!\", \"participant\": \"p0\"}, {\"condition\": \"snorkel\", \"overall satisfaction\": 2, \"ease of use\": 3, \"expressivity\": 4, \"ease of learning\": 3, \"feedback\": \"\", \"how to improve\": \"\", \"other\": \"\", \"comments\": \"\", \"participant\": \"p1\"}, {\"condition\": \"ruler\", \"overall satisfaction\": 4, \"ease of use\": 5, \"expressivity\": 4, \"ease of learning\": 4, \"feedback\": \"I noticed that my label accuracy did not constantly improving: first improves and then drops. Maybe this is just an extreme case, but I feel it is important to validate if other users also show similar trend.\", \"how to improve\": \"Scalability: the system becomes slower towards the end. Further optimization & approximation could be considered.\", \"other\": \"\", \"comments\": \"\", \"participant\": \"p1\"}, {\"condition\": \"snorkel\", \"overall satisfaction\": 2, \"ease of use\": 2, \"expressivity\": 4, \"ease of learning\": 2, \"feedback\": \"My strategy was to directly take a look at examples and came up with salient words/phrases to write down keyword-based labeling functions. \", \"how to improve\": \"I was confused with the metric shown in the second block of the Apply function section. The comment \\\"Don't worry\\\" was not enough for me to disregard the value. :p\\n\\nMy work was going back and forth between labeling function, applying function and training a classifier. I would be helpful if the cells that the user runs are compiled into a single function (on a single cell) so I could simply call the function.\\n\\nFor example, prepare a function that traverses the namespace to list up any functions that begin with lf (or a longer prefix if it conflicts with something).\\n\", \"other\": \"The instructions are clear and the user study is organized well. I'm curious about the (psychological) effect of the time limit and being monitored/recorded.\\n\\nI may not pay attention but the input argument `x` of each labeling function was not clear at the beginning, which took a couple of minutes to figure out.\", \"comments\": \"\", \"participant\": \"p2\"}, {\"condition\": \"ruler\", \"overall satisfaction\": 4, \"ease of use\": 5, \"expressivity\": 3, \"ease of learning\": 5, \"feedback\": \"My strategy is simply adding salient words/phrases of each class while monitoring the dev set performance. As I added labeling functions, I tried to create more detailed rules as I progress.\", \"how to improve\": \"1)\\nIt was not very intuitive how the system makes use of the labeling functions I made. What I was confused is when I saw the recall of class 0 dropped after adding labeling function for class 0. I had to conduct label-function engineering to figure out the best combination.\\n\\n2)\\nAs I asked during the user study, it would be helpful if I could directly add rules that are not activated by the current example (which I could with Snorkel.)\\n\\n3)\\nSomehow, it seems that the data has more positive examples than negative examples. It will be helpful if the system has a search function to retrieve a negative example (that contains certain words etc.)\\n\\n4)\\nI found several examples from which I didn't want to create labeling functions, but I would like to simply label the examples. This may be for the user study but only allowing the user to create labeling functions may not be the best way.\\n\\n5)\\nSimilarly, in practice, it should be better having a base classifier/base dictionary as a starting point. For example (I worked on a sentiment analysis task today), using a pre-trained classifier (trained on other sentiment classifier dataset) and/and/or using sentiment dictionary that contains words with sentiment polarity information. I feel like what I did with the system is approximately close to reconstructing an affective dictionary from scratch (tuned toward the dataset, in one sense.)\\n\", \"other\": \"The user study was well organized and instructions were clear.\\n\\nI'm wondering if the user study randomly shuffles the order of methods. Although two tools/datasets are different, I feel like I was more prepared to work on the task.\", \"comments\": \"\", \"participant\": \"p2\"}, {\"condition\": \"snorkel\", \"overall satisfaction\": 3, \"ease of use\": 4, \"expressivity\": 2, \"ease of learning\": 4, \"feedback\": \"I tried to create a bunch of labeling functions on filtering by certain tokens at beginning and see how each works; then I modified those that turn out to have most wrong labels and got better results. Also I tried to have labeling functions on length and non-letters of the text, which seems to be not very useful. I should have tried to ensemble the labeling functions, which would have improve the coverage a lot.\", \"how to improve\": \"Providing more stats/exploration options in terms of helping the user improve coverage. Instead of simply showing overlapping and conflicts, it would be better to see the samples and stats of two LFs by their overlap or conflict. Also it could automatically search for ensembling the labeling functions and provide suggestions to the user.\", \"other\": \"\", \"comments\": \"\", \"participant\": \"p8\"}, {\"condition\": \"ruler\", \"overall satisfaction\": 4, \"ease of use\": 4, \"expressivity\": 2, \"ease of learning\": 5, \"feedback\": \"I think it is task dependent. For sentiment analysis, the coverage of labeling functions on certain tokens or phrases can be relatively small and may not be accurate as there are many variants in the phrase and the negation would affect the result. \", \"how to improve\": \"Like snorkel, I think auto suggestions on improving the coverage of LFs could be very helpful. And it may be useful to allow users combine LFs or edit LFs in python in order to reduce the execution time and to be more flexible for users with coding experience. The regex concept should be very useful but I only use it for several times, and I think it would be great if the tool can suggest regex expressions based on user's annotation.\", \"other\": \"I feel that there could be 10-15 more time after the tutorial for users to play with the tool on the example task. In both user studies I figured out ways to improve the performance shortly after it is finished.\", \"comments\": \"\", \"participant\": \"p8\"}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_bars = alt.Chart(exit_survey).mark_errorbar(extent='stderr').encode(\n",
    "  x=alt.X('ease of use:Q'),\n",
    "  y=alt.Y('condition:N'),\n",
    "    color=alt.Color('condition:N', sort=['ruler'])\n",
    ")\n",
    "\n",
    "points = alt.Chart(exit_survey).mark_point(filled=True).encode(\n",
    "  x=alt.X('ease of use:Q', aggregate='mean'),\n",
    "  y=alt.Y('condition:N'),\n",
    " color=alt.Color('condition:N', sort=['ruler'])\n",
    ")\n",
    "\n",
    "error_bars + points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "colab_type": "code",
    "id": "eo3aHJAUS64U",
    "outputId": "b4ac871f-9998-41a2-a568-044f1dff04e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tWilcoxon signed rank test with continuity correction\n",
      "\n",
      "data:  df_q$overall.satisfaction[df_q$condition == \"ruler\"] and df_q$overall.satisfaction[df_q$condition == \"snorkel\"]\n",
      "V = 43, p-value = 0.1151\n",
      "alternative hypothesis: true location shift is not equal to 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%R \n",
    "library(stringr)\n",
    "names(df_q)<-str_replace_all(names(df_q), c(\" \" = \".\" , \",\" = \"\" )) # R doesn't well handle  col names with space. \n",
    "wilcox.test(df_q$overall.satisfaction[df_q$condition==\"ruler\"],df_q$overall.satisfaction[df_q$condition==\"snorkel\"] , paired = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LQkq6x-aJIwA"
   },
   "source": [
    "Not significantly different **(p=0.12 > 0.05)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 77
    },
    "colab_type": "code",
    "id": "UuOSbHQHTqP2",
    "outputId": "1e17d3a6-8bba-4187-8b3c-bf775edd09db"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>.y.</th>\n",
       "      <th>group1</th>\n",
       "      <th>group2</th>\n",
       "      <th>effsize</th>\n",
       "      <th>n1</th>\n",
       "      <th>n2</th>\n",
       "      <th>magnitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>overall.satisfaction</td>\n",
       "      <td>ruler</td>\n",
       "      <td>snorkel</td>\n",
       "      <td>0.514882</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    .y. group1   group2   effsize  n1  n2 magnitude\n",
       "1  overall.satisfaction  ruler  snorkel  0.514882  10  10     large"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%R wilcox_effsize(df_q, overall.satisfaction~condition, paired=T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CGBll-k_UJcc"
   },
   "source": [
    "We got a large effect size for the significace of the difference in **ease of use**.  Now, we move on to **ease of learning**.\n",
    "\n",
    "#### Ease of learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "colab_type": "code",
    "id": "SbtnawpAUGX6",
    "outputId": "ed41cc54-a713-45bd-e8fa-91d4cb965f93"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-54de6d1105214a38b4920328dea41cd0\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-54de6d1105214a38b4920328dea41cd0\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-54de6d1105214a38b4920328dea41cd0\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"layer\": [{\"mark\": {\"type\": \"errorbar\", \"extent\": \"stderr\"}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"condition\", \"sort\": [\"ruler\"]}, \"x\": {\"type\": \"quantitative\", \"field\": \"ease of learning\"}, \"y\": {\"type\": \"nominal\", \"field\": \"condition\"}}}, {\"mark\": {\"type\": \"point\", \"filled\": true}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"condition\", \"sort\": [\"ruler\"]}, \"x\": {\"type\": \"quantitative\", \"aggregate\": \"mean\", \"field\": \"ease of learning\"}, \"y\": {\"type\": \"nominal\", \"field\": \"condition\"}}}], \"data\": {\"name\": \"data-bf287e31fd84c14b647e1d12f1a39ff4\"}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-bf287e31fd84c14b647e1d12f1a39ff4\": [{\"condition\": \"ruler\", \"overall satisfaction\": 4, \"ease of use\": 4, \"expressivity\": 2, \"ease of learning\": 4, \"feedback\": \"\", \"how to improve\": \"1. summary panel for labeling function helping to group and delete/add LFs. 2. adding LFs not by examples should be combined\", \"other\": \"\", \"comments\": \"\", \"participant\": \"p7\"}, {\"condition\": \"snorkel\", \"overall satisfaction\": 3, \"ease of use\": 2, \"expressivity\": 5, \"ease of learning\": 4, \"feedback\": \"\", \"how to improve\": \"\", \"other\": \"\", \"comments\": \"\", \"participant\": \"p7\"}, {\"condition\": \"ruler\", \"overall satisfaction\": 5, \"ease of use\": 5, \"expressivity\": 5, \"ease of learning\": 5, \"feedback\": \"I wonder if I should have added more functions more quickly and done more pruning given the diffs.\\n\\nI do think that organizing labelling functions into concepts helped quite a bit. However, users who are using concepts to organize their functions (and thus modifying their concepts over time) would want to see diffs for the statistics after each concept modification.\", \"how to improve\": \"Below are the bugs we discussed. I am not suggesting they all need to be fixed :).\\n1. When I submitted a value modification for a concept row, the span annotations in the text did not update unless a new span could be identified in the text after the modification. Example: changing \\\"firearm\\\" to \\\"fire\\\" would cause the annotations to update, but a change from \\\"firearm\\\" to \\\"filkjwerlkjsdf\\\" would not cause an update.\\n2. I could not delete concept rows.\\n\\nBelow are some possible improvements I can think of:\\n1. I wasn't always sure when a concept modification had taken effect. It would be nice if there were some indication in the UI that the concept modification was in fact affect the output.\\n2. We could show a plot of the historical performance of the model over time. The plot could be a multi-line chart of statistics in the top-right corner on the y-axis and the chronological change id on the x-axis. Then, if a user were to click on a point in the plot, they would see a modal that would ask them if they would like to download the model from that point in time.\\n3. It would be nice to see class-specific statistics for snorkel's labelling model.\\n4. Should we give the user the option to select \\\"weighted average\\\" or \\\"simple average\\\" for the statistics on the entire development set?\\n5. I do think that organizing labelling functions into concepts helped quite a bit. Users who are using concepts to organize their functions (and thus modifying their concepts over time) would want to see diffs for the statistics after each concept modification.\", \"other\": \"I thought it was conducted very well!\", \"comments\": \"Thanks!\", \"participant\": \"p3\"}, {\"condition\": \"snorkel\", \"overall satisfaction\": 4, \"ease of use\": 4, \"expressivity\": 5, \"ease of learning\": 4, \"feedback\": \"What worked:\\n1. Writing my own functions to analyze why labeling functions were making incorrect predictions\\n2. I didn't have time to do this, but I would have probably analyzed the model performance metrics myself. We ran into a problem where we weren't entirely sure how the metrics were being calculated, and so I would probably calculate them myself to have complete understanding.\", \"how to improve\": \"They may already have this, but I would add the ability to pass in your own metric definitions to the evaluation step\", \"other\": \"The study was great! I would use the \\\"Table of Contents (2)\\\" extension to enable the users to more easily navigate the Jupyter Notebook.\\n\\nhttps://jupyter-contrib-nbextensions.readthedocs.io/en/latest/nbextensions/toc2/README.html\\n\\nhttps://jupyter-contrib-nbextensions.readthedocs.io/en/latest/\", \"comments\": \"We were having trouble determining why Snorkel was telling us we had classified 94 positive results correctly and 0 incorrectly but achieved only 47% accuracy. I think Snorkel was saying we correctly labeled 94 of the 94 actual positive examples, and 47% of the examples we identified as positive were actually positive. In other words, Snorkel was telling us that our recall was 100% and our precision was 47%.\", \"participant\": \"p3\"}, {\"condition\": \"snorkel\", \"overall satisfaction\": 3, \"ease of use\": 2, \"expressivity\": 5, \"ease of learning\": 2, \"feedback\": \"It was hard to write complex functions because of the time limit. I wanted to see overall statistics of the term frequency, but I was not able to check the statistics easily in time.\\nSo, I just used simple keyword matching as labeling functions\", \"how to improve\": \"I would show statistics about term frequency to users to help writing labeling functions. Also, I would like to make some helper functions to get synonyms and antonyms easily available, so that a user can improve coverage of simple keyword-matching strategy.\", \"other\": \"Tutorial and explanations were thorough. The researcher remained unbiased and objective.\", \"comments\": \"\", \"participant\": \"p9\"}, {\"condition\": \"ruler\", \"overall satisfaction\": 4, \"ease of use\": 5, \"expressivity\": 4, \"ease of learning\": 5, \"feedback\": \"I tried to construct concepts with relevant keywords for each label. This strategy worked for certain label (spam) but didn't work well for another label (not spam). I also tried to build rules based on entity labels, but it didn't work well.\", \"how to improve\": \"I'd add the 'not have' condition. It was hard to find out 'must-have' keywords for the 'not spam' label. I'd like to add some statistical characteristics (word count, text length) of a data record and synthesize in/equalities using them.\", \"other\": \"\", \"comments\": \"\", \"participant\": \"p9\"}, {\"condition\": \"snorkel\", \"overall satisfaction\": 3, \"ease of use\": 3, \"expressivity\": 4, \"ease of learning\": 4, \"feedback\": \"I'm used using Python, but I usually need to double check many of the commands syntax, so it makes the process of generating the rules a little bit slower.\", \"how to improve\": \"The pre-defined functions provided by the responsible for the experiment helped a lot. Having more pre-defined functions would be very useful (even for programmers).\", \"other\": \"I liked it.\", \"comments\": \"\", \"participant\": \"p4\"}, {\"condition\": \"ruler\", \"overall satisfaction\": 5, \"ease of use\": 5, \"expressivity\": 4, \"ease of learning\": 5, \"feedback\": \"I enjoyed using the tool! I could quickly define a set of rules with reasonable Precision/Recall over the available data. It would take much longer to get to the same performance without the help of Ruler. \\nOne type of rule that I could not create is for negative examples. I tried to create a rule that would be a negative example of spam. In the controlled experiment scenario (as it is a binary classification task) I could get the same effect by set the \\\"negative example rule\\\" for one class as a \\\"positive example\\\" rule to the other class.\\nAlso, I tried to create a rule (based on my domain knowledge) that was not specifically associated to a instance, but I could not.\", \"how to improve\": \"Allowing the addition of:\\n1)  \\\"negative examples rules\\\";\\n2) general rules (not associated to any specific example);\\n3) a \\\"python window\\\" in which you could use python code (as used in Snorkel), thus explore best of both worlds (the easiness and speed of current Ruler, and the expressiveness of Snorkel)\", \"other\": \"\", \"comments\": \"\", \"participant\": \"p4\"}, {\"condition\": \"ruler\", \"overall satisfaction\": 5, \"ease of use\": 4, \"expressivity\": 4, \"ease of learning\": 3, \"feedback\": \"Work\\n1. The tool can capture keyword-based functions. \\n2. The tool supports AND and OR operators. \\n\\nDidn't work\\n1. Some terms are not well-defined (e.g. Concept).\\n2. The tool lacks step-by-step documents.  \", \"how to improve\": \"1. Give formal definitions to key terms. \\n2. Prepare a step-by-step tutorial.  \", \"other\": \"The tool is cool. \", \"comments\": \"\", \"participant\": \"p6\"}, {\"condition\": \"snorkel\", \"overall satisfaction\": 4, \"ease of use\": 4, \"expressivity\": 4, \"ease of learning\": 2, \"feedback\": \"Worked\\n1. Snorkel supports python that I am familiar with. \\n2. Snorkel enables me to leverage programming skills to label data.\\n\\nDidn't work\\n1. Snorkel is coding intensive that I have to run multiple Snorkel cells to evaluate labelling functions. \\n2. Snorkel does not instantly evaluate labeling functions. I have to rerun evaluation codes each time I update labeling functions. \", \"how to improve\": \"1. Reduce unnecessary coding as much as possible. \\n2. Make the evaluation of labeling functions instant. \", \"other\": \"The Snorkel tool is cool. \", \"comments\": \"\", \"participant\": \"p6\"}, {\"condition\": \"ruler\", \"overall satisfaction\": 3, \"ease of use\": 5, \"expressivity\": 3, \"ease of learning\": 5, \"feedback\": \"\", \"how to improve\": \"\", \"other\": \"\", \"comments\": \"\", \"participant\": \"p5\"}, {\"condition\": \"snorkel\", \"overall satisfaction\": 5, \"ease of use\": 4, \"expressivity\": 5, \"ease of learning\": 5, \"feedback\": \"\", \"how to improve\": \"\", \"other\": \"\", \"comments\": \"\", \"participant\": \"p5\"}, {\"condition\": \"ruler\", \"overall satisfaction\": 4, \"ease of use\": 3, \"expressivity\": 4, \"ease of learning\": 2, \"feedback\": \"Categorizing tokens was hard.for me\", \"how to improve\": \"When I mouse over token(s), I wished I had a popup to categorize it\", \"other\": \"\", \"comments\": \"It was great exercise for me! thank you!\", \"participant\": \"p0\"}, {\"condition\": \"snorkel\", \"overall satisfaction\": 5, \"ease of use\": 4, \"expressivity\": 5, \"ease of learning\": 4, \"feedback\": \"I firstly write functions so that recalls get high.\", \"how to improve\": \"If non-essential codes (e.g. evaluation codes) were defined out of the notebooks, they would be more easy to understand.\", \"other\": \"\", \"comments\": \"Great experiments! I will look into snorkel as I have some ML tasks. Thanks!\", \"participant\": \"p0\"}, {\"condition\": \"snorkel\", \"overall satisfaction\": 2, \"ease of use\": 3, \"expressivity\": 4, \"ease of learning\": 3, \"feedback\": \"\", \"how to improve\": \"\", \"other\": \"\", \"comments\": \"\", \"participant\": \"p1\"}, {\"condition\": \"ruler\", \"overall satisfaction\": 4, \"ease of use\": 5, \"expressivity\": 4, \"ease of learning\": 4, \"feedback\": \"I noticed that my label accuracy did not constantly improving: first improves and then drops. Maybe this is just an extreme case, but I feel it is important to validate if other users also show similar trend.\", \"how to improve\": \"Scalability: the system becomes slower towards the end. Further optimization & approximation could be considered.\", \"other\": \"\", \"comments\": \"\", \"participant\": \"p1\"}, {\"condition\": \"snorkel\", \"overall satisfaction\": 2, \"ease of use\": 2, \"expressivity\": 4, \"ease of learning\": 2, \"feedback\": \"My strategy was to directly take a look at examples and came up with salient words/phrases to write down keyword-based labeling functions. \", \"how to improve\": \"I was confused with the metric shown in the second block of the Apply function section. The comment \\\"Don't worry\\\" was not enough for me to disregard the value. :p\\n\\nMy work was going back and forth between labeling function, applying function and training a classifier. I would be helpful if the cells that the user runs are compiled into a single function (on a single cell) so I could simply call the function.\\n\\nFor example, prepare a function that traverses the namespace to list up any functions that begin with lf (or a longer prefix if it conflicts with something).\\n\", \"other\": \"The instructions are clear and the user study is organized well. I'm curious about the (psychological) effect of the time limit and being monitored/recorded.\\n\\nI may not pay attention but the input argument `x` of each labeling function was not clear at the beginning, which took a couple of minutes to figure out.\", \"comments\": \"\", \"participant\": \"p2\"}, {\"condition\": \"ruler\", \"overall satisfaction\": 4, \"ease of use\": 5, \"expressivity\": 3, \"ease of learning\": 5, \"feedback\": \"My strategy is simply adding salient words/phrases of each class while monitoring the dev set performance. As I added labeling functions, I tried to create more detailed rules as I progress.\", \"how to improve\": \"1)\\nIt was not very intuitive how the system makes use of the labeling functions I made. What I was confused is when I saw the recall of class 0 dropped after adding labeling function for class 0. I had to conduct label-function engineering to figure out the best combination.\\n\\n2)\\nAs I asked during the user study, it would be helpful if I could directly add rules that are not activated by the current example (which I could with Snorkel.)\\n\\n3)\\nSomehow, it seems that the data has more positive examples than negative examples. It will be helpful if the system has a search function to retrieve a negative example (that contains certain words etc.)\\n\\n4)\\nI found several examples from which I didn't want to create labeling functions, but I would like to simply label the examples. This may be for the user study but only allowing the user to create labeling functions may not be the best way.\\n\\n5)\\nSimilarly, in practice, it should be better having a base classifier/base dictionary as a starting point. For example (I worked on a sentiment analysis task today), using a pre-trained classifier (trained on other sentiment classifier dataset) and/and/or using sentiment dictionary that contains words with sentiment polarity information. I feel like what I did with the system is approximately close to reconstructing an affective dictionary from scratch (tuned toward the dataset, in one sense.)\\n\", \"other\": \"The user study was well organized and instructions were clear.\\n\\nI'm wondering if the user study randomly shuffles the order of methods. Although two tools/datasets are different, I feel like I was more prepared to work on the task.\", \"comments\": \"\", \"participant\": \"p2\"}, {\"condition\": \"snorkel\", \"overall satisfaction\": 3, \"ease of use\": 4, \"expressivity\": 2, \"ease of learning\": 4, \"feedback\": \"I tried to create a bunch of labeling functions on filtering by certain tokens at beginning and see how each works; then I modified those that turn out to have most wrong labels and got better results. Also I tried to have labeling functions on length and non-letters of the text, which seems to be not very useful. I should have tried to ensemble the labeling functions, which would have improve the coverage a lot.\", \"how to improve\": \"Providing more stats/exploration options in terms of helping the user improve coverage. Instead of simply showing overlapping and conflicts, it would be better to see the samples and stats of two LFs by their overlap or conflict. Also it could automatically search for ensembling the labeling functions and provide suggestions to the user.\", \"other\": \"\", \"comments\": \"\", \"participant\": \"p8\"}, {\"condition\": \"ruler\", \"overall satisfaction\": 4, \"ease of use\": 4, \"expressivity\": 2, \"ease of learning\": 5, \"feedback\": \"I think it is task dependent. For sentiment analysis, the coverage of labeling functions on certain tokens or phrases can be relatively small and may not be accurate as there are many variants in the phrase and the negation would affect the result. \", \"how to improve\": \"Like snorkel, I think auto suggestions on improving the coverage of LFs could be very helpful. And it may be useful to allow users combine LFs or edit LFs in python in order to reduce the execution time and to be more flexible for users with coding experience. The regex concept should be very useful but I only use it for several times, and I think it would be great if the tool can suggest regex expressions based on user's annotation.\", \"other\": \"I feel that there could be 10-15 more time after the tutorial for users to play with the tool on the example task. In both user studies I figured out ways to improve the performance shortly after it is finished.\", \"comments\": \"\", \"participant\": \"p8\"}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 126,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_bars = alt.Chart(df_q).mark_errorbar(extent='stderr').encode(\n",
    "  x=alt.X('ease of learning:Q'),\n",
    "  y=alt.Y('condition:N'),\n",
    "    color=alt.Color('condition:N', sort=['ruler'])\n",
    ")\n",
    "\n",
    "points = alt.Chart(df_q).mark_point(filled=True).encode(\n",
    "  x=alt.X('ease of learning:Q', aggregate='mean'),\n",
    "  y=alt.Y('condition:N'),\n",
    " color=alt.Color('condition:N', sort=['ruler'])\n",
    ")\n",
    "\n",
    "error_bars + points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "26yNmbciUUb-"
   },
   "source": [
    "Looks like  participants found **ruler** easier to learn than **snorkel**. Now let's test that hypothesis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "GqYQG3vKUSPX",
    "outputId": "b27f2d3b-702d-4b9a-efb3-18f967509c4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_relResult(statistic=1.9639610121239313, pvalue=0.08112618884584057)"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ruler_learn =  df_q[df_q['condition']=='ruler']['ease of learning']\n",
    "snorkel_learn = df_q[df_q['condition']=='snorkel']['ease of learning']\n",
    "stats.ttest_rel(ruler_learn, snorkel_learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 77
    },
    "colab_type": "code",
    "id": "QBbdvzDkUWV6",
    "outputId": "bdd9c5c1-e28f-43a6-a68a-74805bc0bd2a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>.y.</th>\n",
       "      <th>group1</th>\n",
       "      <th>group2</th>\n",
       "      <th>effsize</th>\n",
       "      <th>n1</th>\n",
       "      <th>n2</th>\n",
       "      <th>magnitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ease.of.learning</td>\n",
       "      <td>ruler</td>\n",
       "      <td>snorkel</td>\n",
       "      <td>0.621059</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>moderate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                .y. group1   group2   effsize  n1  n2 magnitude\n",
       "1  ease.of.learning  ruler  snorkel  0.621059  10  10  moderate"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%R cohens_d(df_q,ease.of.learning~condition,paired=T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y3VdGgwaUbaB"
   },
   "source": [
    "The difference in **ease of learning** between two conditions, **ruler** and **snorkel**, is not statistically signficant (**pvalue = 0.08 > 0.05**). \n",
    "\n",
    "Finally, let's look into **overall satisfaction** of participants with the respective tools.\n",
    "\n",
    "#### Satisfaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "colab_type": "code",
    "id": "bDT5wwzlUXvm",
    "outputId": "92e2850f-9e8b-40ef-8bda-8bba7366632d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-9add643487a84bbc8d58f9050e15febe\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-9add643487a84bbc8d58f9050e15febe\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-9add643487a84bbc8d58f9050e15febe\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"layer\": [{\"mark\": {\"type\": \"errorbar\", \"extent\": \"stderr\"}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"condition\", \"sort\": [\"ruler\"]}, \"x\": {\"type\": \"quantitative\", \"field\": \"overall satisfaction\"}, \"y\": {\"type\": \"nominal\", \"field\": \"condition\"}}}, {\"mark\": {\"type\": \"point\", \"filled\": true}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"condition\", \"sort\": [\"ruler\"]}, \"x\": {\"type\": \"quantitative\", \"aggregate\": \"mean\", \"field\": \"overall satisfaction\"}, \"y\": {\"type\": \"nominal\", \"field\": \"condition\"}}}], \"data\": {\"name\": \"data-bf287e31fd84c14b647e1d12f1a39ff4\"}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-bf287e31fd84c14b647e1d12f1a39ff4\": [{\"condition\": \"ruler\", \"overall satisfaction\": 4, \"ease of use\": 4, \"expressivity\": 2, \"ease of learning\": 4, \"feedback\": \"\", \"how to improve\": \"1. summary panel for labeling function helping to group and delete/add LFs. 2. adding LFs not by examples should be combined\", \"other\": \"\", \"comments\": \"\", \"participant\": \"p7\"}, {\"condition\": \"snorkel\", \"overall satisfaction\": 3, \"ease of use\": 2, \"expressivity\": 5, \"ease of learning\": 4, \"feedback\": \"\", \"how to improve\": \"\", \"other\": \"\", \"comments\": \"\", \"participant\": \"p7\"}, {\"condition\": \"ruler\", \"overall satisfaction\": 5, \"ease of use\": 5, \"expressivity\": 5, \"ease of learning\": 5, \"feedback\": \"I wonder if I should have added more functions more quickly and done more pruning given the diffs.\\n\\nI do think that organizing labelling functions into concepts helped quite a bit. However, users who are using concepts to organize their functions (and thus modifying their concepts over time) would want to see diffs for the statistics after each concept modification.\", \"how to improve\": \"Below are the bugs we discussed. I am not suggesting they all need to be fixed :).\\n1. When I submitted a value modification for a concept row, the span annotations in the text did not update unless a new span could be identified in the text after the modification. Example: changing \\\"firearm\\\" to \\\"fire\\\" would cause the annotations to update, but a change from \\\"firearm\\\" to \\\"filkjwerlkjsdf\\\" would not cause an update.\\n2. I could not delete concept rows.\\n\\nBelow are some possible improvements I can think of:\\n1. I wasn't always sure when a concept modification had taken effect. It would be nice if there were some indication in the UI that the concept modification was in fact affect the output.\\n2. We could show a plot of the historical performance of the model over time. The plot could be a multi-line chart of statistics in the top-right corner on the y-axis and the chronological change id on the x-axis. Then, if a user were to click on a point in the plot, they would see a modal that would ask them if they would like to download the model from that point in time.\\n3. It would be nice to see class-specific statistics for snorkel's labelling model.\\n4. Should we give the user the option to select \\\"weighted average\\\" or \\\"simple average\\\" for the statistics on the entire development set?\\n5. I do think that organizing labelling functions into concepts helped quite a bit. Users who are using concepts to organize their functions (and thus modifying their concepts over time) would want to see diffs for the statistics after each concept modification.\", \"other\": \"I thought it was conducted very well!\", \"comments\": \"Thanks!\", \"participant\": \"p3\"}, {\"condition\": \"snorkel\", \"overall satisfaction\": 4, \"ease of use\": 4, \"expressivity\": 5, \"ease of learning\": 4, \"feedback\": \"What worked:\\n1. Writing my own functions to analyze why labeling functions were making incorrect predictions\\n2. I didn't have time to do this, but I would have probably analyzed the model performance metrics myself. We ran into a problem where we weren't entirely sure how the metrics were being calculated, and so I would probably calculate them myself to have complete understanding.\", \"how to improve\": \"They may already have this, but I would add the ability to pass in your own metric definitions to the evaluation step\", \"other\": \"The study was great! I would use the \\\"Table of Contents (2)\\\" extension to enable the users to more easily navigate the Jupyter Notebook.\\n\\nhttps://jupyter-contrib-nbextensions.readthedocs.io/en/latest/nbextensions/toc2/README.html\\n\\nhttps://jupyter-contrib-nbextensions.readthedocs.io/en/latest/\", \"comments\": \"We were having trouble determining why Snorkel was telling us we had classified 94 positive results correctly and 0 incorrectly but achieved only 47% accuracy. I think Snorkel was saying we correctly labeled 94 of the 94 actual positive examples, and 47% of the examples we identified as positive were actually positive. In other words, Snorkel was telling us that our recall was 100% and our precision was 47%.\", \"participant\": \"p3\"}, {\"condition\": \"snorkel\", \"overall satisfaction\": 3, \"ease of use\": 2, \"expressivity\": 5, \"ease of learning\": 2, \"feedback\": \"It was hard to write complex functions because of the time limit. I wanted to see overall statistics of the term frequency, but I was not able to check the statistics easily in time.\\nSo, I just used simple keyword matching as labeling functions\", \"how to improve\": \"I would show statistics about term frequency to users to help writing labeling functions. Also, I would like to make some helper functions to get synonyms and antonyms easily available, so that a user can improve coverage of simple keyword-matching strategy.\", \"other\": \"Tutorial and explanations were thorough. The researcher remained unbiased and objective.\", \"comments\": \"\", \"participant\": \"p9\"}, {\"condition\": \"ruler\", \"overall satisfaction\": 4, \"ease of use\": 5, \"expressivity\": 4, \"ease of learning\": 5, \"feedback\": \"I tried to construct concepts with relevant keywords for each label. This strategy worked for certain label (spam) but didn't work well for another label (not spam). I also tried to build rules based on entity labels, but it didn't work well.\", \"how to improve\": \"I'd add the 'not have' condition. It was hard to find out 'must-have' keywords for the 'not spam' label. I'd like to add some statistical characteristics (word count, text length) of a data record and synthesize in/equalities using them.\", \"other\": \"\", \"comments\": \"\", \"participant\": \"p9\"}, {\"condition\": \"snorkel\", \"overall satisfaction\": 3, \"ease of use\": 3, \"expressivity\": 4, \"ease of learning\": 4, \"feedback\": \"I'm used using Python, but I usually need to double check many of the commands syntax, so it makes the process of generating the rules a little bit slower.\", \"how to improve\": \"The pre-defined functions provided by the responsible for the experiment helped a lot. Having more pre-defined functions would be very useful (even for programmers).\", \"other\": \"I liked it.\", \"comments\": \"\", \"participant\": \"p4\"}, {\"condition\": \"ruler\", \"overall satisfaction\": 5, \"ease of use\": 5, \"expressivity\": 4, \"ease of learning\": 5, \"feedback\": \"I enjoyed using the tool! I could quickly define a set of rules with reasonable Precision/Recall over the available data. It would take much longer to get to the same performance without the help of Ruler. \\nOne type of rule that I could not create is for negative examples. I tried to create a rule that would be a negative example of spam. In the controlled experiment scenario (as it is a binary classification task) I could get the same effect by set the \\\"negative example rule\\\" for one class as a \\\"positive example\\\" rule to the other class.\\nAlso, I tried to create a rule (based on my domain knowledge) that was not specifically associated to a instance, but I could not.\", \"how to improve\": \"Allowing the addition of:\\n1)  \\\"negative examples rules\\\";\\n2) general rules (not associated to any specific example);\\n3) a \\\"python window\\\" in which you could use python code (as used in Snorkel), thus explore best of both worlds (the easiness and speed of current Ruler, and the expressiveness of Snorkel)\", \"other\": \"\", \"comments\": \"\", \"participant\": \"p4\"}, {\"condition\": \"ruler\", \"overall satisfaction\": 5, \"ease of use\": 4, \"expressivity\": 4, \"ease of learning\": 3, \"feedback\": \"Work\\n1. The tool can capture keyword-based functions. \\n2. The tool supports AND and OR operators. \\n\\nDidn't work\\n1. Some terms are not well-defined (e.g. Concept).\\n2. The tool lacks step-by-step documents.  \", \"how to improve\": \"1. Give formal definitions to key terms. \\n2. Prepare a step-by-step tutorial.  \", \"other\": \"The tool is cool. \", \"comments\": \"\", \"participant\": \"p6\"}, {\"condition\": \"snorkel\", \"overall satisfaction\": 4, \"ease of use\": 4, \"expressivity\": 4, \"ease of learning\": 2, \"feedback\": \"Worked\\n1. Snorkel supports python that I am familiar with. \\n2. Snorkel enables me to leverage programming skills to label data.\\n\\nDidn't work\\n1. Snorkel is coding intensive that I have to run multiple Snorkel cells to evaluate labelling functions. \\n2. Snorkel does not instantly evaluate labeling functions. I have to rerun evaluation codes each time I update labeling functions. \", \"how to improve\": \"1. Reduce unnecessary coding as much as possible. \\n2. Make the evaluation of labeling functions instant. \", \"other\": \"The Snorkel tool is cool. \", \"comments\": \"\", \"participant\": \"p6\"}, {\"condition\": \"ruler\", \"overall satisfaction\": 3, \"ease of use\": 5, \"expressivity\": 3, \"ease of learning\": 5, \"feedback\": \"\", \"how to improve\": \"\", \"other\": \"\", \"comments\": \"\", \"participant\": \"p5\"}, {\"condition\": \"snorkel\", \"overall satisfaction\": 5, \"ease of use\": 4, \"expressivity\": 5, \"ease of learning\": 5, \"feedback\": \"\", \"how to improve\": \"\", \"other\": \"\", \"comments\": \"\", \"participant\": \"p5\"}, {\"condition\": \"ruler\", \"overall satisfaction\": 4, \"ease of use\": 3, \"expressivity\": 4, \"ease of learning\": 2, \"feedback\": \"Categorizing tokens was hard.for me\", \"how to improve\": \"When I mouse over token(s), I wished I had a popup to categorize it\", \"other\": \"\", \"comments\": \"It was great exercise for me! thank you!\", \"participant\": \"p0\"}, {\"condition\": \"snorkel\", \"overall satisfaction\": 5, \"ease of use\": 4, \"expressivity\": 5, \"ease of learning\": 4, \"feedback\": \"I firstly write functions so that recalls get high.\", \"how to improve\": \"If non-essential codes (e.g. evaluation codes) were defined out of the notebooks, they would be more easy to understand.\", \"other\": \"\", \"comments\": \"Great experiments! I will look into snorkel as I have some ML tasks. Thanks!\", \"participant\": \"p0\"}, {\"condition\": \"snorkel\", \"overall satisfaction\": 2, \"ease of use\": 3, \"expressivity\": 4, \"ease of learning\": 3, \"feedback\": \"\", \"how to improve\": \"\", \"other\": \"\", \"comments\": \"\", \"participant\": \"p1\"}, {\"condition\": \"ruler\", \"overall satisfaction\": 4, \"ease of use\": 5, \"expressivity\": 4, \"ease of learning\": 4, \"feedback\": \"I noticed that my label accuracy did not constantly improving: first improves and then drops. Maybe this is just an extreme case, but I feel it is important to validate if other users also show similar trend.\", \"how to improve\": \"Scalability: the system becomes slower towards the end. Further optimization & approximation could be considered.\", \"other\": \"\", \"comments\": \"\", \"participant\": \"p1\"}, {\"condition\": \"snorkel\", \"overall satisfaction\": 2, \"ease of use\": 2, \"expressivity\": 4, \"ease of learning\": 2, \"feedback\": \"My strategy was to directly take a look at examples and came up with salient words/phrases to write down keyword-based labeling functions. \", \"how to improve\": \"I was confused with the metric shown in the second block of the Apply function section. The comment \\\"Don't worry\\\" was not enough for me to disregard the value. :p\\n\\nMy work was going back and forth between labeling function, applying function and training a classifier. I would be helpful if the cells that the user runs are compiled into a single function (on a single cell) so I could simply call the function.\\n\\nFor example, prepare a function that traverses the namespace to list up any functions that begin with lf (or a longer prefix if it conflicts with something).\\n\", \"other\": \"The instructions are clear and the user study is organized well. I'm curious about the (psychological) effect of the time limit and being monitored/recorded.\\n\\nI may not pay attention but the input argument `x` of each labeling function was not clear at the beginning, which took a couple of minutes to figure out.\", \"comments\": \"\", \"participant\": \"p2\"}, {\"condition\": \"ruler\", \"overall satisfaction\": 4, \"ease of use\": 5, \"expressivity\": 3, \"ease of learning\": 5, \"feedback\": \"My strategy is simply adding salient words/phrases of each class while monitoring the dev set performance. As I added labeling functions, I tried to create more detailed rules as I progress.\", \"how to improve\": \"1)\\nIt was not very intuitive how the system makes use of the labeling functions I made. What I was confused is when I saw the recall of class 0 dropped after adding labeling function for class 0. I had to conduct label-function engineering to figure out the best combination.\\n\\n2)\\nAs I asked during the user study, it would be helpful if I could directly add rules that are not activated by the current example (which I could with Snorkel.)\\n\\n3)\\nSomehow, it seems that the data has more positive examples than negative examples. It will be helpful if the system has a search function to retrieve a negative example (that contains certain words etc.)\\n\\n4)\\nI found several examples from which I didn't want to create labeling functions, but I would like to simply label the examples. This may be for the user study but only allowing the user to create labeling functions may not be the best way.\\n\\n5)\\nSimilarly, in practice, it should be better having a base classifier/base dictionary as a starting point. For example (I worked on a sentiment analysis task today), using a pre-trained classifier (trained on other sentiment classifier dataset) and/and/or using sentiment dictionary that contains words with sentiment polarity information. I feel like what I did with the system is approximately close to reconstructing an affective dictionary from scratch (tuned toward the dataset, in one sense.)\\n\", \"other\": \"The user study was well organized and instructions were clear.\\n\\nI'm wondering if the user study randomly shuffles the order of methods. Although two tools/datasets are different, I feel like I was more prepared to work on the task.\", \"comments\": \"\", \"participant\": \"p2\"}, {\"condition\": \"snorkel\", \"overall satisfaction\": 3, \"ease of use\": 4, \"expressivity\": 2, \"ease of learning\": 4, \"feedback\": \"I tried to create a bunch of labeling functions on filtering by certain tokens at beginning and see how each works; then I modified those that turn out to have most wrong labels and got better results. Also I tried to have labeling functions on length and non-letters of the text, which seems to be not very useful. I should have tried to ensemble the labeling functions, which would have improve the coverage a lot.\", \"how to improve\": \"Providing more stats/exploration options in terms of helping the user improve coverage. Instead of simply showing overlapping and conflicts, it would be better to see the samples and stats of two LFs by their overlap or conflict. Also it could automatically search for ensembling the labeling functions and provide suggestions to the user.\", \"other\": \"\", \"comments\": \"\", \"participant\": \"p8\"}, {\"condition\": \"ruler\", \"overall satisfaction\": 4, \"ease of use\": 4, \"expressivity\": 2, \"ease of learning\": 5, \"feedback\": \"I think it is task dependent. For sentiment analysis, the coverage of labeling functions on certain tokens or phrases can be relatively small and may not be accurate as there are many variants in the phrase and the negation would affect the result. \", \"how to improve\": \"Like snorkel, I think auto suggestions on improving the coverage of LFs could be very helpful. And it may be useful to allow users combine LFs or edit LFs in python in order to reduce the execution time and to be more flexible for users with coding experience. The regex concept should be very useful but I only use it for several times, and I think it would be great if the tool can suggest regex expressions based on user's annotation.\", \"other\": \"I feel that there could be 10-15 more time after the tutorial for users to play with the tool on the example task. In both user studies I figured out ways to improve the performance shortly after it is finished.\", \"comments\": \"\", \"participant\": \"p8\"}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_bars = alt.Chart(df_q).mark_errorbar(extent='stderr').encode(\n",
    "  x=alt.X('overall satisfaction:Q'),\n",
    "  y=alt.Y('condition:N'),\n",
    "    color=alt.Color('condition:N', sort=['ruler'])\n",
    ")\n",
    "\n",
    "points = alt.Chart(df_q).mark_point(filled=True).encode(\n",
    "  x=alt.X('overall satisfaction:Q', aggregate='mean'),\n",
    "  y=alt.Y('condition:N'),\n",
    " color=alt.Color('condition:N', sort=['ruler'])\n",
    ")\n",
    "\n",
    "error_bars + points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "syFxpuUuUiT2"
   },
   "source": [
    "Allright. Is this difference statistically significant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "seG9PtmqUfzX",
    "outputId": "3953041a-4f1a-4edf-f492-9d937f4699b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_relResult(statistic=1.9215378456610457, pvalue=0.08684229054535088)"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ruler_overall =  df_q[df_q['condition']=='ruler']['overall satisfaction']\n",
    "snorkel_overall = df_q[df_q['condition']=='snorkel']['overall satisfaction']\n",
    "stats.ttest_rel(ruler_overall, snorkel_overall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 77
    },
    "colab_type": "code",
    "id": "uoWV_egJUjpq",
    "outputId": "9bc0fdc2-5f94-48ea-deac-bf66c9317fbf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>.y.</th>\n",
       "      <th>group1</th>\n",
       "      <th>group2</th>\n",
       "      <th>effsize</th>\n",
       "      <th>n1</th>\n",
       "      <th>n2</th>\n",
       "      <th>magnitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>overall.satisfaction</td>\n",
       "      <td>ruler</td>\n",
       "      <td>snorkel</td>\n",
       "      <td>0.607644</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>moderate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    .y. group1   group2   effsize  n1  n2 magnitude\n",
       "1  overall.satisfaction  ruler  snorkel  0.607644  10  10  moderate"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%R cohens_d(df_q,overall.satisfaction~condition,paired=T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "37ioqV1OUpJ4"
   },
   "source": [
    "The difference in overall  satisfaction with two tools, **ruler** and **snorkel**, is not statistically signficant (**pvalue = 0.09 > 0.05**). "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOLfeh/s2aDNyTfsoZ6ZqWE",
   "include_colab_link": true,
   "name": "ruler_user_study_figures.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
